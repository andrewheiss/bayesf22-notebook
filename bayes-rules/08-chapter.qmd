---
title: "Reading notes"
subtitle: "Posterior inference and prediction"
date: "September 28, 2022"
---

[(Original chapter)](https://www.bayesrulesbook.com/chapter-8.html)

```{r warning=FALSE, message=FALSE}
library(bayesrules)
library(tidyverse)
library(brms)
library(cmdstanr)
library(tidybayes)
library(ggdist)

# Plot stuff
clrs <- MetBrewer::met.brewer("Lakota", 6)
theme_set(theme_bw())

# Seed stuff
set.seed(1234)
BAYES_SEED <- 1234
```


# The general setup

We want to know the probability that an artist in the MoMA is Gen X or younger (born after 1965). This is our $\pi$.

We'll use a vague $\operatorname{Beta}(4, 6)$ prior for $\pi$ and say that the probability is probably below 0.5, but we're not super sure where it is exactly:

```{r}
ggplot() +
  stat_function(fun = ~dbeta(., 4, 6), geom = "area", fill = clrs[1])
```

Here's the data:

```{r}
data("moma_sample", package = "bayesrules")
head(moma_sample)
```

Only 14 are Gen X:

```{r}
moma_sample |> 
  count(genx)
```

Through the magic of conjugate families, we can calculate the exact posterior:

$$
\begin{aligned}
Y &\sim \operatorname{Binomial}(100, \pi) \\
\pi &= \operatorname{Beta}(4, 6)
\end{aligned}
$$

Since we observe $Y = 14$, then the actual exact posterior is

$$
\pi \mid (Y = 14) \sim \operatorname{Beta}(4 + 14, 6 + 100 - 14) \rightarrow \operatorname{Beta}(18, 92)
$$

```{r}
ggplot() +
  stat_function(aes(fill = "Prior: Beta(4, 6)"),
                fun = ~dbeta(., 4, 6), geom = "area") +
  stat_function(aes(fill = "Posterior: Beta(18, 92)"),
                fun = ~dbeta(., 18, 92), geom = "area") +
  scale_fill_manual(values = c(clrs[2], clrs[1]))
```


Neat! We have a posterior, but now we have to do something with it:

- Estimation
- Hypothesis testing
- Prediction

But first, for fun, here are some MCMC-based approximations of the posterior:

::: {.panel-tabset}
### brms

```{r model-pi-brms, cache=TRUE}
model_pi_brms <- brm(
  bf(num_genx | trials(artworks) ~ 0 + Intercept),
  data = list(num_genx = 14, artworks = 100),
  family = binomial(link = "identity"),
  prior(beta(4, 6), class = b, lb = 0, ub = 1),
  sample_prior = TRUE,  # For calculating Bayes Ratios
  iter = 5000, warmup = 1000, seed = BAYES_SEED,
  backend = "cmdstanr", cores = 4, refresh = 0
)

model_pi_brms_prior_only <- brm(
  bf(num_genx | trials(artworks) ~ 0 + Intercept),
  data = list(num_genx = 14, artworks = 100),
  family = binomial(link = "identity"),
  prior(beta(4, 6), class = b, lb = 0, ub = 1),
  sample_prior = "only",  # For calculating Bayes Ratios
  iter = 5000, warmup = 1000, seed = BAYES_SEED,
  backend = "cmdstanr", cores = 4, refresh = 0
)
```

```{r}
model_pi_brms
```


### Stan

**08-stan/genx.stan**

```{stan, file="08-stan/genx.stan", eval=FALSE, output.var=""}
```

```{r compile-genx-model, cache=TRUE}
model_pi_stan <- cmdstan_model("08-stan/genx.stan")
```

```{r}
pi_stan_samples <- model_pi_stan$sample(
  data = list(artworks = 100, num_genx = 14),
  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, 
  refresh = 0, seed = BAYES_SEED
)
```

:::


# 8.1: Posterior estimation

Our posterior $\operatorname{Beta}(18, 92)$ is a complete distribution, but we often need to work with summaries of that distribution. The mean here is 16% ($\frac{18}{18 + 92} = 0.1636$), meaning that it is most likely the case that 16% of MoMA artists are Gen X or younger, but it could be anywhere between 10-25ish%

We can calculate a 95% credible interval around the median using quantiles:

```{r}
qbeta(c(0.025, 0.975), 18, 92)
```

There's a 95% posterior probability that somewhere between 10% and 24$ of museum artists are Gen X or younger:

```{r}
post_mean <- 18 / (18 + 92)
post_median <- qbeta(0.5, 18, 92)
post_mode <- (18 - 1)/(18 + 92 - 2)

ggplot() +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area",
                fill = colorspace::lighten(clrs[3], 0.4)) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.025, 0.975), 18, 92),
                fill = clrs[3]) +
  geom_vline(xintercept = post_mode) +
  xlim(c(0, 0.4)) +
  labs(x = "π")
```

We don't have to use 95%; that's just arbitrary. We can use different levels:

```{r}
ggplot() +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area",
                fill = colorspace::lighten(clrs[3], 0.9)) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.025, 0.975), 18, 92),
                aes(fill = "95%")) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.055, 0.945), 18, 92),
                aes(fill = "89%")) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.1, 0.9), 18, 92),
                aes(fill = "80%")) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.25, 0.75), 18, 92),
                aes(fill = "50%")) +
  geom_vline(xintercept = post_mode) +
  scale_fill_manual(values = colorspace::lighten(clrs[3], c(0.1, 0.3, 0.5, 0.7))) +
  xlim(c(0, 0.4)) +
  labs(x = "π", fill = "Credible interval")
```

This posterior is a little lopsided, so we might want to make an interval that's not centered at the mode of π, but instead centered at the highest posterior density. 

::: {.panel-tabset}
### brms

```{r}
model_pi_brms |> 
  spread_draws(b_Intercept) |> 
  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.95))
```

```{r}
model_pi_brms |> 
  spread_draws(b_Intercept) |> 
  ggplot(aes(x = b_Intercept)) +
  stat_slab(aes(fill_ramp = stat(level)),
            .width = c(0.02, 0.5, 0.89, 0.95, 1),
            point_interval = "median_hdci",
            fill = clrs[3]) +
  scale_fill_ramp_discrete(range = c(0.2, 1)) +
  labs(fill_ramp = "Credible interval")
```

### Stan

```{r}
pi_stan_samples |> 
  spread_draws(pi) |> 
  median_hdci(pi, .width = c(0.5, 0.89, 0.95))
```

```{r}
pi_stan_samples |> 
  spread_draws(pi) |> 
  ggplot(aes(x = pi)) +
  stat_slab(aes(fill_ramp = stat(level)),
            .width = c(0.02, 0.5, 0.89, 0.95, 1),
            point_interval = "median_hdci",
            fill = clrs[3]) +
  scale_fill_ramp_discrete(range = c(0.2, 1)) +
  labs(fill_ramp = "Credible interval")
```

:::

# 8.2: Posterior hypothesis testing

What if we read somewhere that fewer than 20% of museum artists are Gen X or younger? We can calculate the posterior probability of this scenario, or $P(\pi < 0.2 \mid Y = 14)$

With the exact posterior, that's super easy:

```{r}
post_prob <- pbeta(0.2, 18, 92)
post_prob

ggplot() +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area",
                fill = colorspace::lighten(clrs[3], 0.4)) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = c(0, 0.2),
                fill = clrs[3]) +
  geom_vline(xintercept = 0.2) +
  xlim(c(0, 0.4)) +
  labs(x = "π")
```

85% of the distribution is below 0.2, so we can say there's an 85% chance that Gen X artists constitute 20% or fewer of modern art museum artists.

That's easy!

Here it is with MCMC:

::: {.panel-tabset}
### brms

```{r}
model_pi_brms |> 
  spread_draws(b_Intercept) |> 
  count(b_Intercept < 0.2) |> 
  mutate(prob = n / sum(n))

model_pi_brms |> 
  spread_draws(b_Intercept) |> 
  ggplot(aes(x = b_Intercept)) +
  stat_halfeye(aes(fill_ramp = stat(x < 0.2)), fill = clrs[3]) +
  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[3], 0.4), guide = "none")
```

### Stan

```{r}
pi_stan_samples |> 
  spread_draws(pi) |> 
  count(pi < 0.2) |> 
  mutate(prob = n / sum(n))

pi_stan_samples |> 
  spread_draws(pi) |> 
  ggplot(aes(x = pi)) +
  stat_halfeye(aes(fill_ramp = stat(x < 0.2)), fill = clrs[3]) +
  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[3], 0.4), guide = "none")
```

:::

## One-sided tests (probability of direction)

We can also use a hypothesis testing framework and present two competing hypotheses:

$$
\begin{split}
H_0: & \; \; \pi \ge 0.2 \\
H_a: & \; \; \pi < 0.2
\end{split}
$$

We already know the probability of $H_a$ (`r round(post_prob, 3)`), so the probability of $H_0$ is 1 minus tuat, or `r round(1 - post_prob, 3)`. The posterior odds is the ratio of those two probabilities

$$
\text{posterior odds} = \frac{P(H_a \mid Y = 14)}{P(H_0 \mid Y = 14)} = \frac{0.849}{0.151} \approx 5.622
$$

```{r}
post_odds <- post_prob / (1 - post_prob)
post_odds
```


That means that π is ≈6 times more likely to be below 20% than to be above 20%

That's all based on the posterior though. Back before we knew anything, we had a prior of $\operatorname{Beta}(6, 4)$, an in that world, we had a 9% chance that it was true and a 91% chance that it was all false

```{r}
prior_prob <- pbeta(0.2, 4, 6)
prior_prob
1 - prior_prob
```

So the prior odds were only 1 in 10:

```{r}
prior_odds <- prior_prob / (1 - prior_prob)
prior_odds
```

Finally, we can do something more useful with these prior and posterior odds and calculate the **Bayes Factor**, which is just their ratio:

$$
\text{Bayes Factor} = \frac{\text{Posterior odds}}{\text{Prior odds}}
$$

```{r}
BF <- post_odds / prior_odds
BF
```

After learning about 14 Gen X artists, "the posterior odds of our hypothesis … are roughly 60 times higher than the prior odds", which is "fairly convincing"

No significance testing, no failing to reject nulls. Just vibes.

`Evid.Ratio` here is the posterior probability of the hypothesis being true / posterior probability of the hypothesis not being true, or the same as `post_odds` above.

```{r}
h <- hypothesis(model_pi_brms, "Intercept < 0.2")
h
plot(h)
```

If we want the same Bayes Factor ratio that *Bayes Rules!* calculates, we need to use the evidence ratio from brms and calculate `prior_odds` by hand:

```{r}
prior_prob <- pbeta(0.2, 4, 6)
prior_odds <- prior_prob / (1 - prior_prob)

post_odds_brms <- h$hypothesis$Evid.Ratio

BF_brms <- post_odds_brms / prior_odds
BF_brms
```


## Two-sided tests (ROPE stuff)

What if we want to know whether or not 30% of museum artists are Gen X or younger, not just a direction? Now we're dealing with two sides:

$$
\begin{split}
H_0: & \; \; \pi = 0.3 \\
H_a: & \; \; \pi \ne 0.3 \\
\end{split}
$$

We already know the 95% credible interval for π, and 0.3 doesn't really fit well in it:

```{r}
ggplot() +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area",
                fill = colorspace::lighten(clrs[3], 0.9)) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.025, 0.975), 18, 92),
                fill = clrs[3]) +
  geom_vline(xintercept = 0.3) +
  xlim(c(0, 0.4)) +
  labs(x = "π", fill = "Credible interval")
```

That provides us with good evidence that the hypothesis that 30% of artists are Gen X is not correct. It's subtantially outside of the credible interval. But what does substantial mean? We get to define that.

We can be like Kruschke and define a buffer around 0.3, or a region of practical equivalence (ROPE). Here we'll do 0.3±0.05, or between 0.25 and 0.35. We can calculate how much of the posterior is outside of that ROPE.

Since we know the actual posterior is $\operatorname{Beta}(18, 92)$, we can find the percentage of the area of the curve that falls in the ROPE with `pbeta()`:

```{r}
prop_in_rope <- pbeta(0.35, 18, 92) - pbeta(0.25, 18, 92)
prop_in_rope
1 - prop_in_rope
```

98.7% of the posterior is outside of that ROPE. I'd say a value of 30% is pretty substantially far away from the posterior and thus really unlikely.

```{r}
ggplot() +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area",
                fill = colorspace::lighten(clrs[3], 0.9)) +
  stat_function(fun = ~dbeta(., 18, 92), geom = "area", 
                xlim = qbeta(c(0.025, 0.975), 18, 92),
                fill = clrs[3]) +
  annotate(geom = "rect", xmin = 0.25, xmax = 0.35, ymin = -Inf, ymax = Inf, alpha = 0.3) +
  geom_vline(xintercept = 0.3) +
  xlim(c(0, 0.4)) +
  labs(x = "π", fill = "Credible interval")
```

We can do this with the MCMC draws too and we get the same results:

::: {.panel-tabset}
### brms

```{r}
model_pi_brms |> 
  spread_draws(b_Intercept) |> 
  summarize(prop_in_rope = sum(b_Intercept > 0.25 & b_Intercept < 0.35) / n(),
            prop_outside_rope = 1 - prop_in_rope)
```

### Stan

```{r}
pi_stan_samples |> 
  spread_draws(pi) |> 
  summarize(prop_in_rope = sum(pi > 0.25 & pi < 0.35) / n(),
            prop_outside_rope = 1 - prop_in_rope)
```

:::


# 8.3: Posterior prediction

(This stuff is all covered [in my guide here](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/) too)

We get data for 20 more pieces of art at the museum. Based on what we know about π, how many would we predict would be by Gen X artists?

It's reasonable to think 3 (since 20 * 0.16 = 3), but that misses out on two levels of uncertainty:

- Sampling variability in the data - even if π is truly 0.16, the amount we get in the sample will vary just because of randomness
- Posterior variability in π - it could be anywhere between 0.1 and 0.24

The posterior predictive model takes both kinds of uncertainty into account

There's technically a mathy way to get at posterior predictions, and the book covers it, but it's a complicated mess and they even conclude by saying "In this book, we’ll never need to do something like this again"

In the book, the actual posterior predictive probability that 3 of the 20 new artists will be Gen X, based on a posterior that saw 14 (i.e. the model we created), is 0.2217.

We can approximate that exact 0.2217 with the MCMC draws too. With brms models we can use `posterior_predict()`, `posterior_linpred()`, and `posterior_epred()` to extract different types of posterior outcomes on different scales. With raw Stan output, we have to do a little more work ourselves.

::: {.panel-tabset}

### brms

We want to use `predicted_draws()` since that incorporates both kinds of uncertainty, and it returns values that are predicted counts, not probabilities or π ([see my guide for more](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/#tldr-diagrams-and-cheat-sheets))

```{r}
predicted_genx_after_20 <- model_pi_brms |> 
  predicted_draws(newdata = tibble(artworks = 20)) |> 
  group_by(.prediction) |> 
  summarize(n = n()) |> 
  mutate(prop = n / sum(n))
predicted_genx_after_20

ggplot(predicted_genx_after_20, aes(x = factor(.prediction), y = prop)) + 
  geom_col()

# Posterior predictive probability that 3/20 will be Gen X is roughly the same
# as 0.2217!
predicted_genx_after_20 |> 
  filter(.prediction == 3) |> 
  pull(prop)
```

We can also get the variability in just π if we wanted by using `linpred_draws()`:

```{r}
model_pi_brms |> 
  linpred_draws(newdata = tibble(artworks = 20)) |> 
  ungroup() |> 
  ggplot(aes(x = .linpred)) +
  stat_halfeye()
```

And if we use `epred_draws()`, we'll get the expected number of Gen X artworks:

```{r}
model_pi_brms |> 
  epred_draws(newdata = tibble(artworks = 20)) |> 
  ungroup() |> 
  ggplot(aes(x = .epred)) +
  stat_halfeye()
```

Lovely.

### Stan

Raw Stan requires a little more work. We could theoretically use Stan to generate posterior predictions with a `generated quantities` block:

``` stan
generated quantities {
  vector[1000] num_genx_rep;

  for (i in 1:1000) {
    num_genx_rep[i] = binomial_rng(20, pi);
  }
}
```

But that requires either hard-coding two numbers into the Stan code: 1000 for the number of simulations and 20 for the number of new artworks. If we want to change any of those, we'd have to recompile, which is tedious.

Alternatively, we could add a couple variables to the `data` block and pass them through R:

``` stan
data {
  // other variables
  int<lower=1> n_sims;
  int<lower=1> new_artworks;
}

// other blocks

generated quantities {
  vector[n_sims] num_genx_rep;

  for (i in 1:n_sims) {
    num_genx_rep[i] = binomial_rng(new_artworks, pi);
  }
}
```

We'd then need to include values for those new variables in the list of data we pass to Stan:

```{r eval=FALSE}
pi_stan_samples <- model_pi_stan$sample(
  data = list(artworks = 100, num_genx = 14, new_artworks = 20, n_sims = 1000),
  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, 
  refresh = 0, seed = BAYES_SEED
)
```

That would work great and the results from Stan would include 1000 predictions for the number of Gen X artists. But it feels a little excessive to keep rerunning the original 14-artworks model over and over for different numbers of new artworks.

So instead we can use R to build the posterior predictions, since we have all the posterior values of π in the MCMC chains, and since all we're really doing with Stan is using Stan's version of `rbinom()` anyway (`binomial_rng()`).

```{r}
predicted_genx_after_20_stan <- pi_stan_samples |> 
  spread_draws(pi) |> 
  mutate(.prediction = rbinom(n(), size = 20, prob = pi)) |> 
  group_by(.prediction) |> 
  summarize(n = n()) |> 
  mutate(prop = n / sum(n))
predicted_genx_after_20_stan

ggplot(predicted_genx_after_20_stan, aes(x = factor(.prediction), y = prop)) + 
  geom_col()

# Posterior predictive probability that 3/20 will be Gen X is roughly the same
# as 0.2217!
predicted_genx_after_20_stan |> 
  filter(.prediction == 3) |> 
  pull(prop)
```

:::

I haven't figured out a way to get `posterior_linpred()` and `posterior_epred()` with raw Stan like this though. :(


# 8.4: Posterior analysis with MCMC

Oh ha, this whole section shows how to do everything above with Stan, but I already did that above with both brms and raw Stan, so just, um look up there ↑.
