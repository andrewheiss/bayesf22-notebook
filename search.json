[
  {
    "objectID": "bayes-rules/02-chapter.html",
    "href": "bayes-rules/02-chapter.html",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import article data\ndata(fake_news)\n\nperc <- scales::label_percent(accuracy = 1)\nperc2 <- scales::label_percent(accuracy = 0.01)\nHow many are fake vs. real?\n60/150 or 40% of news articles are fake .\nHow is the use of exclamation marks distributed across fake and real news articles?\n16/60 or 26.67% of news articles with !s are fake; only 2/90 or 2.22% of real articles with have !s.\nOur prior is thus that 40% of news articles are fake. We have new data, that !s are more common in fake news articles. So what’s the posterior if we find an article with a !?"
  },
  {
    "objectID": "bayes-rules/02-chapter.html#conditional-probabilities",
    "href": "bayes-rules/02-chapter.html#conditional-probabilities",
    "title": "2: Bayes’ Rule",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nWe have two variables:\n\nFake vs. real status\nUse of exclamation points\n\nThese features can vary at random across different articles, so we have to represent that randomness with probabilities models\n\nPrior probability model\nWe know from previous data that 40% are fake and 60% are real. If \\(B\\) means the article is fake, we can write that as\n\\[\nP(B) = 0.40 \\text{ and } P(B^c) = 0.60\n\\]\n\n\nConditional probability\nThe occurrence of !s depends on fakeness. Conditional probabilities of !s and fakeness, where \\(A\\) is the use of an exclamation mark:\n\\[\nP(A \\mid B) = 0.2667 \\text{ and } P(A \\mid B^c) = 0.0222\n\\]\nBy comparing conditional vs. unconditional probabilities, we learn how much \\(B\\) can inform our understanding of \\(A\\).\nAn event \\(A\\) might increase in probability given \\(B\\), like how the probability of joining an orchestra is greater if we know someonw practices daily:\n\\[\nP(\\text{join orchestra} \\mid \\text{practice daily}) > P(\\text{join orchestra})\n\\]\nOr the probability of getting the flu is lower if you know someone washes their hands a lot:\n\\[\nP(\\text{get flu} \\mid \\text{wash hands regularly}) < P(\\text{get flu})\n\\]\n\n\nLikelihood\nLikelihood is kind of like the inverse of probability (not really! just that the order of A and B matters)\n\nIf we know \\(B\\), the conditional probability \\(P(\\cdot \\mid B)\\) lets us compare the probabilities of an unknown event \\(A\\) (or \\(A^c\\)) occurring with \\(B\\), or\n\\[\nP(A \\mid B) \\text{ vs. } P(A^c \\mid B)\n\\]\nIf we know \\(A\\), the likelihood function \\(L(\\cdot \\mid A) = P(A \\mid \\cdot)\\) lets us compare the relative compatibility of data \\(A\\) with events \\(B\\) or \\(B^c\\)\n\\[\nL(B \\mid A) \\text{ vs. } L(B^c \\mid A)\n\\]\n\nWhat this looks like in practice, where \\(A\\) means having an exclamation mark and \\(B\\) means being fake:\n\n# Prior probability\nrow_prior <- fake_news %>% \n  count(type) %>% \n  mutate(prop = n / sum(n)) %>% \n  select(-n) %>% \n  pivot_wider(names_from = type, values_from = prop)\n\n# Likelihood\nrow_likelihood <- fake_news %>% \n  count(type, title_has_excl) %>% \n  pivot_wider(names_from = title_has_excl, values_from = n) %>% \n  mutate(likelihood = `TRUE` / (`TRUE` + `FALSE`)) %>% \n  select(-c(`FALSE`, `TRUE`)) %>%\n  pivot_wider(names_from = type, values_from = likelihood)\n\nbind_cols(Statistic = c(\"Prior probability\", \"Likelihood\"),\n          bind_rows(row_prior, row_likelihood)) %>% \n  mutate(Total = fake + real) %>% \n  rename(`Fake ($B$)` = fake, \n         `Real ($B^c$)` = real) %>% \n  knitr::kable(digits = 3)\n\n\n\n\nStatistic\nFake (\\(B\\))\nReal (\\(B^c\\))\nTotal\n\n\n\n\nPrior probability\n0.400\n0.600\n1.000\n\n\nLikelihood\n0.267\n0.022\n0.289\n\n\n\n\n\n\n\nNormalizing constants\nThe last piece we need is the marginal probability of observing exclamation points across all articles, or \\(P(A)\\), which is the normalizing constant, or \\(P(B)L(B \\mid A) + P(B^c)L(B^c \\mid A)\\)\n\nfake_news %>% \n  count(type, title_has_excl) %>% \n  mutate(prop = n / sum(n)) %>% \n  filter(title_has_excl == TRUE) %>% \n  summarize(normalizing_constant = sum(prop))\n##   normalizing_constant\n## 1                 0.12\n\n\n\nFinal analytical posterior\nThus, given this formula:\n\\[\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\\]\n…we have\n\\[\n\\text{posterior} = \\frac{0.4 \\times 0.2667}{0.12} = 0.889\n\\]"
  },
  {
    "objectID": "bayes-rules/02-chapter.html#simulation",
    "href": "bayes-rules/02-chapter.html#simulation",
    "title": "2: Bayes’ Rule",
    "section": "Simulation",
    "text": "Simulation\nWe can simulate this too\n\nsim_params <- tibble(type = c(\"real\", \"fake\"),\n                     prior = c(0.6, 0.4))\n\nset.seed(1234)\n\nsims <- sample(sim_params$type, size = 10000, prob = sim_params$prior, replace = TRUE) %>% \n  enframe(value = \"type\") %>% \n  mutate(data_model = case_when(type == \"fake\" ~ 0.2667,\n                                type == \"real\" ~ 0.0222)) %>% \n  rowwise() %>% \n  mutate(usage = sample(c(\"no\", \"yes\"), size = 1,\n                        prob = c(1 - data_model, data_model))) %>% \n  ungroup()\n\nsims %>% \n  tabyl(usage, type) %>% \n  adorn_totals(c(\"col\", \"row\"))\n##  usage fake real Total\n##     no 2914 5872  8786\n##    yes 1075  139  1214\n##  Total 3989 6011 10000\n\nggplot(sims, aes(x = type, fill = usage)) +\n  geom_bar(position = position_fill())\n\n\n\n\n# Posterior\nsims %>% \n  filter(usage == \"yes\") %>% \n  count(type) %>% \n  mutate(prop = n / sum(n))\n## # A tibble: 2 × 3\n##   type      n  prop\n##   <chr> <int> <dbl>\n## 1 fake   1075 0.886\n## 2 real    139 0.114"
  },
  {
    "objectID": "bayes-rules/02-chapter.html#chess-simulation",
    "href": "bayes-rules/02-chapter.html#chess-simulation",
    "title": "2: Bayes’ Rule",
    "section": "Chess simulation",
    "text": "Chess simulation\n\nchess <- c(0.2, 0.5, 0.8)\nprior <- c(0.1, 0.25, 0.65)\n\nset.seed(1234)\nchess_sim <- tibble(pi = sample(chess, size = 10000, prob = prior, replace = TRUE)) %>% \n  mutate(y = rbinom(n(), size = 6, prob = pi))\n\nchess_sim %>% \n  count(pi) %>% \n  mutate(prop = n / sum(n))\n## # A tibble: 3 × 3\n##      pi     n   prop\n##   <dbl> <int>  <dbl>\n## 1   0.2   986 0.0986\n## 2   0.5  2523 0.252 \n## 3   0.8  6491 0.649\n\nchess_sim %>% \n  ggplot(aes(x = y)) +\n  stat_count(aes(y = ..prop..)) +\n  facet_wrap(vars(pi))\n## Warning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(prop)` instead."
  },
  {
    "objectID": "bayes-rules/03-chapter.html",
    "href": "bayes-rules/03-chapter.html",
    "title": "3: The Beta-Binomial Bayesian Model",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#beta-prior-model",
    "href": "bayes-rules/03-chapter.html#beta-prior-model",
    "title": "3: The Beta-Binomial Bayesian Model",
    "section": "3.1: Beta prior model",
    "text": "3.1: Beta prior model\n\\[\n\\pi \\sim \\operatorname{Beta}(45, 55)\n\\]\n\nggplot() +\n  geom_function(fun = ~dbeta(., 45, 55), n = 1001) +\n  labs(title = \"dbeta(45, 55)\")\n\nggplot() +\n  geom_function(fun = ~dprop(., mean = 0.45, size = 100), n = 1001) +\n  labs(title = \"dprop(0.45, 100)\")"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#binomial-data-model-and-likelihood",
    "href": "bayes-rules/03-chapter.html#binomial-data-model-and-likelihood",
    "title": "3: The Beta-Binomial Bayesian Model",
    "section": "3.2: Binomial data model and likelihood",
    "text": "3.2: Binomial data model and likelihood\n\\[\nY \\mid \\pi = \\operatorname{Binomial}(50, \\pi)\n\\]"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#beta-posterior-model",
    "href": "bayes-rules/03-chapter.html#beta-posterior-model",
    "title": "3: The Beta-Binomial Bayesian Model",
    "section": "3.3: Beta posterior model",
    "text": "3.3: Beta posterior model\n\\[\n\\begin{aligned}\nY \\mid \\pi &= \\operatorname{Binomial}(50, \\pi) \\\\\n\\pi &\\sim \\operatorname{Beta}(45, 55)\n\\end{aligned}\n\\]\n\nmodel_election <- brm(\n  bf(support | trials(n_in_poll) ~ 0 + Intercept),\n  data = list(support = 30, n_in_poll = 50),\n  family = binomial(link = \"identity\"),\n  prior(beta(45, 55), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n\n\nmodel_election %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clrs[6]) +\n  labs(x = \"π\", title = \"Plausible values of π that could produce\\na poll where 30/50 voters support Michelle\")\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\nmodel_election %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dbeta(., 45, 55), aes(fill = \"Beta(45, 55) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6])"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#milgrams-experiment",
    "href": "bayes-rules/03-chapter.html#milgrams-experiment",
    "title": "3: The Beta-Binomial Bayesian Model",
    "section": "Milgram’s experiment",
    "text": "Milgram’s experiment\n\\[\n\\begin{aligned}\nY \\mid \\pi &= \\operatorname{Binomial}(40, \\pi) \\\\\n\\pi &\\sim \\operatorname{Beta}(1, 10)\n\\end{aligned}\n\\]\n\nmodel_milgram <- brm(\n  bf(obey | trials(participants) ~ 0 + Intercept),\n  data = list(obey = 26, participants = 40),\n  family = binomial(link = \"identity\"),\n  prior(beta(1, 10), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n\n\nmodel_milgram %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dbeta(., 1, 10), aes(fill = \"Beta(1, 10) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(0, 1))"
  },
  {
    "objectID": "bayes-rules/04-practice.html",
    "href": "bayes-rules/04-practice.html",
    "title": "4: Balance and sequentiality in Bayesian Analyses",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)"
  },
  {
    "objectID": "bayes-rules/04-practice.html#section",
    "href": "bayes-rules/04-practice.html#section",
    "title": "4: Balance and sequentiality in Bayesian Analyses",
    "section": "4.9.2",
    "text": "4.9.2\n\nThe local ice cream shop is open until it runs out of ice cream for the day. It’s 2 p.m. and Chad wants to pick up an ice cream cone. He asks his coworkers about the chance (π) that the shop is still open. Their Beta priors for π are below:\n\n\nworker_priors <- tribble(\n  ~coworker, ~prior,\n  \"Kimya\", \"Beta(1, 2)\",\n  \"Fernando\", \"Beta(0.5, 1)\",\n  \"Ciara\", \"Beta(3, 10)\",\n  \"Taylor\", \"Beta(2, 0.1)\"\n)\n\nworker_priors %>% knitr::kable()\n\n\n\n\ncoworker\nprior\n\n\n\n\nKimya\nBeta(1, 2)\n\n\nFernando\nBeta(0.5, 1)\n\n\nCiara\nBeta(3, 10)\n\n\nTaylor\nBeta(2, 0.1)\n\n\n\n\n\n\n4.4: Choice of prior\n\nVisualize and summarize (in words) each coworker’s prior understanding of Chad’s chances to satisfy his ice cream craving.\n\n\nworker_prior_densities <- worker_priors %>% \n  mutate(shapes = str_match_all(prior, \"Beta\\\\((\\\\d+\\\\.?\\\\d*), (\\\\d+\\\\.?\\\\d*)\\\\)\")) %>% \n  mutate(shape1 = map_dbl(shapes, ~as.numeric(.x[2])),\n         shape2 = map_dbl(shapes, ~as.numeric(.x[3]))) %>% \n  mutate(range = list(seq(0, 1, length.out = 1001))) %>% \n  mutate(density = pmap(list(range, shape1, shape2), ~dbeta(..1, ..2, ..3)))\n\nworker_prior_densities %>% \n  unnest(c(range, density)) %>% \n  # Truncate this a little for plotting\n  filter(range <= 0.99, range >= 0.01) %>%\n  ggplot(aes(x = range, y = density, fill = coworker, color = coworker)) +\n  geom_line(size = 1) +\n  geom_area(position = position_identity(), alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\nKimya is relatively uncertain, but leaning towards believing that π is low\nFernando puts heavy weight on very low probabilities of π—moreso than Kimya\nCiara believes that π is clustered around 20%, and no higher than 50%\nTaylor is the most optimistic, believing that π is exceptionally high (like 90+%)\n\n\n\n4.5 & 4.6: Simulating and identifying the posterior\n\nChad peruses the shop’s website. On 3 of the past 7 days, they were still open at 2 p.m.. Complete the following for each of Chad’s coworkers:\n\nsimulate their posterior model;\ncreate a histogram for the simulated posterior; and\nuse the simulation to approximate the posterior mean value of π\n\n\nand\n\nComplete the following for each of Chad’s coworkers:\n\nidentify the exact posterior model of π;\ncalculate the exact posterior mean of π; and\ncompare these to the simulation results in the previous exercise.\n\n\n\nCiara only\n\\[\n\\begin{aligned}\n(Y = 3) \\mid \\pi &= \\operatorname{Binomial}(7, \\pi) \\\\\n\\pi &\\sim \\operatorname{Beta}(3, 10)\n\\end{aligned}\n\\]\n\nBayes RulesGrid approximationbrmsRaw StanExact posterior\n\n\nThe Bayes Rules simulation approach is to simulate all the possibly likelihoods (here 0–7 / 7) and then filter to just choose one (3)\n\npi_ciara <- tibble(pi = rbeta(10000, 3, 10)) %>% \n  mutate(y = rbinom(10000, size = 7, prob = pi))\n\nggplot(pi_ciara, aes(x = pi, y = y)) +\n  geom_point(aes(color = (y == 3)))\n\n\n\n\n\npi_ciara %>% \n  filter(y == 3) %>% \n  summarize(across(pi, lst(mean, median)))\n## # A tibble: 1 × 2\n##   pi_mean pi_median\n##     <dbl>     <dbl>\n## 1   0.302     0.296\n\npi_ciara %>% \n  filter(y == 3) %>% \n  ggplot(aes(x = pi)) +\n  geom_density()\n\n\n\n\n\n\n\npi_grid <- tibble(pi_grid = seq(0, 1, length.out = 1001)) %>%\n  mutate(prior_beta = dbeta(pi_grid, 3, 10)) %>% \n  mutate(likelihood = dbinom(3, size = 7, prob = pi_grid)) %>% \n  mutate(posterior = (likelihood * prior_beta) / sum(likelihood * prior_beta))\n\npi_samples <- pi_grid %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE)\n\npi_samples %>% \n  summarize(across(pi_grid, lst(mean, median)))\n## # A tibble: 1 × 2\n##   pi_grid_mean pi_grid_median\n##          <dbl>          <dbl>\n## 1        0.301          0.295\n\nggplot(pi_samples, aes(x = pi_grid)) +\n  geom_density()\n\n\n\n\n\n\n\nmodel_pi <- brm(\n  bf(days_open | trials(weekdays) ~ 0 + Intercept),\n  data = list(days_open = 3, weekdays = 7),\n  family = binomial(link = \"identity\"),\n  prior(beta(3, 10), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n\n\nmodel_pi %>% \n  spread_draws(b_Intercept) %>%\n  summarize(across(b_Intercept, lst(mean, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 1 × 8\n##   b_Intercept_mean b_Intercept_median     y  ymin  ymax .width .point .interval\n##              <dbl>              <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>    \n## 1            0.298              0.292 0.292 0.110 0.493   0.95 median hdci\n\n\nmodel_pi %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dbeta(., 3, 10), aes(fill = \"Beta(3, 10) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(0, 1))\n\n\n\n\n\n\npi_stan.stan:\n\n// Things coming in from R\ndata {\n  int<lower=0> total_days;  // Possible days open (corresponds to binomial trials)\n  int<lower=0> days_open;  // Outcome variable\n}\n\n// Things to estimate\nparameters {\n  real<lower=0, upper=1> pi;  // Probability the store is open\n}\n\n// Models and distributions\nmodel {\n  // Prior\n  pi ~ beta(3, 10);\n  \n  // Likelihood\n  days_open ~ binomial(total_days, pi);\n  \n  // Internally, these ~ formulas really look like this:\n  //   target += beta_lpdf(pi | 3, 10);\n  //   target += binomial_lpmf(days_open | total_days, pi);\n  // The ~ notation is a lot nicer and maps onto the notation more directly\n}\n\n\nmodel_stan <- rstan::sampling(\n  pi_stan,\n  data = list(days_open = 3, total_days = 7),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4\n)\n\n\nmodel_stan %>% \n  spread_draws(pi) %>% \n  summarize(across(pi, lst(mean, median)))\n## # A tibble: 1 × 2\n##   pi_mean pi_median\n##     <dbl>     <dbl>\n## 1   0.298     0.292\n\n\nmodel_stan %>% \n  spread_draws(pi) %>% \n  ggplot(aes(x = pi)) +\n  stat_halfeye()\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\nFrom equation 3.10 in Bayes Rules!:\n\\[\n\\begin{aligned}\n\\pi \\mid (Y = y) &\\sim \\operatorname{Beta}(\\alpha + y, \\quad \\beta + n - y) \\\\\n\\pi \\mid (Y = 3) &\\sim \\operatorname{Beta}(3 + 3, \\quad 10 + 7 - 3) \\\\\n&\\sim \\operatorname{Beta}(6, 14)\n\\end{aligned}\n\\]\n\nggplot() +\n  geom_function(fun = ~dbeta(., 6, 14))\n\n\n\n\nAnd from equation 3.11 in Bayes Rules!:\n\\[\n\\begin{aligned}\nE(\\pi \\mid Y = y) &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\nE(\\pi \\mid Y = 3) &= \\frac{3 + 3}{3 + 10 + 7} \\\\\n&= \\frac{6}{20} = 0.3 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\operatorname{Mode}(\\pi \\mid Y = y) &= \\frac{\\alpha + y - 1}{\\alpha + \\beta + n - 2} \\\\\n\\operatorname{Mode}(\\pi \\mid Y = 3) &= \\frac{3 + 3 - 1}{3 + 10 + 7 - 2} \\\\\n&= \\frac{5}{18} = 0.27\\bar{7} \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\operatorname{Var}(\\pi \\mid Y = y) &= \\frac{(\\alpha + y)~(\\beta + n - y)}{(\\alpha + \\beta + n)^2~(\\alpha + \\beta + n + 1)} \\\\\n\\operatorname{Var}(\\pi \\mid Y = 3) &= \\frac{(3 + 3)~(10 + 7 - 3)}{(3 + 10 + 7)^2~(3 + 10 + 7 + 1)} \\\\\n&= \\frac{6 \\times 14}{20^2 \\times 21} = \\frac{84}{8,400} = 0.01 \\\\\n\\operatorname{SD}(\\pi \\mid Y = 3) &= \\sqrt{0.01} = 0.1\n\\end{aligned}\n\\]\nVerify using summarize_beta_binomial():\n\nsummarize_beta_binomial(alpha = 3, beta = 10, y = 3, n = 7)\n##       model alpha beta      mean      mode        var        sd\n## 1     prior     3   10 0.2307692 0.1818182 0.01267963 0.1126039\n## 2 posterior     6   14 0.3000000 0.2777778 0.01000000 0.1000000\n\n\n\n\n\n\nAll four coworkers\n(sans raw Stan; brms is fine and great for this anyway)\n\n# Clean this table of coworkers up a bit\npriors_clean <- worker_prior_densities %>% \n  select(coworker, prior, shape1, shape2)\n\npriors_clean %>% \n  knitr::kable()\n\n\n\n\ncoworker\nprior\nshape1\nshape2\n\n\n\n\nKimya\nBeta(1, 2)\n1.0\n2.0\n\n\nFernando\nBeta(0.5, 1)\n0.5\n1.0\n\n\nCiara\nBeta(3, 10)\n3.0\n10.0\n\n\nTaylor\nBeta(2, 0.1)\n2.0\n0.1\n\n\n\n\n\n\nBayes RulesGrid approximationbrmsExact posterior\n\n\n\nbr_simulation_pi <- priors_clean %>% \n  mutate(pi_sim = map2(shape1, shape2, ~{\n    tibble(pi = rbeta(10000, .x, .y)) %>% \n      mutate(y = rbinom(10000, size = 7, prob = pi))\n  }))\n\nbr_simulation_pi %>% \n  unnest(pi_sim) %>% \n  ggplot(aes(x = pi, y = y)) +\n  geom_point(aes(color = (y == 3))) +\n  facet_wrap(vars(coworker))\n\n\n\n\n\nbr_simulation_pi %>% \n  unnest(pi_sim) %>% \n  filter(y == 3) %>% \n  group_by(coworker) %>% \n  summarize(across(pi, lst(mean, median)))\n## # A tibble: 4 × 3\n##   coworker pi_mean pi_median\n##   <chr>      <dbl>     <dbl>\n## 1 Ciara      0.305     0.297\n## 2 Fernando   0.416     0.414\n## 3 Kimya      0.400     0.391\n## 4 Taylor     0.545     0.564\n\nbr_simulation_pi %>% \n  unnest(pi_sim) %>% \n  filter(y == 3) %>% \n  ggplot(aes(x = pi, fill = coworker, color = coworker)) +\n  geom_density(size = 1, alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\nCool cool. Fernando and Kimya’s flatter priors end up leading to posteriors of ≈40%. Taylor’s extreme optimism leads to a posterior mean of 57%! Ciara’s more reasonable range leads to a posterior of 30%.\n\n\n\ngrid_simulation_pi <- priors_clean %>% \n  mutate(pi_grid = list(seq(0.01, 0.99, length.out = 1001))) %>% \n  mutate(prior_beta = pmap(list(pi_grid, shape1, shape2), ~{\n    dbeta(..1, ..2, ..3)\n  })) %>% \n  mutate(likelihood = map(pi_grid, ~{\n    dbinom(3, size = 7, prob = .x)\n  })) %>% \n  mutate(posterior = map2(prior_beta, likelihood, ~{\n    (.y * .x) / sum(.y * .x)\n  })) \n\ngrid_simulation_samples <- grid_simulation_pi %>% \n  unnest(c(pi_grid, posterior)) %>% \n  group_by(coworker) %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE)\n\ngrid_simulation_samples %>% \n  group_by(coworker) %>% \n  summarize(across(pi_grid, lst(mean, median)))\n## # A tibble: 4 × 3\n##   coworker pi_grid_mean pi_grid_median\n##   <chr>           <dbl>          <dbl>\n## 1 Ciara           0.300          0.294\n## 2 Fernando        0.412          0.405\n## 3 Kimya           0.399          0.394\n## 4 Taylor          0.550          0.555\n\ngrid_simulation_samples %>% \n  ggplot(aes(x = pi_grid, fill = coworker, color = coworker)) +\n  geom_density(size = 1, alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\n\n\nbrms_pi <- priors_clean %>% \n  mutate(stan_prior = map2(shape1, shape2, ~{\n    prior_string(glue::glue(\"beta({.x}, {.y})\"), class = \"b\", lb = 0, ub = 1)\n  })) %>% \n  mutate(model = map(stan_prior, ~{\n    brm(\n      bf(days_open | trials(weekdays) ~ 0 + Intercept),\n      data = list(days_open = 3, weekdays = 7),\n      family = binomial(link = \"identity\"),\n      prior = .x,\n      iter = 5000, warmup = 1000, seed = BAYES_SEED,\n      backend = \"rstan\", cores = 4\n    )\n  }))\n## Compiling Stan program...\n## Start sampling\n## Compiling Stan program...\n## Start sampling\n## Compiling Stan program...\n## Start sampling\n## Compiling Stan program...\n## Start sampling\n\n\nbrms_pi %>% \n  mutate(draws = map(model, ~spread_draws(., b_Intercept))) %>% \n  unnest(draws) %>% \n  group_by(coworker) %>% \n  summarize(across(b_Intercept, lst(mean, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 4 × 9\n##   coworker b_Intercept_mean b_Intercep…¹     y  ymin  ymax .width .point .inte…²\n##   <chr>               <dbl>        <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>  \n## 1 Ciara               0.298        0.292 0.292 0.110 0.493   0.95 median hdci   \n## 2 Fernando            0.411        0.403 0.403 0.125 0.726   0.95 median hdci   \n## 3 Kimya               0.399        0.392 0.392 0.130 0.693   0.95 median hdci   \n## 4 Taylor              0.547        0.552 0.552 0.258 0.850   0.95 median hdci   \n## # … with abbreviated variable names ¹​b_Intercept_median, ²​.interval\n\n\nbrms_pi %>% \n  mutate(draws = map(model, ~gather_draws(., b_Intercept))) %>% \n  unnest(draws) %>% \n  ggplot(aes(x = .value, fill = coworker)) +\n  stat_halfeye(alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\n\n\nlik_y <- 3\nlik_n <- 7\n\nposteriors_manual <- priors_clean %>% \n  mutate(posterior_shape1 = shape1 + lik_y,\n         posterior_shape2 = shape2 + lik_n - lik_y)\n\nposteriors_manual %>% \n  mutate(posterior = glue::glue(\"Beta({posterior_shape1}, {posterior_shape2})\")) %>% \n  select(coworker, prior, posterior) %>% \n  knitr::kable()\n\n\n\n\ncoworker\nprior\nposterior\n\n\n\n\nKimya\nBeta(1, 2)\nBeta(4, 6)\n\n\nFernando\nBeta(0.5, 1)\nBeta(3.5, 5)\n\n\nCiara\nBeta(3, 10)\nBeta(6, 14)\n\n\nTaylor\nBeta(2, 0.1)\nBeta(5, 4.1)\n\n\n\n\n\n\nposteriors_manual_plot <- posteriors_manual %>% \n  mutate(range = list(seq(0.01, 0.99, length.out = 1001))) %>% \n  mutate(density = pmap(list(range, posterior_shape1, posterior_shape2), ~dbeta(..1, ..2, ..3)))\n\nposteriors_manual_plot %>% \n  unnest(c(range, density)) %>% \n  ggplot(aes(x = range, y = density, fill = coworker, color = coworker)) +\n  geom_line(size = 1) +\n  geom_area(position = position_identity(), alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\nposteriors_manual_summary <- posteriors_manual %>% \n  group_by(coworker) %>% \n  summarize(summary = map2(shape1, shape2, ~{\n    summarize_beta_binomial(alpha = .x, beta = .y, y = lik_y, n = lik_n)\n  })) %>% \n  unnest(summary) \n\nposteriors_manual_summary %>% \n  select(-coworker) %>% \n  kableExtra::kbl(digits = 3) %>% \n  kableExtra::pack_rows(index = table(posteriors_manual_summary$coworker)) %>% \n  kableExtra::kable_styling()\n\n\n\n \n  \n    model \n    alpha \n    beta \n    mean \n    mode \n    var \n    sd \n  \n \n\n  Ciara\n\n    prior \n    3.0 \n    10.0 \n    0.231 \n    0.182 \n    0.013 \n    0.113 \n  \n  \n    posterior \n    6.0 \n    14.0 \n    0.300 \n    0.278 \n    0.010 \n    0.100 \n  \n  Fernando\n\n    prior \n    0.5 \n    1.0 \n    0.333 \n    1.000 \n    0.089 \n    0.298 \n  \n  \n    posterior \n    3.5 \n    5.0 \n    0.412 \n    0.385 \n    0.025 \n    0.160 \n  \n  Kimya\n\n    prior \n    1.0 \n    2.0 \n    0.333 \n    0.000 \n    0.056 \n    0.236 \n  \n  \n    posterior \n    4.0 \n    6.0 \n    0.400 \n    0.375 \n    0.022 \n    0.148 \n  \n  Taylor\n\n    prior \n    2.0 \n    0.1 \n    0.952 \n    1.000 \n    0.015 \n    0.121 \n  \n  \n    posterior \n    5.0 \n    4.1 \n    0.549 \n    0.563 \n    0.025 \n    0.157"
  },
  {
    "objectID": "bayes-rules/05-chapter.html",
    "href": "bayes-rules/05-chapter.html",
    "title": "5: Conjugate families",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\nConjugate families are neat because they let us calculate exact posteriors without difficult integration, like the beta binomial trick:\n\\[\n\\pi \\mid (Y = y) \\sim \\operatorname{Beta}(\\alpha + y, \\quad \\beta + n - y)\n\\]"
  },
  {
    "objectID": "bayes-rules/05-chapter.html#gamma-poisson-conjugate-family",
    "href": "bayes-rules/05-chapter.html#gamma-poisson-conjugate-family",
    "title": "5: Conjugate families",
    "section": "5.2 Gamma-Poisson conjugate family",
    "text": "5.2 Gamma-Poisson conjugate family\nUseful for modeling rates and counts, like fraudulent phone calls per day.\n\nRate of calls per day = \\(\\lambda\\). Any positive value.\nNumber of calls per day = \\(Y_i\\). Any non-negative integer.\n\n\nPoisson distributions\nSee this from program evaluation too, where I give up and say\n\nI have absolutely zero mathematical intuition for how [\\(\\lambda\\)] works. The two shape parameters for a Beta distribution at least fit in a fraction and you can wrap your head around that, but the lambda in a Poisson distribution is just a mystery to me.\n\nIn general, as the rate of events \\(\\lambda\\) increases…\n\nthe typical number of events increases,\nthe variability increases, and\nthe skew decreases\n\n\nexpand_grid(y = 0:12, lambda = c(1, 2, 5)) %>% \n  mutate(density = dpois(y, lambda)) %>% \n  ggplot(aes(x = y, y = density)) +\n  geom_col() + \n  facet_wrap(vars(lambda), \n             labeller = as_labeller(function(x) glue::glue(\"Poisson(λ = {x})\")))\n\n\n\n\n\n\nGamma distributions\nGamma models are positive and right skewed, and conjugate to Poisson. They take two hyperparameters: \\(s\\) (shape) and \\(r\\) (rate). Exponential models are Gammas with \\(s = 1\\).\n\nexpand_grid(y = seq(0, 7, length.out = 1001), s = c(1, 2, 4), r = c(1, 2)) %>% \n  mutate(density = dgamma(y, shape = s, rate = r)) %>% \n  mutate(panel_name = glue::glue(\"Gamma(s = {s}, r = {r})\"),\n         panel_name = fct_inorder(panel_name)) %>% \n  ggplot(aes(x = y, y = density)) +\n  geom_area() + \n  facet_wrap(vars(panel_name), dir = \"v\", nrow = 2)\n\n\n\n\nTuning the gamma prior: We think the rate of calls is 5 a day, with a range of 2–7ish. Through trial and error, it looks like \\(\\lambda \\sim \\operatorname{Gamma}(10, 2)\\) fits that well:\n\nggplot() +\n  geom_function(fun = ~dgamma(., shape = 10, rate = 2)) +\n  xlim(c(0, 15)) +\n  labs(x = \"λ\")\n\n\n\n\n\n\nGamma-Poisson conjugacy\nGamma and Poisson families work together like Beta and binomial!\nModel:\n\\[\n\\begin{aligned}\nY_i \\mid \\lambda &\\stackrel{\\text{ind}}{\\sim} \\operatorname{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\operatorname{Gamma}(s, r)\n\\end{aligned}\n\\]\nAnd the magical posterior based on the two distributions’ conjugacy:\n\\[\n\\lambda \\mid y \\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n)\n\\]\nAnd summary statistics:\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{s}{r} \\\\\n\\operatorname{Mode}(\\lambda) &= \\frac{s - 1}{r} \\\\\n\\operatorname{Var}(\\lambda) &= \\frac{s}{r^2} \\\\\n\\operatorname{SD}(\\lambda) &= \\sqrt{\\frac{s}{r^2}}\n\\end{aligned}\n\\]\nTime to try it!\nWe’ll assume that the daily rate of calls \\(\\lambda\\) is distributed with \\(\\operatorname{Gamma}(10, 2)\\). Over 4 days, we receive 6, 2, 2, and 1 calls. That’s an \\(n\\) of 4 and a \\(\\sum y_i\\) of (6 + 2 + 2 + 1), or 11, and an average of \\(\\frac{11}{4}\\), or 2.75.\nThe posterior model is then the \\(\\operatorname{Gamma}(10, 2)\\) prior mixed with the likelihood in fancy conjugate-y ways:\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (6, 2, 2, 1) &\\sim \\operatorname{Gamma}(10 + 11,\\quad 2 + 4) \\\\\n&\\sim \\operatorname{Gamma}(21, 6)\n\\end{aligned}\n\\]\nThis new data changes our understanding of the rate of calls per day:\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{10}{2} = 5 \\text{ calls a day, from prior} \\\\\nE[\\lambda \\mid (6, 2, 2, 1)] &= \\frac{21}{6} = 3.5 \\text{ calls a day, from posterior}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nSD(\\lambda) &= \\sqrt{\\frac{10}{2^2}} = 1.581 \\\\\nSD[\\lambda \\mid (6, 2, 2, 1)] &= \\sqrt{\\frac{21}{6^2}} = 0.764\n\\end{aligned}\n\\]\nAnd here’s what that looks like:\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 10, rate = 2), \n                geom = \"area\", aes(fill = \"Gamma(10, 2) prior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = (10 + 11), rate = (2 + 4)),\n                geom = \"area\", aes(fill = \"Gamma(21, 6) posterior\"), alpha = 0.75) +\n  xlim(c(0, 15)) +\n  scale_fill_manual(values = clrs[5:6])\n\n\n\n\nAnd confirming with the summarize_gamma_poisson() helper function:\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4)\n##       model shape rate mean     mode       var        sd\n## 1     prior    10    2  5.0 4.500000 2.5000000 1.5811388\n## 2 posterior    21    6  3.5 3.333333 0.5833333 0.7637626\n\nAnd confirming with brms, just because this conjugate prior stuff feels like dark magic:\n\nmodel_rate <- brm(\n  bf(fraud_calls ~ 0 + Intercept),\n  data = list(fraud_calls = c(6, 2, 2, 1)),\n  family = poisson(link = \"identity\"),\n  prior = prior(gamma(10, 2), class = b, lb = 0),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n\n\nmodel_rate %>% \n  spread_draws(b_Intercept) %>%\n  summarize(across(b_Intercept, lst(mean, sd, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 1 × 9\n##   b_Intercept_mean b_Intercept…¹ b_Int…²     y  ymin  ymax .width .point .inte…³\n##              <dbl>         <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>  \n## 1             3.50         0.756    3.44  3.44  2.09  5.00   0.95 median hdci   \n## # … with abbreviated variable names ¹​b_Intercept_sd, ²​b_Intercept_median,\n## #   ³​.interval\n\n\nmodel_rate %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dgamma(., 10, 2), aes(fill = \"Gamma(10, 2) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(0, 15))\n\n\n\n\nAHHH it works!"
  },
  {
    "objectID": "bayes-rules/05-chapter.html#normal-normal-conjugate-family",
    "href": "bayes-rules/05-chapter.html#normal-normal-conjugate-family",
    "title": "5: Conjugate families",
    "section": "5.3: Normal-Normal conjugate family",
    "text": "5.3: Normal-Normal conjugate family\nGeneral story here: we’re interested in \\(\\mu\\)m or the average volume of the hippocampus. Wikipedia says that one half is between 3–3.5 cm3, so the total volume is between 6–7 cm3.\n\nNormal distributions\nNormal distribution defined with \\(\\mu\\) and \\(\\sigma\\) (I’ve got this intuition, but I’ll plot it anyway):\n\nexpand_grid(y = seq(-2, 10, length.out = 1001), \n            params = list(list(mu = 2, sigma = 0.5), \n                          list(mu = 2, sigma = 1),\n                          list(mu = 4, sigma = 2))) %>%\n  mutate(density = map2_dbl(y, params, ~dnorm(.x, .y$mu, .y$sigma))) %>% \n  mutate(panel_name = map_chr(params, ~glue::glue(\"N({.x$mu}, {.x$sigma})\"))) %>% \n  ggplot(aes(x = y, y = density)) +\n  geom_area() +\n  facet_wrap(vars(panel_name))\n\n\n\n\n\n\nNormal prior\nSo, if we think the volume of the hippocampus is 6.5 cm3, ± 0.8, we can do 6.5 ± (2 * 0.4), or:\n\\[\n\\mu \\sim \\mathcal{N}(6.5, 0.4^2)\n\\]\nHere’s what that looks like:\n\nggplot() +\n  geom_function(fun = ~dnorm(., mean = 6.5, sd = 0.4)) +\n  xlim(c(5, 8))\n\n\n\n\n\n\nNormal-normal conjugacy\nNormal-normal situations are conjugates, which means we can find exact posteriors without complex integration. This is a little more complicated than the nice easy Beta-binomial or even the Gamma-Poisson conjugates, though.\nHere’s the model:\n\\[\n\\begin{aligned}\nY_i \\mid \\mu &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}({\\mu, \\sigma^2}) \\\\\n\\mu &\\sim \\mathcal{N}(\\theta, \\tau^2)\n\\end{aligned}\n\\]\nAnd the magical posterior:\n\\[\n\\mu \\mid \\vec{y} \\; \\sim \\;  \\mathcal{N}\\bigg(\\theta\\frac{\\sigma^2}{n\\tau^2+\\sigma^2} + \\bar{y}\\frac{n\\tau^2}{n\\tau^2+\\sigma^2}, \\; \\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2}\\bigg)\n\\]\nWow that’s a mess. We need these things:\n\nPrior mean (\\(\\theta\\))\nPrior sd (\\(\\tau\\))\nObserved mean (\\(\\bar{y}\\))\nObserved sd (\\(\\sigma\\))\nNumber of observations (\\(n\\))\n\nLet’s try it with real data, with the football data from bayesrules. What’s the average hippocampus volume for football players with concussions? This is our \\(\\bar{y}\\).\n\nconcussion_subjects <- bayesrules::football %>% \n  filter(group == \"fb_concuss\")\n\nconcussion_subjects %>% \n  summarize(across(volume, lst(mean, sd)))\n##   volume_mean volume_sd\n## 1      5.7346 0.5933976\n\nIn the book, they look at the distribution and figure that a standard deviation of 0.5 seems reasonable (and it’s basically that in the data too)\n\nconcussion_subjects %>% \n  ggplot(aes(x = volume)) +\n  geom_density()\n\n\n\n\nWith that, we have all these pieces:\n\nPrior mean (\\(\\theta\\)): 6.5\nPrior sd (\\(\\tau\\)): 0.4\nObserved mean (\\(\\bar{y}\\)): 5.735\nObserved (assumed) sd (\\(\\sigma\\)): 0.5\nNumber of observations (\\(n\\)): 25\n\nMath time!\n\\[\n\\begin{aligned}\n\\mu \\mid \\vec{y} \\; &\\sim \\; \\mathcal{N}\\bigg(\\theta\\frac{\\sigma^2}{n\\tau^2+\\sigma^2} + \\bar{y}\\frac{n\\tau^2}{n\\tau^2+\\sigma^2}, \\; \\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2}\\bigg) \\\\\n&\\sim \\; \\mathcal{N}\\bigg(6.5\\frac{0.5^2}{25 \\times 0.4^2+0.5^2} + 5.735\\frac{25 \\times 0.4^2}{25 \\times 0.4^2+0.5^2}, \\; \\frac{0.4^2 \\times 0.5^2}{25 \\times 0.4^2+0.5^2}\\bigg) \\\\\n&\\sim \\; \\mathcal{N}\\bigg(5.78, 0.009^2\\bigg)\n\\end{aligned}\n\\]\nOr, with the summarize_normal_normal() helper function:\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,\n                        y_bar = 5.735, n = 25)\n##       model mean mode         var         sd\n## 1     prior 6.50 6.50 0.160000000 0.40000000\n## 2 posterior 5.78 5.78 0.009411765 0.09701425\n\nAnd here’s what that looks like:\n\nggplot() +\n  stat_function(fun = ~dnorm(., mean = 6.5, sd = 0.4), \n                geom = \"area\", aes(fill = \"N(6.5, 0.4) prior\"), alpha = 0.75,\n                n = 1001) +\n  stat_function(fun = ~dnorm(., mean = 5.78, sd = 0.097),\n                geom = \"area\", aes(fill = \"N(5.78, 0.097) posterior\"), alpha = 0.75,\n                n = 1001) +\n  xlim(c(5, 8)) +\n  scale_fill_manual(values = clrs[6:5], guide = guide_legend(reverse = TRUE))\n\n\n\n\nAnd confirming with brms:\n\nmodel_volume <- brm(\n  bf(volume ~ 0 + Intercept),\n  data = concussion_subjects,\n  family = gaussian(),\n  prior = prior(normal(6.5, 0.4), class = b),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n\n\nmodel_volume %>% \n  spread_draws(b_Intercept) %>%\n  summarize(across(b_Intercept, lst(mean, sd, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 1 × 9\n##   b_Intercept_mean b_Intercept…¹ b_Int…²     y  ymin  ymax .width .point .inte…³\n##              <dbl>         <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>  \n## 1             5.80         0.124    5.80  5.80  5.57  6.05   0.95 median hdci   \n## # … with abbreviated variable names ¹​b_Intercept_sd, ²​b_Intercept_median,\n## #   ³​.interval\n\n\nmodel_volume %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dnorm(., 6.5, 0.4), aes(fill = \"N(6.5, 0.4) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(5, 8))\n\n\n\n\nSO COOL.\n\n\nOverriding the observed sd with an assumed sd\nBUT that’s not actually correct because it’s using the actual observed standard deviation (0.5934) instead of the assumed standard deviation (0.5) from the conjugate calculation earlier. I can’t figure out how to override brms’s sd, but we can use raw Stan:\nnormal_normal.stan:\n\ndata {\n  int<lower = 1> N;  // Number of observations\n  vector[N] volume;  // Observed hippocampus volumes\n  real volume_sd;    // Assumed sd of hippocampus volumes\n}\n\nparameters {\n  real mu;  // Posterior average hippocampus volume\n}\n\nmodel {\n  // Prior\n  mu ~ normal(6.5, 0.4);\n\n  // Likelihood\n  volume ~ normal(mu, volume_sd);\n}\n\n\nmodel_volume_stan <- rstan::sampling(\n  normal_normal,\n  data = list(volume = concussion_subjects$volume, \n              volume_sd = 0.5,\n              N = nrow(concussion_subjects)),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4\n)\n\nThe results are basically identical to the math-based version!\n\nmodel_volume_stan %>% \n  spread_draws(mu) %>%\n  summarize(across(mu, lst(mean, sd, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(mu_hdci)\n## # A tibble: 1 × 9\n##   mu_mean  mu_sd mu_median     y  ymin  ymax .width .point .interval\n##     <dbl>  <dbl>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>    \n## 1    5.78 0.0978      5.78  5.78  5.59  5.98   0.95 median hdci\n\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,\n                        y_bar = 5.735, n = 25)\n##       model mean mode         var         sd\n## 1     prior 6.50 6.50 0.160000000 0.40000000\n## 2 posterior 5.78 5.78 0.009411765 0.09701425\n\nAnd the distribution is the same too:\n\nmodel_volume_stan %>% \n  gather_draws(mu) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dnorm(., 6.5, 0.4), aes(fill = \"N(6.5, 0.4) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(5, 8))"
  },
  {
    "objectID": "bayes-rules/05-practice.html",
    "href": "bayes-rules/05-practice.html",
    "title": "5: Conjugate families",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234"
  },
  {
    "objectID": "bayes-rules/05-practice.html#practice-gamma-poisson",
    "href": "bayes-rules/05-practice.html#practice-gamma-poisson",
    "title": "5: Conjugate families",
    "section": "Practice: Gamma-Poisson",
    "text": "Practice: Gamma-Poisson\n\n5.1: Tuning a Gamma prior\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{s}{r} \\\\\n\\operatorname{Mode}(\\lambda) &= \\frac{s - 1}{r} \\\\\n\\operatorname{Var}(\\lambda) &= \\frac{s}{r^2}\n\\end{aligned}\n\\]\n\nThe most common value of λ is 4, and the mean is 7.\n\\[\n\\begin{cases}\n\\frac{s}{r} = 7 & [\\text{Mean}] \\\\\n\\frac{s-1}{r} = 4 & [\\text{Mode}] \\\\\n\\end{cases}\n\\]\nSolving the system of equations gives:\n\\[\n\\begin{aligned}\nr &= \\frac{1}{3} \\\\\ns &= \\frac{7}{3}\n\\end{aligned}\n\\]\nOr with code:\n\neq_mean <- function(r) 7 * r\neq_mode <- function(r) (4 * r) + 1\n\nr <- uniroot(function(x) eq_mean(x) - eq_mode(x), c(0, 100))$root\ns <- eq_mean(r)\n\nr; s\n## [1] 0.3333333\n## [1] 2.333333\n\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 7/3, rate = 1/3), geom = \"area\",\n                fill = clrs[2]) +\n  geom_vline(xintercept = 4, linetype = \"21\", color = clrs[6]) +\n  annotate(geom = \"label\", x = 4, y = 0.01, label = \"Mode\", \n           fill = clrs[6], color = \"white\") +\n  geom_vline(xintercept = 7, linetype = \"23\", color = clrs[3]) +\n  annotate(geom = \"label\", x = 7, y = 0.01, label = \"Mean\", \n           fill = clrs[3], color = \"white\") +\n  xlim(c(0, 20))\n\n\n\n\n\nThe most common value of λ is 10 and the mean is 12.\n\\[\n\\begin{cases}\n\\frac{s}{r} = 12 & [\\text{Mean}] \\\\\n\\frac{s-1}{r} = 10 & [\\text{Mode}] \\\\\n\\end{cases}\n\\]\nSolving the system of equations gives:\n\\[\n\\begin{aligned}\nr &= \\frac{1}{2} \\\\\ns &= 6\n\\end{aligned}\n\\]\nOr with code:\n\neq_mean <- function(r) 12 * r\neq_mode <- function(r) (10 * r) + 1\n\nr <- uniroot(function(x) eq_mean(x) - eq_mode(x), c(0, 100))$root\ns <- eq_mean(r)\n\nr; s\n## [1] 0.5\n## [1] 6\n\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 6, rate = 0.5), geom = \"area\",\n                fill = clrs[2]) +\n  geom_vline(xintercept = 10, linetype = \"21\", color = clrs[6]) +\n  annotate(geom = \"label\", x = 10, y = 0.01, label = \"Mode\", \n           fill = clrs[6], color = \"white\") +\n  geom_vline(xintercept = 12, linetype = \"23\", color = clrs[3]) +\n  annotate(geom = \"label\", x = 12, y = 0.02, label = \"Mean\", \n           fill = clrs[3], color = \"white\") +\n  xlim(c(0, 40))\n\n\n\n\n\nThe most common value of λ is 5, and the variance is 3.\n\\[\n\\begin{cases}\n\\frac{s-1}{r} = 5 & [\\text{Mode}] \\\\\n\\frac{s}{r^2} = 3 & [\\text{Variance}] \\\\\n\\end{cases}\n\\]\nSolving the system of equations gives:\n\\[\n\\begin{aligned}\nr &= \\frac{5 \\pm \\sqrt{37}}{6} \\\\\ns &= \\frac{31 \\pm 5\\sqrt{37}}{6}\n\\end{aligned}\n\\]\nlol k\n\nr <- (5 + sqrt(37)) / 6\ns <- (31 + 5*sqrt(37)) / 6\n\nr; s\n## [1] 1.847127\n## [1] 10.23564\n\nYep, it works:\n\ntable(round(rgamma(10000, shape = s, rate = r)))\n## \n##    1    2    3    4    5    6    7    8    9   10   11   12   13 \n##    2  149  845 1871 2427 2108 1338  692  355  143   42   22    6\nvar(rgamma(10000, shape = s, rate = r))\n## [1] 3.013767\n\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = s, rate = r), geom = \"area\",\n                fill = clrs[2]) +\n  geom_vline(xintercept = 5, linetype = \"21\", color = clrs[6]) +\n  annotate(geom = \"label\", x = 5, y = 0.01, label = \"Mode\",\n           fill = clrs[6], color = \"white\") +\n  xlim(c(0, 12))\n\n\n\n\n\nAnd so on… I get the point :)\n\n\n5.2: Poisson likelihood functions\nFrom equation 5.6 in Bayes Rules:\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{\\sum y_i}e^{-n\\lambda}\n\\]\nIn code:\n\nL <- function(lambda, sum_y, n) lambda^sum_y * exp(-n * lambda)\n\n(3, 7, 19)\n\nobserved_data <- c(3, 7, 19)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 3\n\\(\\sum y_i\\) = 29\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{29} e^{-3\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 20))\n\n\n\n\n\n(12, 12, 12, 0)\n\nobserved_data <- c(12, 12, 12, 0)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 4\n\\(\\sum y_i\\) = 36\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{36} e^{-4\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 20))\n\n\n\n\n\n(12)\n\nobserved_data <- c(12)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 1\n\\(\\sum y_i\\) = 12\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{12} e^{-\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 30))\n\n\n\n\n\n(16, 10, 17, 11, 11)\n\nobserved_data <- c(16, 10, 17, 11, 11)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 5\n\\(\\sum y_i\\) = 65\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{65} e^{-5\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 20))\n\n\n\n\n\n\n5.3: Gamma-Poisson posteriors\nPrior:\n\\[\n\\lambda \\sim \\operatorname{Gamma}(24, 2)\n\\]\nModel:\n\\[\n\\begin{aligned}\nY_i \\mid \\lambda &\\stackrel{\\text{ind}}{\\sim} \\operatorname{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\operatorname{Gamma}(s, r)\n\\end{aligned}\n\\]\nConjugate posterior:\n\\[\n\\lambda \\mid y \\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n)\n\\]\n(3, 7, 19)\n\nobserved_data <- c(3, 7, 19)\n\n\n\\(n\\) = 3\n\\(\\sum y_i\\) = 29\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (3, 7, 19) &\\sim \\operatorname{Gamma}(24 + 29,\\quad 2 + 3) \\\\\n&\\sim \\operatorname{Gamma}(53, 5)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (3, 7, 19)] &= \\frac{53}{5} = 10.6 \\text{ from posterior}\n\\end{aligned}\n\\]\n\n(12, 12, 12, 0)\n\nobserved_data <- c(12, 12, 12, 0)\n\n\n\\(n\\) = 4\n\\(\\sum y_i\\) = 36\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (12, 12, 12, 0) &\\sim \\operatorname{Gamma}(24 + 36,\\quad 2 + 4) \\\\\n&\\sim \\operatorname{Gamma}(60, 6)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (12, 12, 12, 0)] &= \\frac{60}{6} = 10 \\text{ from posterior}\n\\end{aligned}\n\\]\n\n(12)\n\nobserved_data <- c(12)\n\n\n\\(n\\) = 1\n\\(\\sum y_i\\) = 12\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (12) &\\sim \\operatorname{Gamma}(24 + 12,\\quad 2 + 1) \\\\\n&\\sim \\operatorname{Gamma}(36, 3)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (12)] &= \\frac{36}{3} = 12 \\text{ from posterior}\n\\end{aligned}\n\\]\n\n(16, 10, 17, 11, 11)\n\nobserved_data <- c(16, 10, 17, 11, 11)\n\n\n\\(n\\) = 5\n\\(\\sum y_i\\) = 65\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (16, 10, 17, 11, 11) &\\sim \\operatorname{Gamma}(24 + 65,\\quad 2 + 5) \\\\\n&\\sim \\operatorname{Gamma}(89, 7)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (16, 10, 17, 11, 11)] &= \\frac{89}{7} = 12.714 \\text{ from posterior}\n\\end{aligned}\n\\]\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 24, rate = 2), \n                geom = \"area\", aes(fill = \"Gamma(24, 2) prior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 53, rate = 5),\n                geom = \"area\", aes(fill = \"Gamma(53, 5) posterior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 60, rate = 6),\n                geom = \"area\", aes(fill = \"Gamma(60, 6) posterior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 36, rate = 3),\n                geom = \"area\", aes(fill = \"Gamma(36, 3) posterior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 89, rate = 7),\n                geom = \"area\", aes(fill = \"Gamma(89, 7) posterior\"), alpha = 0.75) +\n  xlim(c(5, 25)) +\n  scale_fill_manual(values = clrs[1:5])\n\n\n\n\n\n\n5.5 & 5.6: Text messages"
  },
  {
    "objectID": "bayes-rules/05-practice.html#practice-normal-normal",
    "href": "bayes-rules/05-practice.html#practice-normal-normal",
    "title": "5: Conjugate families",
    "section": "Practice: Normal-Normal",
    "text": "Practice: Normal-Normal\n\n5.8: Normal likelihood functions\n\n\n5.9 & 5.10: Investing in stock\n\n\n5.11: Normal-normal calculation\n\n\n5.12: Control brains"
  },
  {
    "objectID": "bayes-rules/06-chapter.html",
    "href": "bayes-rules/06-chapter.html",
    "title": "6: Approximating the posterior",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234"
  },
  {
    "objectID": "bayes-rules/06-chapter.html#grid-approximation",
    "href": "bayes-rules/06-chapter.html#grid-approximation",
    "title": "6: Approximating the posterior",
    "section": "6.1 Grid approximation",
    "text": "6.1 Grid approximation\n\nBeta-binomial example\n\\[\n\\begin{aligned}\nY &\\sim \\operatorname{Binomial}(10, π) \\\\\n\\pi &= \\operatorname{Beta}(2, 2)\n\\end{aligned}\n\\]\nWe can figure this posterior out mathematically using conjugate priors. If we know that Y = 9, then\n\\[\n\\pi \\mid (Y = 9) \\sim \\operatorname{Beta}(2 + 9, 10 - 2 + 2) \\rightarrow \\operatorname{Beta}(11, 3)\n\\]\nBut we can also do this with grid approximation:\n\n# Create a grid of 6 pi values\ngrid_data <- tibble(pi_grid = seq(0, 1, length.out = 6)) |> \n  # Evaluate the prior and likelihood at each pi\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid)) |> \n  # Approximate the posterior\n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\ngrid_data\n## # A tibble: 6 × 5\n##   pi_grid prior likelihood unnormalized posterior\n##     <dbl> <dbl>      <dbl>        <dbl>     <dbl>\n## 1     0    0    0            0          0        \n## 2     0.2  0.96 0.00000410   0.00000393 0.0000124\n## 3     0.4  1.44 0.00157      0.00226    0.00712  \n## 4     0.6  1.44 0.0403       0.0580     0.183    \n## 5     0.8  0.96 0.268        0.258      0.810    \n## 6     1    0    0            0          0\n\nggplot(grid_data, aes(x = pi_grid, y = posterior)) +\n  geom_point() +\n  geom_segment(aes(xend = pi_grid, yend = 0))\n\n\n\n\nThat’s our discretized posterior, but since there are only 6 values, it’s not great. If we sample from it, the samples will be just 0.6, 0.8, etc.:\n\nposterior_samples <- grid_data |> \n  slice_sample(n = 10000, replace = TRUE, weight_by = posterior)\n\nposterior_samples |> count(pi_grid)\n## # A tibble: 3 × 2\n##   pi_grid     n\n##     <dbl> <int>\n## 1     0.4    56\n## 2     0.6  1809\n## 3     0.8  8135\n\nLet’s compare that to the actual \\(\\operatorname{Beta}(11, 3)\\) posterior:\n\nggplot(posterior_samples, aes(x = pi_grid)) +\n  geom_histogram(aes(y = ..density..), color = \"white\", binwidth = 0.1, boundary = 0) +\n  stat_function(fun = ~dbeta(., 11, 3)) +\n  xlim(c(0, 1))\n## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(density)` instead.\n\n\n\n\nlol\nHere’s the same grid approximation with 10,000 grid values this time:\n\ngrid_data <- tibble(pi_grid = seq(0, 1, length.out = 10000)) |> \n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid)) |> \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n# Actual approximated posterior\nggplot(grid_data, aes(x = pi_grid, y = posterior)) +\n  geom_line()\n\n\n\n\n# Samples from the posterior\nposterior_samples <- grid_data |> \n  slice_sample(n = 10000, replace = TRUE, weight_by = posterior)\n\nggplot(posterior_samples, aes(x = pi_grid)) +\n  geom_histogram(aes(y = ..density..), color = \"white\", binwidth = 0.01, boundary = 0) +\n  stat_function(fun = ~dbeta(., 11, 3)) +\n  xlim(c(0, 1))\n\n\n\n\n\n\nGamma-Poisson example\n\\[\n\\begin{aligned}\nY_i &\\sim \\operatorname{Poisson}(\\lambda) \\\\\n\\lambda &= \\operatorname{Gamma}(3, 1)\n\\end{aligned}\n\\]\nIf we see Y = 2 and then Y = 8, our true posterior based on conjugate family magic ends up being this:\n\\[\n\\lambda \\mid Y = (2, 8) \\sim \\operatorname{Gamma}(3 + (2 + 8), 1 + 2) \\rightarrow \\operatorname{Gamma}(13, 3)\n\\]\nGrid time:\n\ngrid_data <- tibble(lambda_grid = seq(0, 15, length.out = 501)) |> \n  mutate(prior = dgamma(lambda_grid, 3, 1),\n         likelihood = dpois(2, lambda_grid) * dpois(8, lambda_grid)) |> \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n# Actual approximated posterior\nggplot(grid_data, aes(x = lambda_grid, y = posterior)) +\n  geom_line()\n\n\n\n\n# Samples from the posterior\nposterior_samples <- grid_data |> \n  slice_sample(n = 10000, replace = TRUE, weight_by = posterior)\n\nggplot(posterior_samples, aes(x = lambda_grid)) +\n  geom_histogram(aes(y = ..density..), color = \"white\", binwidth = 0.5, boundary = 0) +\n  stat_function(fun = ~dgamma(., 13, 3)) +\n  xlim(c(0, 15))\n\n\n\n\nLovely."
  },
  {
    "objectID": "bayes-rules/06-chapter.html#markov-chains-via-rstan",
    "href": "bayes-rules/06-chapter.html#markov-chains-via-rstan",
    "title": "6: Approximating the posterior",
    "section": "6.2 Markov chains via rstan",
    "text": "6.2 Markov chains via rstan\nMCMC samples aren’t independent—each value depends on the previous value (hence “chains”). But you only need to know one previous value of \\(\\theta\\) to calculate the next \\(\\theta\\), so there’s no long history or anything. Also, the chain of \\(\\theta\\) values aren’t even simulated from the posterior. But with magical MCMC algorithms, we can approximate the posterior with the values in the chains\n\nBeta-binomaial\nLet’s do this model again, but with Stan instead of with grid approximation:\n\\[\n\\begin{aligned}\nY &\\sim \\operatorname{Binomial}(10, π) \\\\\n\\pi &= \\operatorname{Beta}(2, 2)\n\\end{aligned}\n\\]\n\n\n\n06-stan/bb_sim.stan\n\n// Step 1: Define the model\n// Stuff from R\ndata {\n  int<lower=0, upper=10> Y;\n}\n\n// Thing to estimate\nparameters {\n  real<lower=0, upper=1> pi;\n}\n\n// Prior and likelihood\nmodel {\n  Y ~ binomial(10, pi);\n  pi ~ beta(2, 2);\n}\n\n\n\nbb_sim <- cmdstan_model(\"06-stan/bb_sim.stan\")\n\n\n# Step 2: Simulate the posterior\n# Compiled cmdstan objects are R6 objects with functions embedded in specific\n# slots, which makes it hard to look them up in the documentation. ?CmdStanModel\n# shows an index of all the available methods, like $sample()\nbb_sim_samples <- bb_sim$sample(\n  data = list(Y = 9),\n  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\n\n# cmdstan samples are also R6 objects with embedded functions. $draws() lets you\n# extract the draws as an array\nbb_sim_samples$draws(variables = \"pi\") |> head(4)\n## # A draws_array: 4 iterations, 4 chains, and 1 variables\n## , , variable = pi\n## \n##          chain\n## iteration    1    2    3    4\n##         1 0.89 0.75 0.71 0.87\n##         2 0.94 0.87 0.56 0.82\n##         3 0.88 0.89 0.59 0.82\n##         4 0.89 0.81 0.67 0.76\n\n# Or we can use posterior::as_draws_array to avoid R6\n# as_draws_array(bb_sim_samples)\n\n# Or even better, use tidybayes\nbb_sim_samples |>\n  spread_draws(pi) |> \n  head(4)\n## # A tibble: 4 × 4\n##   .chain .iteration .draw    pi\n##    <int>      <int> <int> <dbl>\n## 1      1          1     1 0.888\n## 2      1          2     2 0.938\n## 3      1          3     3 0.877\n## 4      1          4     4 0.892\n\nThe values in these chains aren’t independent. In chain 1 here, for instance, it starts with 0.89, then plugs that into the next iteration to get 0.94, then plugs that into the next iteration to get 0.88, then plugs that into the next iteration to get 0.89, and so on.\nThe chain explores the sample space, or range of posterior plausible \\(\\pi\\)s. We want them to explore lots of values along their journey, and we can check that by looking at traceplots (to show the history of the chain) and density plots (to show the distribution of values that were visited)\n\nTrace plot, first few iterationsFull trace plot, separateFull trace plot, mixed\n\n\n\n# Look at the first 20, for fun\nbb_sim_samples |>\n  gather_draws(pi) |> \n  filter(.iteration <= 20) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 1) +\n  facet_grid(rows = vars(.variable), cols = vars(.chain)) +\n  labs(color = \"Chain\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nbb_sim_samples |>\n  gather_draws(pi) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.1) +\n  facet_grid(rows = vars(.variable), cols = vars(.chain)) +\n  labs(color = \"Chain\")\n\n\n\n\n\n\n\nbb_sim_samples |>\n  gather_draws(pi) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.1) +\n  labs(color = \"Chain\")\n\n\n\n\n\n\n\nAnd here’s the distribution of the draws, which should be the same as the numeric \\(\\operatorname{Beta}(11, 3)\\) posterior:\n\nbb_sim_samples |>\n  spread_draws(pi) |> \n  ggplot(aes(x = pi)) +\n  stat_density(geom = \"area\", fill = clrs[1]) +\n  stat_function(fun = ~dbeta(., 11, 3), color = clrs[3], size = 1)\n\n\n\n\n\n\nGamma-Poisson\n\\[\n\\begin{aligned}\nY_i &\\sim \\operatorname{Poisson}(\\lambda) \\\\\n\\lambda &= \\operatorname{Gamma}(3, 1)\n\\end{aligned}\n\\]\n\n\n\n06-stan/gp_sim.stan\n\n// Step 1: Define the model\n// Stuff from R\ndata {\n  array[2] int Y;\n}\n\n// Thing to estimate\nparameters {\n  real<lower=0, upper=15> lambda;\n}\n\n// Prior and likelihood\nmodel {\n  Y ~ poisson(lambda);\n  lambda ~ gamma(3, 1);\n}\n\n\n\ngp_sim <- cmdstan_model(\"06-stan/gp_sim.stan\")\n\n\n# Step 2: Simulate the posterior\ngp_sim_samples <- gp_sim$sample(\n  data = list(Y = c(2, 8)),\n  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\nCheck the chains:\n\ngp_sim_samples |>\n  gather_draws(lambda) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.1) +\n  labs(color = \"Chain\")\n\n\n\n\nCompare the distribution with the true \\(\\operatorname{Gamma}(13, 3)\\) posterior:\n\ngp_sim_samples |>\n  spread_draws(lambda) |> \n  ggplot(aes(x = lambda)) +\n  stat_density(geom = \"area\", fill = clrs[2]) +\n  stat_function(fun = ~dgamma(., 13, 3), color = clrs[3], size = 1)"
  },
  {
    "objectID": "bayes-rules/06-chapter.html#markov-chain-diagnostics",
    "href": "bayes-rules/06-chapter.html#markov-chain-diagnostics",
    "title": "6: Approximating the posterior",
    "section": "6.3: Markov chain diagnostics",
    "text": "6.3: Markov chain diagnostics\n\n6.3.1: Examining trace plots\nTrace plots should look like nothing (“hairy caterpillars”). This indicates that the chains are stable, well-mixed, and converged:\n\ngp_sim_samples |>\n  gather_draws(lambda) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.1) +\n  labs(color = \"Chain\")\n\n\n\n\nWe can also use trace rank plots (trank plots), where we take all the samples for a parameter (\\(\\lambda\\) here), calculate their ranks, and make a histogram of those ranks colored by chain. According to McElreath (p. 284),\n\nIf the chains are exploring the same space efficinetly, the histograms should be similar to one another and largely overlapping.\n\nNeat!\n\ngp_sim_samples |>\n  spread_draws(lambda) |> \n  mutate(draw_rank = rank(lambda)) |> \n  ggplot(aes(x = draw_rank)) +\n  stat_bin(aes(color = factor(.chain)), geom = \"step\", binwidth = 500, \n           position = position_identity(), boundary = 0) + \n  labs(color = \"Chain\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\nHere are some bad traceplots from Figure 6.12 in the book:\n\n\n\n\n\n\n\n\n\nChain A has a noticeable slope, which is a sign that it hasn’t stabilized. It hasn’t found a good range of possible \\(\\pi\\) values. It is mixing slowly.\nChain B gets stuck when exploring smaller values of \\(\\pi\\)\nFix these issues by (1) making sure the model and priors are appropriate, and (2) run the chain for more iterations.\n\n\n6.3.2 Comparing parallel chains\nWe want to see consistency across the four chains. Check with a density plot:\n\ngp_sim_samples |>\n  spread_draws(lambda) |> \n  ggplot(aes(x = lambda, color = factor(.chain))) +\n  geom_density() +\n  labs(color = \"Chain\")\n\n\n\n\n\n\n6.3.3. Effective sample size and autocorrelation\nSince there are so many MCMC draws, it’s tricky to know what the actual sample size is. “How many [truly] independent sample values would it take to produce an equivalently accurate posterior approximation?” That’s what the effective sample size ratio is for:\n\\[\n\\frac{N_\\text{effective}}{N}\n\\]\nThere’s no official rule for this, but it would be bad if a chain had a ratio of less than 0.1, or where the effective sample size is less than 10% of the actual sample size.\nFor both of these models the ratio is 34ish%, which means “our 20,000 Markov chain values are about as useful as only 6800 independent samples (0.34 × 20000).”\n\nbayesplot::neff_ratio(bb_sim_samples)\n##        pi \n## 0.3423651\nbayesplot::neff_ratio(gp_sim_samples)\n##    lambda \n## 0.3456354\n\nThe bayesplot::neff_ratio() uses the ess_basic summary statistic, which Aki Vehtari says is fine here. We can also extract the ESS basic statistic with posterior::ess_basic():\n\nposterior::ess_basic(bb_sim_samples$draws(variables = \"pi\"))\n## [1] 3401.823\n\nHowever, in the documentation for ess_basic(), the Stan team strongly recommends using either ess_bulk or ess_tail, both of which are reported by default in summary() (and also in rstanarm and brms models):\n\nposterior::ess_bulk(bb_sim_samples$draws(variables = \"pi\")) / 10000\n## [1] 0.3114404\nposterior::ess_tail(bb_sim_samples$draws(variables = \"pi\")) / 10000\n## [1] 0.3960722\nbb_sim_samples$summary() |> \n  select(variable, mean, median, ess_bulk, ess_tail)\n## # A tibble: 2 × 5\n##   variable   mean median ess_bulk ess_tail\n##   <chr>     <dbl>  <dbl>    <dbl>    <dbl>\n## 1 lp__     -7.80  -7.52     4526.    4831.\n## 2 pi        0.785  0.800    3140.    3991.\n\nWe can also look at autocorrelation. There’s inherently some degree of autocorrelation, since each draw depends on the previous one, but we still want draws to bounce around and to not be too correlated after a few lagged periods.\nWe can check this with an autocorrelation plot. This shows the correlation between an MCMC draw and the one before it at different lags. When the lag is 0, there’s perfect correlation (since it’s the correlation between the draw and itself). At lag 1, there’s a correlation of 0.5 between a draw and its previous value, and it drops off to near 0 by the time we get to 5 lags. That’s good.\n\n[T]here’s very little correlation between Markov chain values that are more than a few steps apart. This is all good news. It’s more confirmation that our Markov chain is mixing quickly, i.e., quickly moving around the range of posterior plausible π values, and thus at least mimicking an independent sample.\n\n\n# Boring bayesplot way\n# mcmc_acf(bb_sim_samples$draws(variables = \"pi\"))\n\nautocor_manual <- bb_sim_samples |>\n  spread_draws(pi) |> \n  group_by(.chain) |> \n  nest() |> \n  summarize(autocor = map(data, ~{\n    x <- acf(.$pi, plot = FALSE, lag.max = 20)\n    tibble(lag = x$lag, acf = x$acf)\n  })) |>\n  unnest(autocor)\n\nggplot(autocor_manual, aes(x = lag, y = acf, color = factor(.chain))) +\n  geom_line() +\n  scale_x_continuous(breaks = 0:20) +\n  labs(x = \"Lag\", y = \"Autocorrelation\", color = \"Chain\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\nFinally, we can look at \\(\\hat{R}\\) or R-hat. R-hat looks at the consistency of values across chains:\n\nR-hat addresses this consistency by comparing the variability in sampled π values across all chains combined to the variability within each individual chain\n\n\\[\n\\hat{R} \\approx \\sqrt{\\frac{\\operatorname{Variability}_\\text{combined}}{\\operatorname{Variability}_\\text{within}}}\n\\]\nWe want R-hat to be 1. When R-hat > 1, it means there’s instability across chains, and more specifically that “the variability in the combined chains exceeds that within the chains. R-hat > 1.05 is bad (and the Stan people have recently considered thinking about 1.01 as a possible warning sign, and proposed alternative mixing statistics, like R*).\nBasically we want the variability across the chains to look just like the variability within the chains so that it’s impossible to distinguish between them in a trace plot. Can you see any rogue chains here? Nope. We’re good.\n\nrhat_basic(bb_sim_samples$draws(variables = \"pi\"))\n## [1] 1.000178\n\n\nbb_sim_samples |>\n  gather_draws(pi) |> \n  ggplot(aes(x = .iteration, y = .value, group = .chain)) +\n  geom_line(size = 0.1) +\n  labs(color = \"Chain\")"
  },
  {
    "objectID": "bayes-rules/07-chapter.html",
    "href": "bayes-rules/07-chapter.html",
    "title": "7: MCMC under the Hood",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/07-chapter.html#the-big-idea",
    "href": "bayes-rules/07-chapter.html#the-big-idea",
    "title": "7: MCMC under the Hood",
    "section": "7.1: The Big Idea",
    "text": "7.1: The Big Idea\n(SO EXCITED FOR THIS)\nStan uses Hamiltonian Monte Carlo sampling; JAGS uses Gibbs sampling. Both of these are enhanced versions of the fundamental Metropolis-Hastings algorithm for sampling, which we’ll implement here (yay!)\nThink of Markov chains as a tour around the range of posterior possible values of a parameter (like µ or π or whatever). The chains move around that parameter and hopefully converge around it, but the chains need a tour manager to do that properly.\nTrace plots show the tour route; density plots show the relative amount of time spent at each stop or parameter region during the tour.\nThe tour manager’s goal is “to ensure that the density of tour stops in each μ region is proportional to its posterior plausibility”\nWe can automate the tour managing process with an algorithm, like the Metropolis-Hastings algorithm, which consists of two steps.\nAssume the Markov chain is at location \\(\\mu^{(i)}\\) currently. In order to choose the next tour stop, or \\(\\mu^{(i + 1)}\\), follow this process:\n\nPropose a random location for the next tour stop: \\(\\mu^\\prime\\)\nDecide whether to go to \\(\\mu^\\prime\\) or stay at the current location \\(\\mu^{(i)}\\) for another iteration\n\nThat’s it. This simplified special version of Metropolis-Hastings is called the Monte Carlo algorithm.\nHere’s how to implement it. Assume we have a posterior (calculated with magical conjugate prior families) like this:\n\\[\n\\mu \\sim \\mathcal{N}(4, 0.6^2)\n\\]\nWe can draw random values from that distribution and tour it:\n\nmc_tour <- tibble(\n  mu = rnorm(5000, mean = 4, sd = 0.6)\n) |> \n  mutate(.iteration = 1:n())\n\n# Trace plot\nmc_tour |> \n  ggplot(aes(x = .iteration, y = mu)) +\n  geom_line(size = 0.1, alpha = 0.75)\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n# Density plot\nmc_tour |> \n  ggplot(aes(x = mu)) +\n  geom_histogram(aes(y = ..density..),binwidth = 0.25, \n                  color = \"white\", fill = clrs[2]) +\n  geom_function(fun = ~dnorm(., 4, 0.6), color = clrs[3])\n## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(density)` instead.\n\n\n\n\nNeat! The trace plot shows that the tour was stable and had good coverage; the density plot shows that most of the time was spent around 4.\nBut this Monte Carlo algorithm is way too easy. We already know the posterior here! MCMC is great for approximating the posterior when math is too hard; if we can get the posterior through conjugate magic, there’s no need to then randomly sample and tour the posterior.\nSo what do we do if we don’t know the true posterior? We do know some of the posterior—the whole point of Bayes’ rule is that the posterior is proportional to the prior and the likelihood:\n\\[\n\\begin{aligned}\n\\text{Posterior} &\\propto \\text{Prior} \\times \\text{Likelihood} \\\\\nf(\\mu \\mid y = 6.25) &\\propto f(\\mu) \\times L(\\mu \\mid y = 6.25)\n\\end{aligned}\n\\]\n\nplot_data <- tibble(mu = seq(1, 7, length.out = 101)) |> \n  mutate(likelihood = dnorm(6.25, mean = mu, sd = 0.75),\n         prior = dnorm(mu, 0, 1)) |> \n  mutate(unnormalized = likelihood * prior)\n\nggplot(plot_data, aes(x = mu, y = unnormalized)) + \n  geom_area(fill = clrs[1]) +\n  labs(x = \"µ\", y = NULL,\n       title = \"Unnormalized posterior distribution\",\n       subtitle = \"Prior × L(µ | y = 6.25)\")\n\n\n\n\nThis distribution isn’t quite correct—it’s not scaled correctly—but it does “preserve the shape, central tendency, and variability of the actual posterior”. So we know something about the posterior that we can work with, and that can influence the sampling procedure.\nMetropolis-Hastings also does another neat thing. Instead of just choosing a new stop at random, it uses a proposal model to propose possible stops. There are lots of possible proposal models—in Bayes Rules they use a uniform proposal model with a half-width parameter \\(w\\) that adds a window, or range, or neighborhood around the current µ location in the chain, or the current stop.\nSo if we’re currently at \\(\\mu^{(i)}\\), the proposal for the next stop will be drawn from a window of \\(\\mu^{(i)} \\pm w\\), or more formally:\n\\[\n\\mu^\\prime \\sim \\operatorname{Uniform}(\\mu^{(i)} - w, \\mu^{(i)} + w)\n\\]\nIf we’re currently at \\(\\mu = 3\\), for instance, and we’re using a half-width \\(w\\) of 1, the proposal for the next draw will come from runif(n = 1, min = 2, max = 4):\n\nggplot(plot_data, aes(x = mu, y = unnormalized)) + \n  geom_area(fill = clrs[1]) +\n  labs(x = \"µ\", y = NULL,\n       title = \"Unnormalized posterior distribution\",\n       subtitle = \"Prior × L(µ | y = 6.25)\") +\n  scale_x_continuous(breaks = 1:7) +\n  annotate(geom = \"segment\", x = 2, xend = 4, y = 2e-7, yend = 2e-7) + \n  annotate(geom = \"segment\", x = 3, xend = 3, y = 2e-7, yend = 0, linetype = \"21\")\n\n\n\n\nThe second step in the algorithm then decides if the proposal should be accepted or rejected. If the proposed \\(\\mu^\\prime\\) is bad, the chain will hang out for a round before making another proposal, checking if it’s good, and then maybe moving on.\nIf we’re currently at 3 and we draw from \\(\\operatorname{Uniform}(2, 4)\\) and get a 3.8, is that good? Yeah. That fits nicely in the unnormalized posterior distribution, so we should go there. If the uniform distribution proposes a 2, that’s probably not great—that’s really rare."
  },
  {
    "objectID": "bayes-rules/07-chapter.html#the-metropolis-hastings-algorithm",
    "href": "bayes-rules/07-chapter.html#the-metropolis-hastings-algorithm",
    "title": "7: MCMC under the Hood",
    "section": "7.2: The Metropolis-Hastings algorithm",
    "text": "7.2: The Metropolis-Hastings algorithm\nSo in general, Metropolis-Hastings does this:\n\nPropose a random location for the next tour stop, \\(\\mu^\\prime\\), by drawing it from a proposal model\nDecide whether to go to \\(\\mu^\\prime\\) or stay at the current location \\(\\mu^{(i)}\\) for another iteration, based on this process:\n\n\nIf the unnormalized posterior plausibility of \\(\\mu^\\prime > \\mu^{(i)}\\), then definitely go there\nOtherwise maybe go there\n\nThat “maybe go there” part is determined based on an acceptance probability \\(\\alpha\\). It’s like a weighted coin flip—if it’s heads, which has probability \\(\\alpha\\), go there; if it’s tails, which has probability \\(1 - \\alpha\\), stay:\n\\[\n\\mu^{(i+1)} =\n\\begin{cases}\n\\mu^\\prime & \\text{ with probability } \\alpha \\\\\n\\mu & \\text{ with probability } 1 - \\alpha\n\\end{cases}\n\\]\nFormally, the weightedness of this coin flip (or the acceptance model in general) comes from the Metropolis algorithm, which has a symmetric proposal model. It looks like this:\n\\[\n\\alpha = \\min\\left\\lbrace 1, \\; \\frac{f(\\mu^\\prime)\\ L(\\mu^\\prime \\mid y)}{f(\\mu)\\ L(\\mu \\mid y)} \\right\\rbrace\n\\]\nOof that’s a mess. Through some algebra (dividing the numerator and denominator by \\(f(y)\\)), we get this:\n\\[\n\\alpha = \\min\\left\\lbrace 1, \\; \\frac{f(\\mu^\\prime)\\ L(\\mu^\\prime \\mid y)\\ /\\ f(y)}{f(\\mu)\\ L(\\mu \\mid y)\\ /\\ f(y)} \\right\\rbrace = \\min\\left\\lbrace 1, \\; \\frac{f(\\mu^\\prime \\mid y)}{f(\\mu \\mid y)} \\right\\rbrace\n\\]\nThe key part is \\(f(\\mu^\\prime \\mid y)\\) and \\(f(\\mu \\mid y)\\), or how well \\(\\mu\\) and \\(\\mu^\\prime\\) fit in the unnormalized posterior. We look at the ratio of their plausibilities:\n\\[\n\\frac{\\text{Plausibility of proposed } \\mu^\\prime \\text{ in unnormalized posterior}}{\\text{Plausibility of current } \\mu \\text{ in unnormalized posterior}}\n\\]\nThere are two possible outcomes with this ratio:\n\nIf the plausibility of the proposed draw is ≥ the current draw (\\(f(\\mu^\\prime \\mid y) \\geq f(\\mu \\mid y)\\)), \\(\\alpha\\) will be 1, since the ratio will be > 1 (like 0.5 / 0.3 would be 1.666; 4 / 3.9 would be 1.02; and so on). The decision rule says to take the minimum of 1 and the plausibility ratio, so here the minimum is 1 and so \\(\\alpha\\) is 1. This is the “Definitely go there” part of the algorithm.\nIf the plausibility of the proposed draw is < the current draw (\\(f(\\mu^\\prime \\mid y) \\lt f(\\mu \\mid y)\\)), then the ratio will be less than 1 (like 0.3 / 0.5 would be 0.6; 3.9 / 4 would be 0.975; and so on), so \\(\\alpha\\) would be that ratio and not 1 (again, we’re taking the minimum of the two). That ratio then becomes our probability of going to the new draw. The closer the proposed plausibility is to the current plausibility, the higher the chances of visiting there. This is the “Maybe go there” part of the algorithm.\n\nWe can write this algorithm with code:\n\none_mh_iteration <- function(w, current) {\n  # Step 1\n  # Propose the next mu by choosing a value from a uniform distribution with a\n  # window of ±w around the current mu\n  proposal <- runif(1, min = current - w, max = current + w)\n  \n  # Step 2\n  # Decide whether or not to go there\n  # The plausibility is the prior * likelihood\n  proposal_plausibility <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)\n  current_plausibility <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)\n  \n  # Calculate the alpha, taking the minimum of 1 or the ratio of plausiblities\n  alpha <- min(1, proposal_plausibility / current_plausibility)\n  \n  # Determine the next stop based on the alpha\n  next_stop <- sample(c(proposal, current), size = 1, prob = c(alpha, 1 - alpha))\n  \n  return(tibble(proposal, alpha, next_stop))\n}\n\nSo if we’re currently at 3, let’s figure out a next stop with a window of ±1:\n\nset.seed(8)\none_mh_iteration(w = 1, current = 3)\n## # A tibble: 1 × 3\n##   proposal alpha next_stop\n##      <dbl> <dbl>     <dbl>\n## 1     2.93 0.824      2.93\n\nThis iteration proposes going to 2.93. The ratio of plausibilities is less than one here, so we get an alpha of 0.82. The probability of deciding to move from 3 to 2.93 is thus also 0.82. In this case, we accept the proposal and decide to move on, so the next stop is 2.93.\nIn this case, if we’re at 3 and we get a proposal of 2.018, the ratio is really small (0.02), so the probability when deciding to “maybe move on” is tiny. We end up staying at 3.\n\nset.seed(83)\none_mh_iteration(w = 1, current = 3)\n## # A tibble: 1 × 3\n##   proposal  alpha next_stop\n##      <dbl>  <dbl>     <dbl>\n## 1     2.02 0.0171         3"
  },
  {
    "objectID": "bayes-rules/07-chapter.html#implementing-the-metropolis-hastings-algorithm",
    "href": "bayes-rules/07-chapter.html#implementing-the-metropolis-hastings-algorithm",
    "title": "7: MCMC under the Hood",
    "section": "7.3: Implementing the Metropolis-Hastings algorithm",
    "text": "7.3: Implementing the Metropolis-Hastings algorithm\nThat’s the hard part! The rest involves just looping through a bunch of values. We’ll do that with a function:\n\nmh_tour <- function(start = 3, N, w) {\n  # Step 1\n  # Start the chain at some location\n  current <- start\n  \n  # Step 2\n  # Create an empty vector with enough slots to hold all these draws\n  mu <- rep(0, N)\n  \n  # Step 3\n  # Markov chain time\n  for (i in 1:N) {\n    # Do one iteration\n    where_next <- one_mh_iteration(w = w, current = current)\n    \n    # Save this in the vector\n    mu[i] <- where_next$next_stop\n    \n    # Tell the algorithm where we are now\n    current <- where_next$next_stop\n  }\n  \n  # Step 4\n  # Return all the chain values\n  # (use tidybayes name conventions)\n  return(tibble(.iteration = c(1:N), mu))\n}\n\nHere we go! Let’s make a chain with 5000 values with a uniform proposal model with a window \\(w\\) of ±1:\n\nset.seed(84735)\ntour_1 <- mh_tour(start = 3, N = 5000, w = 1)\n\n# Trace plot\ntour_1 |> \n  ggplot(aes(x = .iteration, y = mu)) +\n  geom_line(size = 0.1, alpha = 0.75)\n\n\n\n\n# Density plot\ntour_1 |> \n  ggplot(aes(x = mu)) +\n  geom_histogram(aes(y = ..density..),binwidth = 0.25, \n                  color = \"white\", fill = clrs[2]) +\n  geom_function(fun = ~dnorm(., 4, 0.6), color = clrs[3])\n\n\n\n\nAHHHHHHH THAT’S AWESOME!!!!!!!!!11!!1!1!!\n\nFun times with parallel chains\nFor bonus fun and excitement, I’ll do four parallel chains with 4 random starting values drawn from \\(\\operatorname{Uniform}(0, 10)\\) (because why not)\n\nlibrary(furrr)\nplan(multisession, workers = 4)\n\nset.seed(123)\nfour_tours <- tibble(.chain = 1:4) |> \n  mutate(start = runif(n(), 0, 10)) |> \n  mutate(tour = future_map(start, ~mh_tour(start = ., N = 5000, w = 1),\n                           .options = furrr_options(seed = TRUE)))\nfour_tours\n## # A tibble: 4 × 3\n##   .chain start tour                \n##    <int> <dbl> <list>              \n## 1      1  2.88 <tibble [5,000 × 2]>\n## 2      2  7.88 <tibble [5,000 × 2]>\n## 3      3  4.09 <tibble [5,000 × 2]>\n## 4      4  8.83 <tibble [5,000 × 2]>\n\nLOOK AT THIS TRACE PLOT\n\nfour_tours |> \n  unnest(tour) |> \n  filter(.chain < 2500) |> \n  ggplot(aes(x = .iteration, y = mu, color = factor(.chain))) +\n  geom_line(size = 0.1, alpha = 0.75) +\n  labs(color = \"Chain\")\n\n\n\n\nLOOK AT THIS TRANK PLOT\n\nfour_tours |> \n  unnest(tour) |> \n  mutate(draw_rank = rank(mu)) |> \n  ggplot(aes(x = draw_rank)) +\n  stat_bin(aes(color = factor(.chain)), geom = \"step\", binwidth = 500,\n           position = position_identity(), boundary = 0) +\n  labs(color = \"Chain\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\nLOOK AT THE DENSITIES ACROSS CHAINS\n\n# Density plot\nfour_tours |> \n  unnest(tour) |> \n  ggplot(aes(x = mu, color = factor(.chain))) +\n  geom_density() +\n  labs(color = \"Chain\")\n\n\n\n\nThis even works with posterior functions like ess_basic. Our effective sample size ratio isn’t that great—our 20,000 Markov chain values are as useful as only 2,400 independent samples(0.12 × 20000).\n\nposterior_mu <- four_tours |> unnest(tour) |> pull(mu)\nposterior::ess_basic(posterior_mu)\n## [1] 2379.859\n\n# neff_ratio\nposterior::ess_basic(posterior_mu) / 20000\n## [1] 0.118993\n\nAnd we can look at autocorrelation, which isn’t great. It takes a while for the chains to settle and the chains have a longer memory—we don’t hit 0ish correlation until 13 lags!\n\nautocor_manual <- four_tours |> \n  unnest(tour) |> \n  group_by(.chain) |> \n  nest() |> \n  summarize(autocor = map(data, ~{\n    x <- acf(.$mu, plot = FALSE, lag.max = 20)\n    tibble(lag = x$lag, acf = x$acf)\n  })) |>\n  unnest(autocor)\n\nggplot(autocor_manual, aes(x = lag, y = acf, color = factor(.chain))) +\n  geom_line() +\n  scale_x_continuous(breaks = 0:20) +\n  labs(x = \"Lag\", y = \"Autocorrelation\", color = \"Chain\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\nFinally, here’s the overall simulated posterior, after throwing away the first 2,500 draws in each chain as warmups (just like we do with Stan):\n\nfour_tours |> \n  unnest(tour) |> \n  filter(.iteration > 2500) |> \n  ggplot(aes(x = mu)) +\n  geom_density(aes(fill = \"Simulated posterior\"), color = FALSE) +\n  geom_function(fun = ~dnorm(., 4, 0.6), aes(color = \"True posterior\")) +\n  scale_fill_manual(values = c(clrs[2]), name = NULL) +\n  scale_color_manual(values = c(clrs[3]), name = NULL)\n\n\n\n\nSO SO COOL!"
  },
  {
    "objectID": "bayes-rules/07-chapter.html#tuning-the-metropolis-hastings-algorithm",
    "href": "bayes-rules/07-chapter.html#tuning-the-metropolis-hastings-algorithm",
    "title": "7: MCMC under the Hood",
    "section": "7.4: Tuning the Metropolis-Hastings algorithm",
    "text": "7.4: Tuning the Metropolis-Hastings algorithm\nPhew. One final wrinkle to play with. We arbitrarily set \\(w = 1\\) here for the window, which defines how wide the neighborhood of possible proposals is. That \\(w\\) term matters a lot—too wide and we’ll bounce around way too much and never settle; too narrow and we’ll get stuck around the initial value.\nThis \\(w\\) is equivalent to the step size.\n\nset.seed(7)\nsim_too_wide <- mh_tour(N = 5000, w = 100)\n\nset.seed(84735)\nsim_too_narrow <- mh_tour(N = 5000, w = 0.01)\n\ntoo_wide <- ggplot(sim_too_wide, aes(x = .iteration, y = mu)) + \n  geom_line(color = clrs[4]) + \n  geom_hline(yintercept = 4, color = clrs[2], linetype = \"dotted\") +\n  ylim(c(1.6, 6.4)) +\n  labs(title = \"w too wide\")\n\ntoo_narrow <- ggplot(sim_too_narrow, aes(x = .iteration, y = mu)) + \n  geom_line(color = clrs[5]) + \n  geom_hline(yintercept = 4, color = clrs[2], linetype = \"dotted\") +\n  ylim(c(1.6, 6.4)) +\n  labs(title = \"w too narrow\")\n\ntoo_narrow | too_wide"
  },
  {
    "objectID": "bayes-rules/08-chapter.html",
    "href": "bayes-rules/08-chapter.html",
    "title": "7: Posterior inference and prediction",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/08-chapter.html#the-general-setup",
    "href": "bayes-rules/08-chapter.html#the-general-setup",
    "title": "7: Posterior inference and prediction",
    "section": "The general setup",
    "text": "The general setup\nWe want to know the probability that an artist in the MoMA is Gen X or younger (born after 1965). This is our \\(\\pi\\).\nWe’ll use a vague \\(\\operatorname{Beta}(4, 6)\\) prior for \\(\\pi\\) and say that the probability is probably below 0.5, but we’re not super sure where it is exactly:\n\nggplot() +\n  stat_function(fun = ~dbeta(., 4, 6), geom = \"area\", fill = clrs[1])\n\n\n\n\nHere’s the data:\n\ndata(\"moma_sample\", package = \"bayesrules\")\nhead(moma_sample)\n##                artist  country birth death alive  genx gender count\n## 1        Ad Gerritsen    dutch  1940  2015 FALSE FALSE   male     1\n## 2 Kirstine Roepstorff   danish  1972  <NA>  TRUE  TRUE female     3\n## 3    Lisa Baumgardner american  1958  2015 FALSE FALSE female     2\n## 4         David Bates american  1952  <NA>  TRUE FALSE   male     1\n## 5          Simon Levy american  1946  <NA>  TRUE FALSE   male     1\n## 6      Pierre Mercure canadian  1927  1966 FALSE FALSE   male     8\n##   year_acquired_min year_acquired_max\n## 1              1981              1981\n## 2              2005              2005\n## 3              2016              2016\n## 4              2001              2001\n## 5              2012              2012\n## 6              2008              2008\n\nOnly 14 are Gen X:\n\nmoma_sample |> \n  count(genx)\n##    genx  n\n## 1 FALSE 86\n## 2  TRUE 14\n\nThrough the magic of conjugate families, we can calculate the exact posterior:\n\\[\n\\begin{aligned}\nY &\\sim \\operatorname{Binomial}(100, \\pi) \\\\\n\\pi &= \\operatorname{Beta}(4, 6)\n\\end{aligned}\n\\]\nSince we observe \\(Y = 14\\), then the actual exact posterior is\n\\[\n\\pi \\mid (Y = 14) \\sim \\operatorname{Beta}(4 + 14, 6 + 100 - 14) \\rightarrow \\operatorname{Beta}(18, 92)\n\\]\n\nggplot() +\n  stat_function(aes(fill = \"Prior: Beta(4, 6)\"),\n                fun = ~dbeta(., 4, 6), geom = \"area\") +\n  stat_function(aes(fill = \"Posterior: Beta(18, 92)\"),\n                fun = ~dbeta(., 18, 92), geom = \"area\") +\n  scale_fill_manual(values = c(clrs[2], clrs[1]))\n\n\n\n\nNeat! We have a posterior, but now we have to do something with it:\n\nEstimation\nHypothesis testing\nPrediction\n\nBut first, for fun, here are some MCMC-based approximations of the posterior:\n\nbrmsStan\n\n\n\nmodel_pi_brms <- brm(\n  bf(num_genx | trials(artworks) ~ 0 + Intercept),\n  data = list(num_genx = 14, artworks = 100),\n  family = binomial(link = \"identity\"),\n  prior(beta(4, 6), class = b, lb = 0, ub = 1),\n  sample_prior = TRUE,  # For calculating Bayes Ratios\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"cmdstanr\", cores = 4, refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.1 seconds.\n## Chain 2 finished in 0.1 seconds.\n## Chain 3 finished in 0.1 seconds.\n## Chain 4 finished in 0.1 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.1 seconds.\n## Total execution time: 0.2 seconds.\n\nmodel_pi_brms_prior_only <- brm(\n  bf(num_genx | trials(artworks) ~ 0 + Intercept),\n  data = list(num_genx = 14, artworks = 100),\n  family = binomial(link = \"identity\"),\n  prior(beta(4, 6), class = b, lb = 0, ub = 1),\n  sample_prior = \"only\",  # For calculating Bayes Ratios\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"cmdstanr\", cores = 4, refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.1 seconds.\n## Chain 2 finished in 0.1 seconds.\n## Chain 3 finished in 0.1 seconds.\n## Chain 4 finished in 0.1 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.1 seconds.\n## Total execution time: 0.2 seconds.\n\n\nmodel_pi_brms\n##  Family: binomial \n##   Links: mu = identity \n## Formula: num_genx | trials(artworks) ~ 0 + Intercept \n##    Data: list(num_genx = 14, artworks = 100) (Number of observations: 1) \n##   Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.16      0.03     0.10     0.24 1.00     6534     6657\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n08-stan/genx.stan\n\n// Things coming in from R\ndata {\n  int<lower=0> artworks;\n  int<lower=0> num_genx;\n}\n\n// Thing to estimate\nparameters {\n  real<lower=0, upper=1> pi;  // Proportion of Gen X artists\n}\n\n// Prior and likelihood\nmodel {\n  // Prior\n  pi ~ beta(4, 6);\n  \n  // Likelihood\n  num_genx ~ binomial(artworks, pi);\n}\n\n\n\nmodel_pi_stan <- cmdstan_model(\"08-stan/genx.stan\")\n\n\npi_stan_samples <- model_pi_stan$sample(\n  data = list(artworks = 100, num_genx = 14),\n  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds."
  },
  {
    "objectID": "bayes-rules/08-chapter.html#posterior-estimation",
    "href": "bayes-rules/08-chapter.html#posterior-estimation",
    "title": "7: Posterior inference and prediction",
    "section": "8.1: Posterior estimation",
    "text": "8.1: Posterior estimation\nOur posterior \\(\\operatorname{Beta}(18, 92)\\) is a complete distribution, but we often need to work with summaries of that distribution. The mean here is 16% (\\(\\frac{18}{18 + 92} = 0.1636\\)), meaning that it is most likely the case that 16% of MoMA artists are Gen X or younger, but it could be anywhere between 10-25ish%\nWe can calculate a 95% credible interval around the median using quantiles:\n\nqbeta(c(0.025, 0.975), 18, 92)\n## [1] 0.1009084 0.2379286\n\nThere’s a 95% posterior probability that somewhere between 10% and 24% of museum artists are Gen X or younger:\n\npost_mean <- 18 / (18 + 92)\npost_median <- qbeta(0.5, 18, 92)\npost_mode <- (18 - 1)/(18 + 92 - 2)\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.4)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                fill = clrs[3]) +\n  geom_vline(xintercept = post_mode) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\")\n\n\n\n\nWe don’t have to use 95%; that’s just arbitrary. We can use different levels:\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.9)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                aes(fill = \"95%\")) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.055, 0.945), 18, 92),\n                aes(fill = \"89%\")) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.1, 0.9), 18, 92),\n                aes(fill = \"80%\")) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.25, 0.75), 18, 92),\n                aes(fill = \"50%\")) +\n  geom_vline(xintercept = post_mode) +\n  scale_fill_manual(values = colorspace::lighten(clrs[3], c(0.1, 0.3, 0.5, 0.7))) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\", fill = \"Credible interval\")\n\n\n\n\nThis posterior is a little lopsided, so we might want to make an interval that’s not centered at the mode of π, but instead centered at the highest posterior density.\n\nbrmsStan\n\n\n\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.162 0.138   0.184   0.5  median hdci     \n## 2       0.162 0.106   0.215   0.89 median hdci     \n## 3       0.162 0.0944  0.230   0.95 median hdci\n\n\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  ggplot(aes(x = b_Intercept)) +\n  stat_slab(aes(fill_ramp = stat(level)),\n            .width = c(0.02, 0.5, 0.89, 0.95, 1),\n            point_interval = \"median_hdci\",\n            fill = clrs[3]) +\n  scale_fill_ramp_discrete(range = c(0.2, 1)) +\n  labs(fill_ramp = \"Credible interval\")\n## Warning: `stat(level)` was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(level)` instead.\n\n\n\n\n\n\n\npi_stan_samples |> \n  spread_draws(pi) |> \n  median_hdci(pi, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##      pi .lower .upper .width .point .interval\n##   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 0.161 0.137   0.182   0.5  median hdci     \n## 2 0.161 0.108   0.217   0.89 median hdci     \n## 3 0.161 0.0987  0.234   0.95 median hdci\n\n\npi_stan_samples |> \n  spread_draws(pi) |> \n  ggplot(aes(x = pi)) +\n  stat_slab(aes(fill_ramp = stat(level)),\n            .width = c(0.02, 0.5, 0.89, 0.95, 1),\n            point_interval = \"median_hdci\",\n            fill = clrs[3]) +\n  scale_fill_ramp_discrete(range = c(0.2, 1)) +\n  labs(fill_ramp = \"Credible interval\")"
  },
  {
    "objectID": "bayes-rules/08-chapter.html#posterior-hypothesis-testing",
    "href": "bayes-rules/08-chapter.html#posterior-hypothesis-testing",
    "title": "7: Posterior inference and prediction",
    "section": "8.2: Posterior hypothesis testing",
    "text": "8.2: Posterior hypothesis testing\nWhat if we read somewhere that fewer than 20% of museum artists are Gen X or younger? We can calculate the posterior probability of this scenario, or \\(P(\\pi < 0.2 \\mid Y = 14)\\)\nWith the exact posterior, that’s super easy:\n\npost_prob <- pbeta(0.2, 18, 92)\npost_prob\n## [1] 0.8489856\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.4)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = c(0, 0.2),\n                fill = clrs[3]) +\n  geom_vline(xintercept = 0.2) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\")\n\n\n\n\n85% of the distribution is below 0.2, so we can say there’s an 85% chance that Gen X artists constitute 20% or fewer of modern art museum artists.\nThat’s easy!\nHere it is with MCMC:\n\nbrmsStan\n\n\n\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  count(b_Intercept < 0.2) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 2 × 3\n##   `b_Intercept < 0.2`     n  prob\n##   <lgl>               <int> <dbl>\n## 1 FALSE                2332 0.146\n## 2 TRUE                13668 0.854\n\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  ggplot(aes(x = b_Intercept)) +\n  stat_halfeye(aes(fill_ramp = stat(x < 0.2)), fill = clrs[3]) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[3], 0.4), guide = \"none\")\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\npi_stan_samples |> \n  spread_draws(pi) |> \n  count(pi < 0.2) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 2 × 3\n##   `pi < 0.2`     n  prob\n##   <lgl>      <int> <dbl>\n## 1 FALSE       1445 0.144\n## 2 TRUE        8555 0.856\n\npi_stan_samples |> \n  spread_draws(pi) |> \n  ggplot(aes(x = pi)) +\n  stat_halfeye(aes(fill_ramp = stat(x < 0.2)), fill = clrs[3]) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[3], 0.4), guide = \"none\")\n\n\n\n\n\n\n\n\nOne-sided tests (probability of direction)\nWe can also use a hypothesis testing framework and present two competing hypotheses:\n\\[\n\\begin{split}\nH_0: & \\; \\; \\pi \\ge 0.2 \\\\\nH_a: & \\; \\; \\pi < 0.2\n\\end{split}\n\\]\nWe already know the probability of \\(H_a\\) (0.849), so the probability of \\(H_0\\) is 1 minus that, or 0.151. The posterior odds is the ratio of those two probabilities\n\\[\n\\text{posterior odds} = \\frac{P(H_a \\mid Y = 14)}{P(H_0 \\mid Y = 14)} = \\frac{0.849}{0.151} \\approx 5.622\n\\]\n\npost_odds <- post_prob / (1 - post_prob)\npost_odds\n## [1] 5.621883\n\nThat means that π is ≈6 times more likely to be below 20% than to be above 20%\nThat’s all based on the posterior though. Back before we knew anything, we had a prior of \\(\\operatorname{Beta}(6, 4)\\), an in that world, we had a 9% chance that it was true and a 91% chance that it was all false\n\nprior_prob <- pbeta(0.2, 4, 6)\nprior_prob\n## [1] 0.08564173\n1 - prior_prob\n## [1] 0.9143583\n\nSo the prior odds were only 1 in 10:\n\nprior_odds <- prior_prob / (1 - prior_prob)\nprior_odds\n## [1] 0.09366321\n\nFinally, we can do something more useful with these prior and posterior odds and calculate the Bayes Factor, which is just their ratio:\n\\[\n\\text{Bayes Factor} = \\frac{\\text{Posterior odds}}{\\text{Prior odds}}\n\\]\n\nBF <- post_odds / prior_odds\nBF\n## [1] 60.02232\n\nAfter learning about 14 Gen X artists, “the posterior odds of our hypothesis … are roughly 60 times higher than the prior odds”, which is “fairly convincing”\nNo significance testing, no failing to reject nulls. Just vibes.\nEvid.Ratio here is the posterior probability of the hypothesis being true / posterior probability of the hypothesis not being true, or the same as post_odds above.\n\nh <- hypothesis(model_pi_brms, \"Intercept < 0.2\")\nh\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n## 1 (Intercept)-(0.2) < 0    -0.04      0.03    -0.09     0.02       5.86\n##   Post.Prob Star\n## 1      0.85     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\nplot(h)\n\n\n\n\nIf we want the same Bayes Factor ratio that Bayes Rules! calculates, we need to use the evidence ratio from brms and calculate prior_odds by hand:\n\nprior_prob <- pbeta(0.2, 4, 6)\nprior_odds <- prior_prob / (1 - prior_prob)\n\npost_odds_brms <- h$hypothesis$Evid.Ratio\n\nBF_brms <- post_odds_brms / prior_odds\nBF_brms\n## [1] 62.57594\n\n\n\nTwo-sided tests (ROPE stuff)\nWhat if we want to know whether or not 30% of museum artists are Gen X or younger, not just a direction? Now we’re dealing with two sides:\n\\[\n\\begin{split}\nH_0: & \\; \\; \\pi = 0.3 \\\\\nH_a: & \\; \\; \\pi \\ne 0.3 \\\\\n\\end{split}\n\\]\nWe already know the 95% credible interval for π, and 0.3 doesn’t really fit well in it:\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.9)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                fill = clrs[3]) +\n  geom_vline(xintercept = 0.3) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\", fill = \"Credible interval\")\n\n\n\n\nThat provides us with good evidence that the hypothesis that 30% of artists are Gen X is not correct. It’s subtantially outside of the credible interval. But what does substantial mean? We get to define that.\nWe can be like Kruschke and define a buffer around 0.3, or a region of practical equivalence (ROPE). Here we’ll do 0.3±0.05, or between 0.25 and 0.35. We can calculate how much of the posterior is outside of that ROPE.\nSince we know the actual posterior is \\(\\operatorname{Beta}(18, 92)\\), we can find the percentage of the area of the curve that falls in the ROPE with pbeta():\n\nprop_in_rope <- pbeta(0.35, 18, 92) - pbeta(0.25, 18, 92)\nprop_in_rope\n## [1] 0.01250077\n1 - prop_in_rope\n## [1] 0.9874992\n\n98.7% of the posterior is outside of that ROPE. I’d say a value of 30% is pretty substantially far away from the posterior and thus really unlikely.\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.9)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                fill = clrs[3]) +\n  annotate(geom = \"rect\", xmin = 0.25, xmax = 0.35, ymin = -Inf, ymax = Inf, alpha = 0.3) +\n  geom_vline(xintercept = 0.3) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\", fill = \"Credible interval\")\n\n\n\n\nWe can do this with the MCMC draws too and we get the same results:\n\nbrmsStan\n\n\n\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  summarize(prop_in_rope = sum(b_Intercept > 0.25 & b_Intercept < 0.35) / n(),\n            prop_outside_rope = 1 - prop_in_rope)\n## # A tibble: 1 × 2\n##   prop_in_rope prop_outside_rope\n##          <dbl>             <dbl>\n## 1       0.0119             0.988\n\n\n\n\npi_stan_samples |> \n  spread_draws(pi) |> \n  summarize(prop_in_rope = sum(pi > 0.25 & pi < 0.35) / n(),\n            prop_outside_rope = 1 - prop_in_rope)\n## # A tibble: 1 × 2\n##   prop_in_rope prop_outside_rope\n##          <dbl>             <dbl>\n## 1       0.0109             0.989"
  },
  {
    "objectID": "bayes-rules/08-chapter.html#posterior-prediction",
    "href": "bayes-rules/08-chapter.html#posterior-prediction",
    "title": "7: Posterior inference and prediction",
    "section": "8.3: Posterior prediction",
    "text": "8.3: Posterior prediction\n(This stuff is all covered in my guide here too)\nWe get data for 20 more pieces of art at the museum. Based on what we know about π, how many would we predict would be by Gen X artists?\nIt’s reasonable to think 3 (since 20 * 0.16 = 3), but that misses out on two levels of uncertainty:\n\nSampling variability in the data - even if π is truly 0.16, the amount we get in the sample will vary just because of randomness\nPosterior variability in π - it could be anywhere between 0.1 and 0.24\n\nThe posterior predictive model takes both kinds of uncertainty into account\nThere’s technically a mathy way to get at posterior predictions, and the book covers it, but it’s a complicated mess and they even conclude by saying “In this book, we’ll never need to do something like this again”\nIn the book, the actual posterior predictive probability that 3 of the 20 new artists will be Gen X, based on a posterior that saw 14 (i.e. the model we created), is 0.2217.\nWe can approximate that exact 0.2217 with the MCMC draws too. With brms models we can use posterior_predict(), posterior_linpred(), and posterior_epred() to extract different types of posterior outcomes on different scales. With raw Stan output, we have to do a little more work ourselves.\n\nbrmsStan\n\n\nWe want to use predicted_draws() since that incorporates both kinds of uncertainty, and it returns values that are predicted counts, not probabilities or π (see my guide for more)\n\npredicted_genx_after_20 <- model_pi_brms |> \n  predicted_draws(newdata = tibble(artworks = 20)) |> \n  group_by(.prediction) |> \n  summarize(n = n()) |> \n  mutate(prop = n / sum(n))\npredicted_genx_after_20\n## # A tibble: 14 × 3\n##    .prediction     n      prop\n##          <int> <int>     <dbl>\n##  1           0   619 0.0387   \n##  2           1  1968 0.123    \n##  3           2  3126 0.195    \n##  4           3  3588 0.224    \n##  5           4  2979 0.186    \n##  6           5  1890 0.118    \n##  7           6  1107 0.0692   \n##  8           7   460 0.0288   \n##  9           8   171 0.0107   \n## 10           9    66 0.00412  \n## 11          10    22 0.00138  \n## 12          11     1 0.0000625\n## 13          12     2 0.000125 \n## 14          13     1 0.0000625\n\nggplot(predicted_genx_after_20, aes(x = factor(.prediction), y = prop)) + \n  geom_col()\n\n\n\n\n# Posterior predictive probability that 3/20 will be Gen X is roughly the same\n# as 0.2217!\npredicted_genx_after_20 |> \n  filter(.prediction == 3) |> \n  pull(prop)\n## [1] 0.22425\n\nWe can also get the variability in just π if we wanted by using linpred_draws():\n\nmodel_pi_brms |> \n  linpred_draws(newdata = tibble(artworks = 20)) |> \n  ungroup() |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye()\n\n\n\n\nAnd if we use epred_draws(), we’ll get the expected number of Gen X artworks:\n\nmodel_pi_brms |> \n  epred_draws(newdata = tibble(artworks = 20)) |> \n  ungroup() |> \n  ggplot(aes(x = .epred)) +\n  stat_halfeye()\n\n\n\n\nLovely.\n\n\nRaw Stan requires a little more work. We could theoretically use Stan to generate posterior predictions with a generated quantities block:\ngenerated quantities {\n  vector[1000] num_genx_rep;\n\n  for (i in 1:1000) {\n    num_genx_rep[i] = binomial_rng(20, pi);\n  }\n}\nBut that requires either hard-coding two numbers into the Stan code: 1000 for the number of simulations and 20 for the number of new artworks. If we want to change any of those, we’d have to recompile, which is tedious.\nAlternatively, we could add a couple variables to the data block and pass them through R:\ndata {\n  // other variables\n  int<lower=1> n_sims;\n  int<lower=1> new_artworks;\n}\n\n// other blocks\n\ngenerated quantities {\n  vector[n_sims] num_genx_rep;\n\n  for (i in 1:n_sims) {\n    num_genx_rep[i] = binomial_rng(new_artworks, pi);\n  }\n}\nWe’d then need to include values for those new variables in the list of data we pass to Stan:\n\npi_stan_samples <- model_pi_stan$sample(\n  data = list(artworks = 100, num_genx = 14, new_artworks = 20, n_sims = 1000),\n  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, \n  refresh = 0, seed = BAYES_SEED\n)\n\nThat would work great and the results from Stan would include 1000 predictions for the number of Gen X artists. But it feels a little excessive to keep rerunning the original 14-artworks model over and over for different numbers of new artworks.\nSo instead we can use R to build the posterior predictions, since we have all the posterior values of π in the MCMC chains, and since all we’re really doing with Stan is using Stan’s version of rbinom() anyway (binomial_rng()).\n\npredicted_genx_after_20_stan <- pi_stan_samples |> \n  spread_draws(pi) |> \n  mutate(.prediction = rbinom(n(), size = 20, prob = pi)) \n\npredicted_genx_after_20_stan_summarized <- predicted_genx_after_20_stan |> \n  group_by(.prediction) |> \n  summarize(n = n()) |> \n  mutate(prop = n / sum(n))\npredicted_genx_after_20_stan\n## # A tibble: 10,000 × 5\n##    .chain .iteration .draw    pi .prediction\n##     <int>      <int> <int> <dbl>       <int>\n##  1      1          1     1 0.157           6\n##  2      1          2     2 0.151           1\n##  3      1          3     3 0.189           6\n##  4      1          4     4 0.226           4\n##  5      1          5     5 0.192           4\n##  6      1          6     6 0.210           7\n##  7      1          7     7 0.210           6\n##  8      1          8     8 0.174           6\n##  9      1          9     9 0.157           2\n## 10      1         10    10 0.153           2\n## # … with 9,990 more rows\n\nggplot(predicted_genx_after_20_stan_summarized, \n       aes(x = factor(.prediction), y = prop)) + \n  geom_col()\n\n\n\n\n# Posterior predictive probability that 3/20 will be Gen X is roughly the same\n# as 0.2217!\npredicted_genx_after_20_stan_summarized |> \n  filter(.prediction == 3) |> \n  pull(prop)\n## [1] 0.2208\n\nWe can also get the equivalent of posterior_epred() by calculating the average of the predictive posterior:\n\nepred <- predicted_genx_after_20_stan |> \n  summarize(epred = mean(.prediction)) |> \n  pull(epred)\nepred\n## [1] 3.2717\n\nggplot(predicted_genx_after_20_stan, aes(x = .prediction)) + \n  stat_count() +\n  geom_vline(xintercept = epred)\n\n\n\n\nI haven’t figured out a way to get posterior_linpred() (the variability of just π) with raw Stan like this though. :("
  },
  {
    "objectID": "bayes-rules/08-chapter.html#posterior-analysis-with-mcmc",
    "href": "bayes-rules/08-chapter.html#posterior-analysis-with-mcmc",
    "title": "7: Posterior inference and prediction",
    "section": "8.4: Posterior analysis with MCMC",
    "text": "8.4: Posterior analysis with MCMC\nOh ha, this whole section shows how to do everything above with Stan, but I already did that above with both brms and raw Stan, so just, um look up there ↑."
  },
  {
    "objectID": "bayes-rules/09-chapter.html",
    "href": "bayes-rules/09-chapter.html",
    "title": "9: Simple normal regression",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/09-chapter.html#tuning-prior-models-for-regression-parameters",
    "href": "bayes-rules/09-chapter.html#tuning-prior-models-for-regression-parameters",
    "title": "9: Simple normal regression",
    "section": "9.2: Tuning prior models for regression parameters",
    "text": "9.2: Tuning prior models for regression parameters\nWe want to estimate the effect of temperature on bike share ridership. We need to estimate three parameters:\n\n\\(\\beta_0\\), or the intercept. From prior research, we know:\n\n\nOn an average temperature day, say 65 or 70 degrees for D.C., there are typically around 5000 riders, though this average could be somewhere between 3000 and 7000.\n\n\n\\(\\beta_1\\), or the slope. From prior research, we know:\n\n\nFor every one degree increase in temperature, ridership typically increases by 100 rides, though this average increase could be as low as 20 or as high as 180.\n\n\n\\(\\sigma\\), or the variation in ridership. From prior research, we know:\n\n\nAt any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\nThe intercept there is centered at 5000 riders.\nThat gives us these priors:\n\np1 <- ggplot() +\n  geom_function(fun = ~dnorm(., 5000, 1000), size = 1, color = clrs[1]) +\n  xlim(c(1000, 9000)) +\n  labs(x = \"**β<sub>0c</sub>**<br>Average daily ridership, centered\") +\n  theme(axis.title.x = element_markdown())\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\np2 <- ggplot() +\n  geom_function(fun = ~dnorm(., 100, 40), size = 1, color = clrs[3]) +\n  xlim(c(-50, 250)) +\n  labs(x = \"**β<sub>1</sub>**<br>Effect of temperature on ridership\") +\n  theme(axis.title.x = element_markdown())\n\np3 <- ggplot() +\n  geom_function(fun = ~dexp(., 1 / 1250), size = 1, color = clrs[4]) +\n  xlim(c(0, 7000)) +\n  labs(x = \"**σ**<br>Variation in daily ridership\") +\n  theme(axis.title.x = element_markdown())\n\np1 | p2 | p3\n\n\n\n\nMore formally, we can write out the whole model like this:\n\\[\n\\begin{aligned}\nY_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\text{, or}  & \\text{[McElreath's syntax]} \\\\\nY_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) & \\text{[Bayes Rules!'s syntax]}\n\\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_i \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(5000, 1000) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(100, 40) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1 / 1250)\n\\end{aligned}\n\\]\nWe can simulate from all these priors to see how reasonable they are, McElreath-style. In Bayes Rules! they show a picture of this but not the code to make it. I’ll use brms bc it’s easy.\n\npriors <- c(prior(normal(5000, 1000), class = Intercept),\n            prior(normal(100, 40), class = b, coef = \"temp_feel_c\"),\n            prior(exponential(0.0008), class = sigma))\n\nbike_prior_only <- brm(\n  bf(rides ~ temp_feel_c),\n  data = bikes,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\",\n  backend = \"cmdstanr\", cores = 4, seed = BAYES_SEED, refresh = 0\n)\n## Start sampling\n\nThese lines all look reasonable, yay.\n\ndraws_prior <- tibble(temp_feel_c = seq(45 - temp_details$scaled_center, \n                                        90 - temp_details$scaled_center, \n                                        1)) |> \n  add_epred_draws(bike_prior_only, ndraws = 200) |> \n  mutate(unscaled = temp_feel_c + temp_details$scaled_center)\n\ndraws_prior |> \n  ggplot(aes(x = unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  labs(x = \"Temperature\", y = \"Number of rides\")"
  },
  {
    "objectID": "bayes-rules/09-chapter.html#posterior-simulation",
    "href": "bayes-rules/09-chapter.html#posterior-simulation",
    "title": "9: Simple normal regression",
    "section": "9.3: Posterior simulation",
    "text": "9.3: Posterior simulation\n\nRun the model\n\nrstanarmbrmsStan\n\n\n\nbike_rstanarm <- stan_glm(\n  rides ~ temp_feel_c,\n  data = bikes,\n  family = gaussian(),\n  prior_intercept = normal(5000, 1000),\n  prior = normal(100, 40),\n  prior_aux = exponential(0.0008),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\n\n\npriors <- c(prior(normal(5000, 1000), class = Intercept),\n            prior(normal(100, 40), class = b, coef = \"temp_feel_c\"),\n            prior(exponential(0.0008), class = sigma))\n\nbike_brms <- brm(\n  bf(rides ~ temp_feel_c),\n  data = bikes,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"09-manual_cache/bike-brms\"\n)\n## Start sampling\n\n\n\n\n\n\n09-stan/bike-simple.stan\n\ndata {\n  int<lower = 0> n;\n  vector[n] Y;\n  vector[n] X;\n}\n\n/*\n// We could also center things here and then use it in mu below:\n// mu = beta0 + beta1 * X_centered;\n// See https://mc-stan.org/docs/stan-users-guide/standardizing-predictors-and-outputs.html\ntransformed data {\n  vector[n] X_centered;\n  \n  X_centered = X - mean(X);\n}\n*/\n\nparameters {\n  real beta0;\n  real beta1;\n  real<lower = 0> sigma;\n}\n\ntransformed parameters {\n  vector[n] mu;\n  mu = beta0 + beta1 * X;\n}\n\nmodel {\n  Y ~ normal(mu, sigma);\n  \n  beta0 ~ normal(5000, 1000);\n  beta1 ~ normal(100, 40);\n  sigma ~ exponential(0.0008);\n}\n\ngenerated quantities {\n  vector[n] Y_rep;\n  \n  for (i in 1:n) {\n    Y_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n\n\n\nbike_stan <- cmdstan_model(\"09-stan/bike-simple.stan\")\n\n\nbike_stan_samples <- bike_stan$sample(\n  data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel_c),\n  parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n  refresh = 0, seed = BAYES_SEED,\n  output_dir = \"09-stan\"\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 2.0 seconds.\n## Chain 3 finished in 2.0 seconds.\n## Chain 4 finished in 2.0 seconds.\n## Chain 2 finished in 2.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 2.0 seconds.\n## Total execution time: 2.1 seconds.\n\n\n\n\n\n\nDiagnostics\n\nrstanarmbrmsStan\n\n\n\nEffective sample size and R-hat\n\nneff_ratio(bike_rstanarm)\n## (Intercept) temp_feel_c       sigma \n##     0.96345     0.96075     0.96825\nrhat(bike_rstanarm)\n## (Intercept) temp_feel_c       sigma \n##   0.9998765   0.9999727   1.0000086\n\n\n\nTrace plots\n\nbike_rstanarm |> \n  gather_draws(`(Intercept)`, temp_feel_c, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), c(\"(Intercept)\", \"temp_feel_c\", \"sigma\"))) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.05) +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nTrank plots\n\nbike_rstanarm |> \n  gather_draws(`(Intercept)`, temp_feel_c, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), c(\"(Intercept)\", \"temp_feel_c\", \"sigma\"))) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank)) +\n  stat_bin(aes(color = factor(.chain)), geom = \"step\", binwidth = 1000,\n           position = position_identity(), boundary = 0) +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable)) +\n  theme(legend.position = \"bottom\",\n        axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nDensity plots\n\nbike_rstanarm |> \n  gather_draws(`(Intercept)`, temp_feel_c, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), c(\"(Intercept)\", \"temp_feel_c\", \"sigma\"))) |> \n  ggplot(aes(x = .value, color = factor(.chain))) +\n  geom_density() +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable), scales = \"free\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nEffective sample size and R-hat\n\nneff_ratio(bike_brms)\n##   b_Intercept b_temp_feel_c         sigma        lprior          lp__ \n##     1.0579051     1.0534749     1.1040274     1.0473025     0.4827253\nrhat(bike_brms)\n##   b_Intercept b_temp_feel_c         sigma        lprior          lp__ \n##      1.000021      1.000172      1.000024      1.000135      1.000185\n\n\n\nTrace plots\n\nbike_brms |> \n  gather_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.05) +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nTrank plots\n\nbike_brms |> \n  gather_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank)) +\n  stat_bin(aes(color = factor(.chain)), geom = \"step\", binwidth = 1000,\n           position = position_identity(), boundary = 0) +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable)) +\n  theme(legend.position = \"bottom\",\n        axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nDensity plots\n\nbike_brms |> \n  gather_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  ggplot(aes(x = .value, color = factor(.chain))) +\n  geom_density() +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable), scales = \"free\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nEffective sample size and R-hat\n\nneff_ratio(bike_stan_samples, pars = c(\"beta0\", \"beta1\", \"sigma\"))\n##    beta0    beta1    sigma \n## 1.077305 1.012467 1.085847\nrhat(bike_stan_samples, pars = c(\"beta0\", \"beta1\", \"sigma\"))\n##     beta0     beta1     sigma \n## 1.0001360 1.0000768 0.9999753\n\n\n\nTrace plots\n\nbike_stan_samples |> \n  gather_draws(beta0, beta1, sigma) |> \n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line(size = 0.05) +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nTrank plots\n\nbike_stan_samples |> \n  gather_draws(beta0, beta1, sigma) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank)) +\n  stat_bin(aes(color = factor(.chain)), geom = \"step\", binwidth = 1000,\n           position = position_identity(), boundary = 0) +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable)) +\n  theme(legend.position = \"bottom\",\n        axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nDensity plots\n\nbike_stan_samples |> \n  gather_draws(beta0, beta1, sigma) |> \n  ggplot(aes(x = .value, color = factor(.chain))) +\n  geom_density() +\n  labs(color = \"Chain\") +\n  facet_wrap(vars(.variable), scales = \"free\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "bayes-rules/09-chapter.html#interpreting-the-posterior",
    "href": "bayes-rules/09-chapter.html#interpreting-the-posterior",
    "title": "9: Simple normal regression",
    "section": "9.4: Interpreting the posterior",
    "text": "9.4: Interpreting the posterior\n\nParameter summaries\n\nrstanarmbrmsStan\n\n\n\ntidy(bike_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 4 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)   3487.      58.0    3413.     3561. \n## 2 temp_feel_c     82.2      5.08     75.7      88.7\n## 3 sigma         1282.      40.9    1231.     1336. \n## 4 mean_PPD      3487.      82.0    3382.     3593.\n\n\nbike_rstanarm |> \n  gather_draws(`(Intercept)`, temp_feel_c, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), c(\"(Intercept)\", \"temp_feel_c\", \"sigma\"))) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\ntidy(bike_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## # A tibble: 3 × 5\n##   term            estimate std.error conf.low conf.high\n##   <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)       3487.      57.5    3414.     3561. \n## 2 temp_feel_c         82.1      5.08     75.6      88.6\n## 3 sd__Observation   1283.      40.3    1232.     1336.\n\n\nbike_brms |> \n  gather_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\nbike_stan_samples$print(variables = c(\"beta0\", \"beta1\", \"sigma\"), \n                        \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.1, 0.9)))\n##  variable    mean  median    sd     10%     90%\n##     beta0 3487.16 3486.85 57.13 3414.32 3560.02\n##     beta1   82.12   82.10  5.06   75.68   88.58\n##     sigma 1282.72 1281.84 40.44 1231.39 1335.31\n\n\nbike_stan_samples |> \n  gather_draws(beta0, beta1, sigma) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\n\n\nFitted draws\n\nrstanarmbrmsStan\n\n\n\nbikes |> \n  add_linpred_draws(bike_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = temp_feel, y = rides)) +\n  geom_point(data = bikes, size = 0.5) +\n  geom_line(aes(y = .linpred, group = .draw), alpha = 0.2, size = 0.5, color = clrs[6]) +\n  labs(x = \"Temperature\", y = \"Rides\")\n\n\n\n\n\n\n\nbikes |> \n  add_linpred_draws(bike_brms, ndraws = 100) |> \n  ggplot(aes(x = temp_feel, y = rides)) +\n  geom_point(data = bikes, size = 0.5) +\n  geom_line(aes(y = .linpred, group = .draw), alpha = 0.2, size = 0.5, color = clrs[6]) +\n  labs(x = \"Temperature\", y = \"Rides\")\n\n\n\n\n\n\n\nbike_stan_samples |> \n  spread_draws(mu[i]) |> \n  mean_qi() |> \n  bind_cols(bikes) |> \n  ggplot(aes(x = temp_feel, y = rides)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = mu), color = clrs[6], size = 1) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = clrs[6], alpha = 0.2) +\n  labs(x = \"Temperature\", y = \"Rides\")\n\n\n\n\nThere’s no easy way to replicate add_linpred_draws(..., ndraws = BLAH) with raw Stan like this without running the model again and generating a bunch of simulated mus based on some variable like n_sim that we build into the Stan model, but we can use the original 500 rows of data and use geom_lineribbon() instead of a spaghetti plot.\n\n\n\n\n\nIs β1 > 0?\nEasily.\n\nrstanarmbrmsStan\n\n\n\nbike_rstanarm |> \n  spread_draws(temp_feel_c) |> \n  count(temp_feel_c > 0) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 1 × 3\n##   `temp_feel_c > 0`     n  prob\n##   <lgl>             <int> <dbl>\n## 1 TRUE              20000     1\n\n\n\n\nbike_brms |> \n  spread_draws(b_temp_feel_c) |> \n  count(b_temp_feel_c > 0) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 1 × 3\n##   `b_temp_feel_c > 0`     n  prob\n##   <lgl>               <int> <dbl>\n## 1 TRUE                20000     1\n\n\n\n\nbike_stan_samples |> \n  spread_draws(beta1) |> \n  count(beta1 > 0) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 1 × 3\n##   `beta1 > 0`     n  prob\n##   <lgl>       <int> <dbl>\n## 1 TRUE        20000     1"
  },
  {
    "objectID": "bayes-rules/09-chapter.html#posterior-prediction",
    "href": "bayes-rules/09-chapter.html#posterior-prediction",
    "title": "9: Simple normal regression",
    "section": "9.5: Posterior prediction",
    "text": "9.5: Posterior prediction\nWe could plug in some temperature, like 75, and get a predicted count of riders (values from the rstanarm model):\n\nvalues_rstanarm <- bike_rstanarm |> \n  tidy() |> \n  split(~term)\n\nb0 <- values_rstanarm$`(Intercept)`$estimate\nb1 <- values_rstanarm$temp_feel_c$estimate\ntemp_mean <- temp_details$scaled_center\n\nb0 + (b1 * (75 - temp_mean))\n## [1] 3968.317\n\nBut this ignores two types of uncertainty:\n\nSampling variability in the data, or in \\(Y\\)\nPosterior variability in parameters, or in \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\)\n\n\nrstanarmbrmsStan\n\n\n\np1 <- bike_rstanarm |> \n  linpred_draws(newdata = tibble(temp_feel_c = (75 - temp_mean))) |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[6]) +\n  labs(title = \"Predicted riders at 75˚\",\n       subtitle = \"µ only; posterior_linpred()\",\n       x = \"Predicted riders\", y = NULL)\n\np2 <- bike_rstanarm |> \n  predicted_draws(newdata = tibble(temp_feel_c = (75 - temp_mean))) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[5]) +\n  labs(title = \"Predicted riders at 75˚\",\n       subtitle = \"rnorm(µ, σ); posterior_predict()\",\n       x = \"Predicted riders\", y = NULL)\n\np1 | p2\n\n\n\n\n\n\n\np1 <- bike_brms |> \n  linpred_draws(newdata = tibble(temp_feel_c = (75 - temp_mean))) |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[6]) +\n  labs(title = \"Predicted riders at 75˚\",\n       subtitle = \"µ only; posterior_linpred()\",\n       x = \"Predicted riders\", y = NULL)\n\np2 <- bike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = (75 - temp_mean))) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[5]) +\n  labs(title = \"Predicted riders at 75˚\",\n       subtitle = \"rnorm(µ, σ); posterior_predict()\",\n       x = \"Predicted riders\", y = NULL)\n\np1 | p2\n\n\n\n\n\n\nAHH SO THIS IS HOW YOU DO newdata = WHATEVER WITH RAW STAN. Build mu (posterior_linpred()) on your own with the coefficients from each draw, then use that mu in rnorm() (posterior_predict()). You can then get the expectation of the posterior (posterior_epred()) by taking the average of that.\n\npredict_75 <- bike_stan_samples |> \n  spread_draws(beta0, beta1, sigma) |> \n  mutate(mu = beta0 + (beta1 * (75 - temp_mean)),  # like posterior_linpred()\n         y_new = rnorm(n(), mean = mu, sd = sigma))  # like posterior_predict()\n\np1 <- predict_75 |> \n  ggplot(aes(x = mu)) +\n  stat_halfeye(fill = clrs[6]) +\n  labs(title = \"Predicted riders at 75˚\",\n       subtitle = \"µ only; like posterior_linpred()\",\n       x = \"Predicted riders\", y = NULL)\n\np2 <- predict_75 |> \n  ggplot(aes(x = y_new)) +\n  stat_halfeye(fill = clrs[5]) +\n  labs(title = \"Predicted riders at 75˚\",\n       subtitle = \"rnorm(µ, σ); like posterior_predict()\",\n       x = \"Predicted riders\", y = NULL)\n\np1 | p2\n\n\n\n\npredict_75 |> \n  summarize(epred = mean(y_new))\n## # A tibble: 1 × 1\n##   epred\n##   <dbl>\n## 1 3974."
  },
  {
    "objectID": "bayes-rules/09-chapter.html#sequential-regression-modeling",
    "href": "bayes-rules/09-chapter.html#sequential-regression-modeling",
    "title": "9: Simple normal regression",
    "section": "9.6: Sequential regression modeling",
    "text": "9.6: Sequential regression modeling\nBayesianism is all about updating. What does this relationship between temperature and ridership as data is slowly collected, like after 30 days, 60 days, and 500 days? How does the posterior evolve and settle?\nI’ll do this just with brms:\n\npriors <- c(prior(normal(5000, 1000), class = Intercept),\n            prior(normal(100, 40), class = b, coef = \"temp_feel_c\"),\n            prior(exponential(0.0008), class = sigma))\n\nbike_phases <- tribble(\n  ~phase, ~data,\n  1,      slice(bikes, 1:30),\n  2,      slice(bikes, 1:60),\n  3,      bikes\n) |> \n  mutate(model = map(data, ~{\n    brm(\n      bf(rides ~ temp_feel_c),\n      data = .,\n      family = gaussian(),\n      prior = priors,\n      chains = 4, iter = 5000*2, seed = BAYES_SEED, \n      backend = \"cmdstanr\", refresh = 0\n    )})\n  )\n## Start sampling\n## Start sampling\n## Start sampling\n\n\nbike_phase_draws <- bike_phases |> \n  mutate(draws = map(model, ~spread_draws(., b_temp_feel_c)))\n\nphases_coefs <- bike_phase_draws |> \n  mutate(coef_plot = map2(draws, phase, ~{\n    ggplot(.x, aes(x = b_temp_feel_c)) +\n      stat_halfeye(fill = clrs[3]) +\n      coord_cartesian(xlim = c(-10, 100)) +\n      labs(title = paste(\"Phase\", .y))\n  }))\n\nwrap_plots(phases_coefs$coef_plot)\n\n\n\n\n\nbike_phase_preds <- bike_phases |> \n  mutate(preds = map2(data, model, ~{\n    .x |> \n      add_linpred_draws(.y, ndraws = 50) |> \n      ungroup()\n  }))\n\nphases_preds <- bike_phase_preds |> \n  mutate(pred_plot = pmap(list(data, preds, phase), ~{\n    ggplot(..2, aes(x = temp_feel, y = rides)) +\n      geom_point(data = ..1, size = 0.5) +\n      geom_line(aes(y = .linpred, group = .draw), \n                alpha = 0.2, size = 0.5, color = clrs[6]) +\n      labs(title = paste(\"Phase\", ..3)) +\n      coord_cartesian(xlim = c(40, 90), ylim = c(0, 7000))\n  }))\n\nwrap_plots(phases_preds$pred_plot)"
  },
  {
    "objectID": "bayes-rules/10-chapter.html",
    "href": "bayes-rules/10-chapter.html",
    "title": "10: Evaluating regression models",
    "section": "",
    "text": "(Original chapter)\n\\[\n\\require{mathtools}\n\\]"
  },
  {
    "objectID": "bayes-rules/10-chapter.html#the-setup",
    "href": "bayes-rules/10-chapter.html#the-setup",
    "title": "10: Evaluating regression models",
    "section": "The setup",
    "text": "The setup\nBack to the model from chapter 9, modeling Capital Bikeshare rides based on daily temperatures:\n\\[\n\\begin{aligned}\nY_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\text{, or}  & \\text{[McElreath's syntax]} \\\\\nY_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) & \\text{[Bayes Rules!'s syntax]}\n\\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_i \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(5000, 1000) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(100, 40) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1 / 1250)\n\\end{aligned}\n\\]\n\nrstanarmbrmsStan\n\n\n\nbike_rstanarm <- stan_glm(\n  rides ~ temp_feel_c,\n  data = bikes,\n  family = gaussian(),\n  prior_intercept = normal(5000, 1000),\n  prior = normal(100, 40),\n  prior_aux = exponential(0.0008),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\n\n\npriors <- c(prior(normal(5000, 1000), class = Intercept),\n            prior(normal(100, 40), class = b, coef = \"temp_feel_c\"),\n            prior(exponential(0.0008), class = sigma))\n\nbike_brms <- brm(\n  bf(rides ~ temp_feel_c),\n  data = bikes,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\n\n\n\n\n10-stan/bike-better.stan\n\ndata {\n  int<lower = 0> n;\n  vector[n] Y;\n  vector[n] X;\n}\n\nparameters {\n  real beta0;\n  real beta1;\n  real<lower = 0> sigma;\n}\n\ntransformed parameters {\n  vector[n] mu;\n  mu = beta0 + beta1 * X;\n}\n\nmodel {\n  Y ~ normal(mu, sigma);\n  \n  beta0 ~ normal(5000, 1000);\n  beta1 ~ normal(100, 40);\n  sigma ~ exponential(0.0008);\n}\n\ngenerated quantities {\n  vector[n] Y_rep;\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n    Y_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n\n\n\nbike_stan_better <- cmdstan_model(\"10-stan/bike-better.stan\")\n\n\nbike_stan_samples <- bike_stan_better$sample(\n  data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel_c),\n  parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 2.5 seconds.\n## Chain 2 finished in 2.5 seconds.\n## Chain 3 finished in 2.5 seconds.\n## Chain 4 finished in 2.5 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 2.5 seconds.\n## Total execution time: 2.6 seconds.\n\n\n\n\nResults:\n\nrstanarmbrmsStan\n\n\n\ntidy(bike_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 4 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)   3487.      58.0    3413.     3561. \n## 2 temp_feel_c     82.2      5.08     75.7      88.7\n## 3 sigma         1282.      40.9    1231.     1336. \n## 4 mean_PPD      3487.      82.0    3382.     3593.\n\n\n\n\ntidy(bike_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## # A tibble: 3 × 5\n##   term            estimate std.error conf.low conf.high\n##   <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)       3487.      57.5    3414.     3561. \n## 2 temp_feel_c         82.1      5.08     75.6      88.6\n## 3 sd__Observation   1283.      40.3    1232.     1336.\n\n\n\n\nbike_stan_samples$print(variables = c(\"beta0\", \"beta1\", \"sigma\"), \n                        \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.1, 0.9)))\n##  variable    mean  median    sd     10%     90%\n##     beta0 3487.16 3486.85 57.13 3414.32 3560.02\n##     beta1   82.12   82.10  5.06   75.68   88.58\n##     sigma 1282.72 1281.84 40.44 1231.39 1335.31"
  },
  {
    "objectID": "bayes-rules/10-chapter.html#how-wrong-is-the-model",
    "href": "bayes-rules/10-chapter.html#how-wrong-is-the-model",
    "title": "10: Evaluating regression models",
    "section": "10.2: How wrong is the model?",
    "text": "10.2: How wrong is the model?\nThe model is definitely wrong—we just care about how wrong it is. There are three assumptions in this model:\n\\[\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\sigma &\\overbrace{\\stackrel{\\text{ind}}{\\sim}}^{\\mathclap{\\text{Assumption 1}}} \\underbrace{\\mathcal{N}(\\mu_i, \\sigma^2)}_{\\text{Assumption 3}}\\\\\n\\mu_i &= \\underbrace{\\beta_{0c} + \\beta_1 X_i}_{\\text{Assumption 2}}\n\\end{aligned}\n\\]\n\nWhen conditioned on \\(X\\), the observed data \\(Y_i\\) for each case \\(i\\) is independent of the observed data for any other case (like \\(Y_j\\), \\(Y_k\\), etc)\nThe typical \\(Y\\) outcome can be written as a linear function of \\(X\\), or \\(\\mu = \\beta_0 + \\beta_1 X\\)\nAt any \\(X\\) value, \\(Y\\) varies normally around \\(\\mu\\) with a consistent variability of \\(\\sigma\\)\n\nWe can use statsy tests for assumption 1, but we don’t really need to—we can use logic instead. By itself, ridership count \\(Y\\) is highly correlated over time, but after controlling for \\(X\\), it’s likely that the autocorrelation with \\(Y\\) is cancelled out or controlled away. We’re probably reasonably okay.\nFor assumptions 2 and 3, we can use a scatterplot and see if (1) it looks linear and (2) it looks normally distributed around the line:\n\nggplot(bikes, aes(x = temp_feel, y = rides)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nLooks linear to me.\nBut just eyeballing it isn’t great. We can test the assumptions more formally with a posterior predictive check:\n\nIf the combined model assumptions are reasonable, then our posterior model should be able to simulate ridership data that’s similar to the original data.\n\nWe can check that by using the \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) parameters from the chains to generate predictions for each of the MCMC draws. Here’s how that works behind the scenes:\nWe have 20,000 sets of intercepts, slopes, and sigmas\n\nbike_brms |> \n  spread_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  select(starts_with(\"b_\"), sigma)\n## # A tibble: 20,000 × 3\n##    b_Intercept b_temp_feel_c sigma\n##          <dbl>         <dbl> <dbl>\n##  1       3457.          80.5 1281.\n##  2       3525.          80.3 1240.\n##  3       3470.          80.1 1259.\n##  4       3500.          80.3 1215 \n##  5       3521.          80.7 1336.\n##  6       3569.          86.7 1272.\n##  7       3562.          87.6 1290.\n##  8       3412.          76.7 1271.\n##  9       3475.          87.9 1329.\n## 10       3490.          74.2 1246.\n## # … with 19,990 more rows\n\nLet’s take just the first draw and plug the original dataset into that model to calculate \\(\\mu\\), then draw from a random normal distribution using \\(\\mu\\) and \\(\\sigma\\):\n\nfirst_draw <- bike_brms |> \n  spread_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  slice(1)\n\none_rep <- bikes |> \n  mutate(mu = first_draw$b_Intercept + (first_draw$b_temp_feel_c * temp_feel_c),\n         y_rep = rnorm(500, mu, first_draw$sigma))\n\none_rep |> \n  select(temp_feel, temp_feel_c, rides, y_rep) |> \n  head(5)\n##   temp_feel temp_feel_c rides      y_rep\n## 1  64.72625   -4.417453   654 2942.93116\n## 2  49.04645  -20.097253  1229 1116.57747\n## 3  51.09098  -18.052723  1454 1708.97944\n## 4  52.63430  -16.509403  1518   22.28822\n## 5  50.79551  -18.348193  1362 2349.27186\n\nAnd we can plot the two distributions to compare the model to the actual data:\n\nggplot(one_rep, aes(x = y_rep)) +\n  geom_density(color = \"lightblue\") + \n  geom_density(aes(x = rides), color = \"darkblue\")\n\n\n\n\nChoose a bunch of posterior draws, plug them in, and you’ve got a homegrown pp_check()!\n\nlotsa_draws <- bike_brms |> \n  spread_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  slice_sample(n = 25) |> \n  mutate(id = 1:n())\n\nlotsa_reps <- lotsa_draws |> \n  mutate(mu = map2(b_Intercept, b_temp_feel_c, ~.x + .y * bikes$temp_feel_c),\n         y_rep = map2(mu, sigma, ~rnorm(500, .x, .y))) |> \n  unnest(y_rep)\n\nggplot(lotsa_reps, aes(x = y_rep)) +\n  geom_density(aes(group = id), color = \"lightblue\", size = 0.25) + \n  geom_density(data = bikes, aes(x = rides), \n               color = \"darkblue\", size = 1)\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\nAnd here’s pp_check() for comparison:\n\npp_check(bike_brms, type = \"dens_overlay\", ndraws = 25)\n\n\n\n\nHeck yes.\nThe plot looks okay-ish. It picks up the average and the general range, but it doesn’t pick up the bimodality in ridership."
  },
  {
    "objectID": "bayes-rules/10-chapter.html#how-accurate-are-the-posterior-predictive-models",
    "href": "bayes-rules/10-chapter.html#how-accurate-are-the-posterior-predictive-models",
    "title": "10: Evaluating regression models",
    "section": "10.3: How accurate are the posterior predictive models?",
    "text": "10.3: How accurate are the posterior predictive models?\nA good model should be able to accurately predict new values of $Y. Here are 3 ways to evaluate the quality of predictions:\n\n10.3.1: Posterior predictive summaries\nHow well does the model predict the data we used to build the model? We can check the fit for October 22, 2012 for fun:\n\noctober22 <- bikes |> \n  filter(date == \"2012-10-22\") |> \n  select(temp_feel, rides)\noctober22\n##   temp_feel rides\n## 1  75.46478  6228\n\nThe temperature that day was 75˚, so let’s simulate the posterior predictive distribution for a day that’s 75˚:\n\nrstanarmbrmsStan\n\n\n\nbike_rstanarm |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = october22$rides, color = \"orange\")\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\nbike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = october22$rides, color = \"orange\")\n\n\n\n\n\n\n\npredict_75_stan <- bike_stan_samples |> \n  spread_draws(beta0, beta1, sigma) |> \n  mutate(mu = beta0 + (beta1 * (75 - temp_details$scaled_center)),\n         y_new = rnorm(n(), mu, sigma))\n\npredict_75_stan |> \n  ggplot(aes(x = y_new)) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = october22$rides, color = \"orange\")\n\n\n\n\n\n\n\nThat orange line is kind of far out there—is that a problem? We can calculate the prediction error, or the distance between the observed \\(Y\\) and the posterior predictive mean \\(Y'\\), or \\(Y - Y'\\). In this case we under-predicted rids. There were 6,228 actual rides; the model only predicted 4,000ish:\n\nbike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ungroup() |> \n  summarize(actual_rides = october22$rides, \n            avg_prediction = mean(.prediction),\n            error = october22$rides - avg_prediction)\n## # A tibble: 1 × 3\n##   actual_rides avg_prediction error\n##          <int>          <dbl> <dbl>\n## 1         6228          3963. 2265.\n\nWe can also think about the relative distance or error by dividing by the standard deviation. In this standardized scale, values beyond 2 or 3 standard deviations are pretty far off. Here we’re 1.76 standard deviations off from the mean, which is fine I guess:\n\nbike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ungroup() |> \n  summarize(actual_rides = october22$rides, \n            avg_prediction = mean(.prediction),\n            error = october22$rides - avg_prediction,\n            error_scaled = error / sd(.prediction))\n## # A tibble: 1 × 4\n##   actual_rides avg_prediction error error_scaled\n##          <int>          <dbl> <dbl>        <dbl>\n## 1         6228          3972. 2256.         1.76\n\nPretty much every observed point falls within the 95% prediction interval, but that range is big (range of ≈4000 rides!)\n\nbike_brms |> \n  add_predicted_draws(newdata = bikes) |> \n  ungroup() |> \n  mutate(error = rides - .prediction) |> \n  summarize(mae = median(abs(error)),\n            mae_scaled = median(abs(error / sd(.prediction))))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1 1252.      0.792\n\n\nbike_brms |> \n  add_predicted_draws(newdata = bikes, ndraws = 100) |> \n  ggplot(aes(x = temp_feel)) +\n  stat_interval(aes(y = .prediction, color_ramp = stat(level)), alpha = 0.25,\n                .width = c(0.5, 0.89, 0.95), color = clrs[3]) +\n  geom_point(aes(y = rides), size = 2, pch = 21, color = \"white\", fill = \"black\")\n## Warning: `stat(level)` was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(level)` instead.\n\n\n\n\nWe can get more official measures with bayesrules::prediction_summary(), but it only works with rstanarm:\n\nbike_rstanarm |> prediction_summary(data = bikes)\n##        mae mae_scaled within_50 within_95\n## 1 988.5532  0.7703406     0.434     0.968\n\n\n\n10.3.2: Cross validation\nWe can use cross validation with \\(k\\) folds: hold out 50 rows for testing, use 450 rows for training, to that a bunch of times with different holdout rows. Leave-one-out CV builds the model with 499 rows, holding just one out as the test data.\n\nrstanarmbrmsStan\n\n\n\nset.seed(84735)\ncv_procedure <- prediction_summary_cv(\n  model = bike_rstanarm, data = bikes, k = 10)\n\n\ncv_procedure$folds\n##    fold       mae mae_scaled within_50 within_95\n## 1     1  990.2897  0.7710331      0.46      0.98\n## 2     2  965.8851  0.7434397      0.42      1.00\n## 3     3  950.3186  0.7298511      0.42      0.98\n## 4     4 1018.1896  0.7905312      0.46      0.98\n## 5     5 1161.8012  0.9085403      0.36      0.96\n## 6     6  937.3706  0.7322373      0.46      0.94\n## 7     7 1269.8300  1.0054441      0.32      0.96\n## 8     8 1111.8921  0.8606423      0.36      1.00\n## 9     9 1098.8982  0.8676561      0.40      0.92\n## 10   10  786.3265  0.6053804      0.56      0.96\n\n\ncv_procedure$cv\n##       mae mae_scaled within_50 within_95\n## 1 1029.08  0.8014756     0.422     0.968\n\n\n\n\nkfold_brms <- bikes |> \n  crossv_kfold(k = 10) |> \n  mutate(model = map(train, ~{\n    brm(\n      bf(rides ~ temp_feel_c),\n      data = .,\n      family = gaussian(),\n      prior = priors,\n      chains = 4, iter = 5000*2, seed = BAYES_SEED, \n      backend = \"cmdstanr\", refresh = 0\n    )\n  }))\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n\n\nmae_kfold_brms <- kfold_brms |> \n  mutate(predictions = map2(test, model, ~{\n    .x |> \n      as_tibble() |> \n      add_predicted_draws(.y) |> \n      ungroup()\n  })) |> \n  mutate(mae = map(predictions, ~{\n    .x |> \n      mutate(error = rides - .prediction) |> \n      summarize(mae = median(abs(error)),\n                mae_scaled = median(abs(error / sd(.prediction))))\n  }))\n\n\nmae_kfold_brms |> \n  select(mae) |> \n  unnest(mae)\n## # A tibble: 10 × 2\n##      mae mae_scaled\n##    <dbl>      <dbl>\n##  1 1247.      0.783\n##  2 1249.      0.817\n##  3 1190.      0.753\n##  4 1230.      0.745\n##  5 1254.      0.811\n##  6 1322.      0.833\n##  7 1309.      0.802\n##  8 1298.      0.834\n##  9 1180.      0.747\n## 10 1274.      0.837\n\n\nmae_kfold_brms |> \n  select(mae) |> \n  unnest(mae) |> \n  summarize(across(everything(), mean))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1 1255.      0.796\n\n\n\n\nkfold_stan <- bikes |> \n  crossv_kfold(k = 10) |> \n  mutate(model = map(train, ~{\n    # Stan likes working with data frames, not whatever the kfold split is\n    df <- as_tibble(.)\n    \n    bike_stan_better$sample(\n      data = list(n = nrow(df), Y = df$rides, X = df$temp_feel_c),\n      parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n      refresh = 0, seed = BAYES_SEED\n    )\n  }))\n\nPhew this is complex and creates a massively huge R object (like multiple GBs when trying to write it as RDS), and it makes Quarto choke when rendering, so I don’t actually run it here. Here’s the code though—it works when running interactively in RStudio (but takes a long time still).\n\nmae_kfold_stan <- kfold_stan |> \n  mutate(predictions = map2(test, model, ~{\n    df <- .x |> as_tibble()\n    \n    draws <- .y |> \n      spread_draws(beta0, beta1, sigma)\n    \n    df |> \n      mutate(mu = map(temp_feel_c, ~draws$beta0 + draws$beta1 * .),\n             yrep = map(mu, ~rnorm(length(.), ., draws$sigma))) |> \n      unnest(yrep)\n  })) |> \n  mutate(mae = map(predictions, ~{\n    .x |> \n      mutate(error = rides - yrep) |> \n      summarize(mae = median(abs(error)),\n                mae_scaled = median(abs(error / sd(yrep))))\n  }))\n\n\nmae_kfold_stan |> \n  select(mae) |> \n  unnest(mae)\n\n\n# This is the saved RDS object that contains mae_kfold_stan |> select(mae) |> unnest(mae)\nmae_kfold_stan_summary\n## # A tibble: 10 × 2\n##      mae mae_scaled\n##    <dbl>      <dbl>\n##  1 1386.      0.896\n##  2 1202.      0.786\n##  3 1246.      0.779\n##  4 1211.      0.736\n##  5 1238.      0.769\n##  6 1266.      0.803\n##  7 1306.      0.843\n##  8 1277.      0.823\n##  9 1207.      0.756\n## 10 1225.      0.787\n\n\nmae_kfold_stan |> \n  select(mae) |> \n  unnest(mae) |> \n  summarize(across(everything(), mean))\n\n\nmae_kfold_stan_summary |> \n  summarize(across(everything(), mean))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1 1256.      0.798\n\n\n\n\n\n\n10.3.3: Expected log-predictive density (ELPD)\nThe intuition behind the expected log-predictive density (or ELPD) is that we want higher values of the pdf at observed data point within the posterior for each prediction. Like, for instance, say we have these two pdfs for two predicted values, with the actual value of \\(Y\\) marked at the line:\n\np1 <- ggplot() +\n  geom_function(fun = ~dnorm(., 0, 1)) +\n  annotate(geom = \"segment\", x = 0.2, xend = 0.2, y = -Inf, yend = dnorm(0.2, 0, 1),\n           linetype = 21) +\n  annotate(geom = \"label\", x = 0.2, y = 0.01, label = \"Actual y\", fontface = \"bold\") +\n  xlim(c(-4, 4)) +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  labs(x = \"Posterior predictive distribution for a single Y′\", y = \"Density\",\n       title = \"Scenario 1\")\n\np2 <- ggplot() +\n  geom_function(fun = ~dnorm(., 0, 1)) +\n  annotate(geom = \"segment\", x = 1.8, xend = 1.8, y = -Inf, yend = dnorm(1.8, 0, 1),\n           linetype = 21) +\n  annotate(geom = \"label\", x = 1.8, y = 0.01, label = \"Actual y\", fontface = \"bold\") +\n  xlim(c(-4, 4)) +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  labs(x = \"Posterior predictive distribution for a single Y′\", y = \"Density\",\n       title = \"Scenario 2\")\n\np1 | p2\n\n\n\n\nIn this case, scenario 1 is better and more accurate—the actual observed value of y fits more nicely in the posterior predictive distribution for that y. We can assign a numerical value to the length of that line (or the value of the pdf at that point). For whatever reason, Bayesians log that distance, so we want the average log posterior predictive density at each new data point. We do this with leave-one-out CV (using the one hold-out data point as the actual data in the scenario above).\nThese ELPD values are on a weird meaningless scale that you can’t really interpret directly. You can compare them across models, though, and higher is better.\nCalculating the log likelihood is tricky and has to be defined in the Stan model. rstanarm and brms handle this automatically; with raw Stan, it has to be done in the generated quantities block (like Monica Alexander does here)\n\nrstanarmbrmsStan\n\n\n\nloo(bike_rstanarm)\n## \n## Computed from 20000 by 500 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo  -4289.0 13.1\n## p_loo         2.5  0.2\n## looic      8578.1 26.2\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n\n\n\n\nloo(bike_brms)\n## \n## Computed from 20000 by 500 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo  -4289.0 13.1\n## p_loo         2.5  0.2\n## looic      8578.0 26.2\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n\n\n\n\nbike_stan_samples$loo()\n## \n## Computed from 20000 by 500 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo  -4289.0 13.1\n## p_loo         2.5  0.2\n## looic      8578.0 26.2\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details."
  },
  {
    "objectID": "bayes-rules/10-chapter.html#how-good-is-the-mcmc-simulation-vs.-how-good-is-the-model",
    "href": "bayes-rules/10-chapter.html#how-good-is-the-mcmc-simulation-vs.-how-good-is-the-model",
    "title": "10: Evaluating regression models",
    "section": "10.4: How good is the MCMC simulation vs. how good is the model?",
    "text": "10.4: How good is the MCMC simulation vs. how good is the model?\nIn general with all these diagnostics, we’re exploring two different questions:\n\nHow good is the MCMC simulation? Is it long enough, are the chains well-mixed, does it converge, etc.?\nHow good is the model? Are the assumptions reasonable, is the model fair, and does it create good predictions?"
  },
  {
    "objectID": "bayes-rules/11-chapter.html",
    "href": "bayes-rules/11-chapter.html",
    "title": "11: Extending the normal regression model",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/11-chapter.html#the-general-setup",
    "href": "bayes-rules/11-chapter.html#the-general-setup",
    "title": "11: Extending the normal regression model",
    "section": "The general setup",
    "text": "The general setup\nWe want to know the relationship between morning and afternoon temperatures in Wollongong and Uluru:\n\nggplot(weather_WU, aes(x = temp9am, y = temp3pm)) +\n  geom_point()\n\n\n\n\nLooks linear enough. Let’s make a model with vague priors (just in rstanarm and brms for now; no need for raw Stan here):\n\\[\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\beta_2, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_{i1} \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(25, 5) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\nrstanarmbrms\n\n\n\nweather_simple_rstanarm <- stan_glm(\n  temp3pm ~ temp9am,\n  data = weather_WU,\n  family = gaussian(),\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\n\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b, coef = \"temp9am\"),\n            prior(exponential(1), class = sigma))\n\nweather_simple_brms <- brm(\n  bf(temp3pm ~ temp9am),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\n\n\nWe’ll skip all the diagnostics for now—they cover them in the book. What really matters is how well this model fits the data, and it’s not great. We’ve got a weird hump in the data that’s not accounted for in the model. We’ve got to do something about that.\n\npp_check(weather_simple_brms, ndraws = 25)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI went into super detail in chapters 9 and 10, with examples in rstanarm, brms, and raw Stan, and with manual calculation of core Bayesian things, like posterior predictive checks, k-fold cross validation, and a bunch of other things.\nI don’t do that in this chapter, since I’ve been doing Bayesian regression stuff for years. Here I just do rstanarm and brms versions of everything and pre-written checks like pp_check(); no raw Stan here."
  },
  {
    "objectID": "bayes-rules/11-chapter.html#utilizing-a-categorical-predictor",
    "href": "bayes-rules/11-chapter.html#utilizing-a-categorical-predictor",
    "title": "11: Extending the normal regression model",
    "section": "11.1: Utilizing a categorical predictor",
    "text": "11.1: Utilizing a categorical predictor\nWhat if we just use location to predict afternoon temperatures? It’s an important factor:\n\nggplot(weather_WU, aes(x = temp3pm, fill = location)) +\n  geom_density(color = NA) +\n  scale_fill_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"3 PM temperature\", fill = NULL)\n\n\n\n\nWe can include it as a term in the model:\n\\[\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_{i1} \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(25, 5) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\nUnderstanding the priors\nThese all look reasonable.\n\nrstanarmbrms\n\n\n\nweather_location_only_prior_rstanarm <- stan_glm(\n  temp3pm ~ location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0,\n  prior_PD = TRUE\n)\n\n\nweather_WU |> \n  add_predicted_draws(weather_location_only_prior_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 10), class = b, coef = \"locationWollongong\"),\n            prior(exponential(1), class = sigma))\n\nweather_location_only_prior_brms <- brm(\n  bf(temp3pm ~ location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  sample_prior = \"only\"\n)\n## Start sampling\n\n\nweather_WU |> \n  add_predicted_draws(weather_location_only_prior_brms, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n\n\n\n\n\n\n\n\nSimulating the posterior\n\nrstanarmbrms\n\n\n\nModel\n\nweather_location_only_rstanarm <- stan_glm(\n  temp3pm ~ location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\ntidy(weather_location_only_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 4 × 5\n##   term               estimate std.error conf.low conf.high\n##   <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)           29.7      0.548    29.0      30.4 \n## 2 locationWollongong   -10.3      0.777   -11.3      -9.30\n## 3 sigma                  5.48     0.279     5.14      5.86\n## 4 mean_PPD              24.6      0.547    23.9      25.3\n\n\n\nCoefficient posteriors\n\nweather_location_only_rstanarm |> \n  gather_draws(`(Intercept)`, locationWollongong, sigma) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nPosterior predictive check\nEw!\n\npp_check(weather_location_only_rstanarm, n = 25)\n\n\n\n\n\n\n\n\nModel\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 10), class = b, coef = \"locationWollongong\"),\n            prior(exponential(1), class = sigma))\n\nweather_location_only_brms <- brm(\n  bf(temp3pm ~ location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\ntidy(weather_location_only_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## # A tibble: 3 × 5\n##   term               estimate std.error conf.low conf.high\n##   <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)           29.7      0.544    29.0      30.4 \n## 2 locationWollongong   -10.3      0.762   -11.2      -9.29\n## 3 sd__Observation        5.44     0.272     5.10      5.79\n\n\n\nCoefficient posteriors\n\nweather_location_only_brms |> \n  gather_draws(b_Intercept, b_locationWollongong, sigma) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nPosterior predictive check\nNot great!\n\npp_check(weather_location_only_brms, ndraws = 25)\n\n\n\n\n\n\n\n\n\n\nPosterior prediction\nWhat’s the predicted afternoon temperature in these two cities?\n\nrstanarmbrms\n\n\n\nweather_location_only_rstanarm |> \n  predicted_draws(newdata = tibble(location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)\n\n\n\n\n\n\n\nweather_location_only_brms |> \n  predicted_draws(newdata = tibble(location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/11-chapter.html#utilizing-two-predictors",
    "href": "bayes-rules/11-chapter.html#utilizing-two-predictors",
    "title": "11: Extending the normal regression model",
    "section": "11.2: Utilizing two predictors",
    "text": "11.2: Utilizing two predictors\nThere’s a recognizable cluster in the data here: location. Wollongong has lower afternoon temperatures than Uluru. That makes sense—Wollongong is a coastal city near Sydney and Uluru is right in the middle of Australia in the desert.\n\nggplot(weather_WU, aes(x = temp9am, y = temp3pm, color = location)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM temperature\", y = \"3 PM temperature\", color = NULL)\n\n\n\n\nSince location is binary, including it as a covariate will shift the intercept up and down\n\\[\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\beta_2, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_{i1} + \\beta_2 X_{i2} \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(25, 5) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\beta_{2} &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\nUnderstanding the priors\nThese all look reasonable.\n\nrstanarmbrms\n\n\n\nweather_location_prior_rstanarm <- stan_glm(\n  temp3pm ~ temp9am + location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0,\n  prior_PD = TRUE\n)\n\n\np1 <- weather_WU |> \n  add_predicted_draws(weather_location_prior_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n# Feed add_epred_draws() brand new data instead of the original data, just for fun\nprior_draws_rstanarm <- weather_WU |> \n  group_by(location) |> \n  summarize(min = min(temp9am),\n            max = max(temp9am)) |> \n  mutate(temp9am = map2(min, max, ~seq(.x, .y, 1))) |> \n  unnest(temp9am) |> \n  add_epred_draws(weather_location_prior_rstanarm, ndraws = 100)\n\np2 <- prior_draws_rstanarm |> \n  ggplot(aes(x = temp9am, y = .epred)) +\n  geom_line(aes(group = paste(location, .draw), color = location), alpha = 0.2) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM temperature\", y = \"3 PM temperature\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\np1 | p2\n\n\n\n\n\n\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b, coef = \"temp9am_c\"),\n            prior(normal(0, 10), class = b, coef = \"locationWollongong\"),\n            prior(exponential(1), class = sigma))\n\nweather_location_prior_brms <- brm(\n  bf(temp3pm ~ temp9am_c + location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  sample_prior = \"only\"\n)\n## Start sampling\n\n\np1 <- weather_WU |> \n  add_predicted_draws(weather_location_prior_brms, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n# Feed add_epred_draws() brand new data instead of the original data, just for fun\nprior_draws_brms <- weather_WU |> \n  group_by(location) |> \n  summarize(min = min(temp9am_c),\n            max = max(temp9am_c)) |> \n  mutate(temp9am_c = map2(min, max, ~seq(.x, .y, 1))) |> \n  unnest(temp9am_c) |> \n  add_epred_draws(weather_location_prior_brms, ndraws = 100) |> \n  mutate(temp9am = temp9am_c + unscaled$temp9am_centered$scaled_center)\n\np2 <- prior_draws_brms |> \n  ggplot(aes(x = temp9am, y = .epred)) +\n  geom_line(aes(group = paste(location, .draw), color = location), alpha = 0.2) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM temperature\", y = \"3 PM temperature\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\np1 | p2\n\n\n\n\n\n\n\n\n\nSimulating the posterior\n\nrstanarmbrms\n\n\n\nModel\n\nweather_location_rstanarm <- stan_glm(\n  temp3pm ~ temp9am + location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\ntidy(weather_location_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 5 × 5\n##   term               estimate std.error conf.low conf.high\n##   <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)          11.3      0.671    10.5      12.2  \n## 2 temp9am               0.857    0.0291    0.820     0.895\n## 3 locationWollongong   -7.06     0.355    -7.51     -6.60 \n## 4 sigma                 2.38     0.121     2.23      2.54 \n## 5 mean_PPD             24.6      0.236    24.3      24.9\n\n\n\nCoefficient posteriors\n\nweather_location_rstanarm |> \n  gather_draws(`(Intercept)`, temp9am, locationWollongong, sigma) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4], clrs[5]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nFitted draws\n\nweather_WU |> \n  add_linpred_draws(weather_location_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = temp9am, y = temp3pm, color = location)) +\n  geom_point(data = weather_WU, size = 0.5) +\n  geom_line(aes(y = .linpred, group = paste(location, .draw)), alpha = 0.2, size = 0.5) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM temperature\", y = \"3 PM temperature\", color = NULL)\n\n\n\n\n\n\nPosterior predictive check\nMuch better!\n\npp_check(weather_location_rstanarm, n = 25)\n\n\n\n\n\n\n\n\nModel\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b, coef = \"temp9am_c\"),\n            prior(normal(0, 10), class = b, coef = \"locationWollongong\"),\n            prior(exponential(1), class = sigma))\n\nweather_location_brms <- brm(\n  bf(temp3pm ~ temp9am_c + location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\ntidy(weather_location_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## Warning in tidy.brmsfit(weather_location_brms, conf.int = TRUE, conf.level =\n## 0.8): some parameter names contain underscores: term naming may be unreliable!\n## # A tibble: 4 × 5\n##   term               estimate std.error conf.low conf.high\n##   <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)          28.1      0.243    27.8      28.4  \n## 2 temp9am_c             0.857    0.0294    0.820     0.895\n## 3 locationWollongong   -7.05     0.355    -7.50     -6.59 \n## 4 sd__Observation       2.37     0.119     2.23      2.53\n\n\n\nCoefficient posteriors\n\nweather_location_brms |> \n  gather_draws(b_Intercept, b_temp9am_c, b_locationWollongong, sigma) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4], clrs[5]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nFitted draws\n\nweather_WU |> \n  add_linpred_draws(weather_location_brms, ndraws = 100) |> \n  ggplot(aes(x = temp9am, y = temp3pm, color = location)) +\n  geom_point(data = weather_WU, size = 0.5) +\n  geom_line(aes(y = .linpred, group = paste(location, .draw)), alpha = 0.2, size = 0.5) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM temperature\", y = \"3 PM temperature\", color = NULL)\n\n\n\n\n\n\nPosterior predictive check\nMuch better!\n\npp_check(weather_location_brms, ndraws = 25)\n\n\n\n\n\n\n\n\n\n\nPosterior prediction\nWhat if it’s 10˚ at 9 AM in both cities? What’s the predicted afternoon temperature?\n\nrstanarmbrms\n\n\n\nweather_location_rstanarm |> \n  predicted_draws(newdata = expand_grid(temp9am = 10, \n                                        location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)\n\n\n\n\n\n\n\nweather_location_brms |> \n  predicted_draws(newdata = expand_grid(temp9am_c = 10 - unscaled$temp9am_centered$scaled_center, \n                                        location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/11-chapter.html#utilizing-interaction-terms",
    "href": "bayes-rules/11-chapter.html#utilizing-interaction-terms",
    "title": "11: Extending the normal regression model",
    "section": "11.3: Utilizing interaction terms",
    "text": "11.3: Utilizing interaction terms\nUsing location as an indicator variable made sense for 9 AM temperatures—the relationship was the same in both locations, but one was just shifted up compared to the other. But that’s not always the case! Here’s 9 AM humidity. It’s wildly different across the two locations (again because Uluru is in the middle of the desert and Wollongong is on the coast).\n\nggplot(weather_WU, aes(x = humidity9am, y = temp3pm, color = location)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM humidity\", y = \"3 PM temperature\", color = NULL)\n\n\n\n\nWe can estimate a different slope for Wollongong by including an interaction term, which makes the model a little more complicated:\n\\[\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\beta_2, \\beta_3, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 (X_{i1} \\times X_{i2}) \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(25, 5) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\beta_{2} &\\sim \\mathcal{N}(0, 10) \\\\\n\\beta_{3} &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\nUnderstanding the priors\nThese look fine I guess. Though only really for the autoscaled rstanarm priors. The super vague brms priors lead to some wild predictions.\n\nrstanarmbrms\n\n\n\nweather_interaction_prior_rstanarm <- stan_glm(\n  temp3pm ~ humidity9am + location + humidity9am:location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0,\n  prior_PD = TRUE\n)\n\n\np1 <- weather_WU |> \n  add_predicted_draws(weather_interaction_prior_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n# Feed add_epred_draws() brand new data instead of the original data, just for fun\nprior_draws_rstanarm <- weather_WU |> \n  group_by(location) |> \n  summarize(min = min(humidity9am),\n            max = max(humidity9am)) |> \n  mutate(humidity9am = map2(min, max, ~seq(.x, .y, 1))) |> \n  unnest(humidity9am) |> \n  add_epred_draws(weather_interaction_prior_rstanarm, ndraws = 100)\n\np2 <- prior_draws_rstanarm |> \n  ggplot(aes(x = humidity9am, y = .epred)) +\n  geom_line(aes(group = paste(location, .draw), color = location), alpha = 0.2) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM humidity\", y = \"3 PM temperature\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\np1 | p2\n\n\n\n\n\n\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b, coef = \"humidity9am_c\"),\n            prior(normal(0, 10), class = b, coef = \"locationWollongong\"),\n            prior(normal(0, 2.5), class = b, coef = \"humidity9am_c:locationWollongong\"),\n            prior(exponential(1), class = sigma))\n\nweather_interaction_prior_brms <- brm(\n  bf(temp3pm ~ humidity9am_c + location + humidity9am_c:location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  sample_prior = \"only\"\n)\n## Start sampling\n\n\np1 <- weather_WU |> \n  add_predicted_draws(weather_interaction_prior_brms, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n# Feed add_epred_draws() brand new data instead of the original data, just for fun\nprior_draws_brms <- weather_WU |> \n  group_by(location) |> \n  summarize(min = min(humidity9am_c),\n            max = max(humidity9am_c)) |> \n  mutate(humidity9am_c = map2(min, max, ~seq(.x, .y, 1))) |> \n  unnest(humidity9am_c) |> \n  add_epred_draws(weather_interaction_prior_brms, ndraws = 100) |> \n  mutate(humidity_9am = humidity9am_c + unscaled$humidity9am_centered$scaled_center)\n\np2 <- prior_draws_brms |> \n  ggplot(aes(x = humidity_9am, y = .epred)) +\n  geom_line(aes(group = paste(location, .draw), color = location), alpha = 0.2) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM humidity\", y = \"3 PM temperature\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\np1 | p2\n\n\n\n\n\n\n\n\n\nSimulating the posterior\n\nrstanarmbrms\n\n\n\nModel\n\nweather_interaction_rstanarm <- stan_glm(\n  temp3pm ~ humidity9am + location + humidity9am:location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\ntidy(weather_interaction_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 6 × 5\n##   term                           estimate std.error conf.low conf.high\n##   <chr>                             <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)                      37.6      0.905    36.4      38.8  \n## 2 humidity9am                      -0.189    0.0190   -0.214    -0.165\n## 3 locationWollongong              -21.8      2.34    -24.8     -18.9  \n## 4 humidity9am:locationWollongong    0.245    0.0370    0.198     0.292\n## 5 sigma                             4.47     0.228     4.19      4.77 \n## 6 mean_PPD                         24.6      0.449    24.0      25.1\n\n\n\nCoefficient posteriors\n\nweather_interaction_rstanarm |> \n  gather_draws(`(Intercept)`, humidity9am, locationWollongong, `humidity9am:locationWollongong`, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), \n                                 c(\"(Intercept)\", \"humidity9am\", \"locationWollongong\",\n                                   \"humidity9am:locationWollongong\", \"sigma\"))) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4], clrs[2], clrs[5]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nFitted draws\n\nweather_WU |> \n  add_linpred_draws(weather_interaction_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = humidity9am, y = temp3pm, color = location)) +\n  geom_point(data = weather_WU, size = 0.5) +\n  geom_line(aes(y = .linpred, group = paste(location, .draw)), alpha = 0.2, size = 0.5) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM humidity\", y = \"3 PM temperature\", color = NULL)\n\n\n\n\n\n\nPosterior predictive check\n\npp_check(weather_interaction_rstanarm, n = 25)\n\n\n\n\n\n\n\n\nModel\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b, coef = \"humidity9am_c\"),\n            prior(normal(0, 10), class = b, coef = \"locationWollongong\"),\n            prior(normal(0, 2.5), class = b, coef = \"humidity9am_c:locationWollongong\"),\n            prior(exponential(1), class = sigma))\n\nweather_interaction_brms <- brm(\n  bf(temp3pm ~ humidity9am_c + location + humidity9am_c:location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\ntidy(weather_interaction_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## Warning in tidy.brmsfit(weather_interaction_brms, conf.int = TRUE, conf.level =\n## 0.8): some parameter names contain underscores: term naming may be unreliable!\n## # A tibble: 5 × 5\n##   term                             estimate std.error conf.low conf.high\n##   <chr>                               <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)                        27.4      0.496    26.8      28.0  \n## 2 humidity9am_c                      -0.191    0.0190   -0.215    -0.166\n## 3 locationWollongong                 -8.68     0.772    -9.67     -7.70 \n## 4 humidity9am_c:locationWollongong    0.247    0.0370    0.200     0.295\n## 5 sd__Observation                     4.43     0.222     4.15      4.72\n\n\n\nCoefficient posteriors\n\nweather_interaction_brms |> \n  gather_draws(b_Intercept, b_humidity9am_c, b_locationWollongong, \n               `b_humidity9am_c:locationWollongong`, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), \n                                 c(\"b_Intercept\", \"b_humidity9am_c\", \"b_locationWollongong\", \n                                   \"b_humidity9am_c:locationWollongong\", \"sigma\"))) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[4], clrs[2], clrs[5]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nFitted draws\n\nweather_WU |> \n  add_linpred_draws(weather_interaction_brms, ndraws = 100) |> \n  ggplot(aes(x = humidity9am, y = temp3pm, color = location)) +\n  geom_point(data = weather_WU, size = 0.5) +\n  geom_line(aes(y = .linpred, group = paste(location, .draw)), alpha = 0.2, size = 0.5) +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  labs(x = \"9 AM humidity\", y = \"3 PM temperature\", color = NULL)\n\n\n\n\n\n\nPosterior predictive check\n\npp_check(weather_interaction_brms, ndraws = 25)\n\n\n\n\n\n\n\n\n\n\nPosterior prediction\nWhat if there’s 50% humidity at 9 AM in both cities? What’s the predicted afternoon temperature?\n\nrstanarmbrms\n\n\n\nweather_interaction_rstanarm |> \n  predicted_draws(newdata = expand_grid(humidity9am = 50, \n                                        location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)\n\n\n\n\n\n\n\nweather_interaction_brms |> \n  predicted_draws(newdata = expand_grid(humidity9am_c = 50 - unscaled$humidity9am_centered$scaled_center, \n                                        location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/11-chapter.html#utilizing-more-than-2-predictors",
    "href": "bayes-rules/11-chapter.html#utilizing-more-than-2-predictors",
    "title": "11: Extending the normal regression model",
    "section": "11.4: Utilizing more than 2 predictors",
    "text": "11.4: Utilizing more than 2 predictors\nLet’s go wild and use everything!\n\\[\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\beta_4 X_{i4} + \\beta_5 X_{i5}\\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(25, 5) \\\\\n\\beta_{1} \\dots \\beta_5 &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\nUnderstanding the priors\nThese look fine to me. brms priors could be better, but whatever.\n\nrstanarmbrms\n\n\n\nweather_full_prior_rstanarm <- stan_glm(\n  temp3pm ~ temp9am + humidity9am + windspeed9am + pressure9am + location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0,\n  prior_PD = TRUE\n)\n\n\nweather_WU |> \n  add_predicted_draws(weather_full_prior_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n\n\n\n\n\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(1), class = sigma))\n\nweather_full_prior_brms <- brm(\n  bf(temp3pm ~ temp9am_c + humidity9am_c + windspeed9am_c + pressure9am_c + location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  sample_prior = \"only\"\n)\n## Start sampling\n\n\nweather_WU |> \n  add_predicted_draws(weather_full_prior_brms, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted 3 PM temperature\", y = \"Density\")\n\n\n\n\n\n\n\n\n\nSimulating the posterior\n\nrstanarmbrms\n\n\n\nModel\n\nweather_full_rstanarm <- stan_glm(\n  temp3pm ~ temp9am + humidity9am + windspeed9am + pressure9am + location,\n  data = weather_WU,\n  family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n\n\ntidy(weather_full_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 8 × 5\n##   term               estimate std.error conf.low conf.high\n##   <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)         37.6     30.8      -2.07     76.8   \n## 2 temp9am              0.802    0.0366    0.756     0.850 \n## 3 humidity9am         -0.0338   0.00927  -0.0458   -0.0218\n## 4 windspeed9am        -0.0131   0.0211   -0.0398    0.0142\n## 5 pressure9am         -0.0231   0.0299   -0.0610    0.0154\n## 6 locationWollongong  -6.42     0.388    -6.92     -5.92  \n## 7 sigma                2.32     0.120     2.17      2.48  \n## 8 mean_PPD            24.6      0.233    24.3      24.9\n\n\n\nCoefficient posteriors\n\nweather_full_rstanarm |> \n  gather_draws(`(Intercept)`, temp9am, humidity9am, windspeed9am, \n               pressure9am, locationWollongong, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable),\n                                 c(\"(Intercept)\", \"temp9am\", \"humidity9am\", \"windspeed9am\", \n                                   \"pressure9am\", \"locationWollongong\", \"sigma\"))) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  guides(fill = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nPosterior predictive check\nSuper nice now\n\npp_check(weather_full_rstanarm, n = 25)\n\n\n\n\n\n\n\n\nModel\n\npriors <- c(prior(normal(25, 5), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(1), class = sigma))\n\nweather_full_brms <- brm(\n  bf(temp3pm ~ temp9am_c + humidity9am_c + windspeed9am_c + pressure9am_c + location),\n  data = weather_WU,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\ntidy(weather_full_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## Warning in tidy.brmsfit(weather_full_brms, conf.int = TRUE, conf.level = 0.8):\n## some parameter names contain underscores: term naming may be unreliable!\n## # A tibble: 7 × 5\n##   term               estimate std.error conf.low conf.high\n##   <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)         27.7      0.255    27.4      28.0   \n## 2 temp9am_c            0.804    0.0364    0.758     0.850 \n## 3 humidity9am_c       -0.0354   0.00919  -0.0471   -0.0237\n## 4 windspeed9am_c      -0.0138   0.0213   -0.0412    0.0136\n## 5 pressure9am_c       -0.0221   0.0299   -0.0604    0.0166\n## 6 locationWollongong  -6.27     0.387    -6.77     -5.77  \n## 7 sd__Observation      2.31     0.117     2.17      2.47\n\n\n\nCoefficient posteriors\n\nget_variables(weather_full_brms)\n##  [1] \"b_Intercept\"          \"b_temp9am_c\"          \"b_humidity9am_c\"     \n##  [4] \"b_windspeed9am_c\"     \"b_pressure9am_c\"      \"b_locationWollongong\"\n##  [7] \"sigma\"                \"lprior\"               \"lp__\"                \n## [10] \"accept_stat__\"        \"treedepth__\"          \"stepsize__\"          \n## [13] \"divergent__\"          \"n_leapfrog__\"         \"energy__\"\nweather_full_brms |> \n  gather_draws(b_Intercept, b_temp9am_c, b_humidity9am_c, b_windspeed9am_c,\n               b_pressure9am_c, b_locationWollongong, sigma) |> \n  ungroup() |> \n  mutate(.variable = fct_relevel(factor(.variable), \n                                 c(\"b_Intercept\", \"b_temp9am_c\", \"b_humidity9am_c\", \"b_windspeed9am_c\",\n                                   \"b_pressure9am_c\", \"b_locationWollongong\", \"sigma\"))) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  guides(fill = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Parameter posterior\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nPosterior predictive check\nDelightful.\n\npp_check(weather_full_brms, ndraws = 25)\n\n\n\n\n\n\n\n\n\n\nPosterior prediction\nWhat if there’s 50% humidity, 10 MPH of wind, pressure of 1015, and temperature of 10˚ at 9 AM in both cities? What’s the predicted afternoon temperature?\n\nrstanarmbrms\n\n\n\nweather_full_rstanarm |> \n  predicted_draws(newdata = expand_grid(humidity9am = 50, \n                                        windspeed9am = 10,\n                                        pressure9am = 1015,\n                                        temp9am = 10,\n                                        location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)\n\n\n\n\n\n\n\nweather_full_brms |> \n  predicted_draws(newdata = expand_grid(humidity9am_c = 50 - unscaled$humidity9am_centered$scaled_center, \n                                        windspeed9am_c = 10 - unscaled$windspeed9am_centered$scaled_center,\n                                        pressure9am_c = 1015 - unscaled$pressure9am_centered$scaled_center,\n                                        temp9am_c = 10 - unscaled$temp9am_centered$scaled_center,\n                                        location = c(\"Wollongong\", \"Uluru\"))) |> \n  ggplot(aes(x = .prediction, y = location, fill = location)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  labs(x = \"Predicted 3 PM temperature\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/11-chapter.html#model-evaluation-and-comparison",
    "href": "bayes-rules/11-chapter.html#model-evaluation-and-comparison",
    "title": "11: Extending the normal regression model",
    "section": "11.5: Model evaluation and comparison",
    "text": "11.5: Model evaluation and comparison\nPhew. We have 5 (actually 10) different models predicting afternoon temperature:\n\n\n\n\n\n\n\n\nweather_simple_rstanarm and weather_simple_brms\ntemp3pm ~ temp9am\n\n\n\nweather_location_only_rstanarm and weather_location_only_brms\ntemp3pm ~ location\n\n\n\nweather_location_rstanarm and weather_location_brms\ntemp3pm ~ temp9am + location\n\n\n\nweather_interaction_rstanarm and weather_interaction_brms\ntemp3pm ~ humidity9am + location +\n          humidity9am:location\n\n\nweather_full_rstanarm and weather_full_brms\ntemp3pm ~ temp9am + humidity9am +\n          windspeed9am + pressure9am +\n          location\n\n\n\n\n\nHow fair is each model?\nThat’s fine here. They’re all equally fair (and the data was collected through an automated system and seems fair)\n\n\nHow wrong is each model?\nWe can compare all the posterior predictive checks. y_rep gets better as the model gets more complex.\n\nrstanarmbrms\n\n\n\nmodels_rstanarm <- tribble(\n  ~model_name, ~model,\n  \"Temperature only\", weather_simple_rstanarm, \n  \"Location only\", weather_location_only_rstanarm, \n  \"Temperature + location\", weather_location_rstanarm, \n  \"Temperature × location\", weather_interaction_rstanarm, \n  \"Full model\", weather_full_rstanarm\n) |> \n  mutate(ppcheck = map2(model, model_name, ~pp_check(.x, n = 25) + labs(title = .y))) |> \n  mutate(loo = map(model, ~loo(.)))\n\n\nwrap_plots(models_rstanarm$ppcheck)\n\n\n\n\n\n\n\nmodels_brms <- tribble(\n  ~model_name, ~model,\n  \"Temperature only\", weather_simple_brms, \n  \"Location only\", weather_location_only_brms, \n  \"Temperature + location\", weather_location_brms, \n  \"Temperature × location\", weather_interaction_brms, \n  \"Full model\", weather_full_brms\n) |> \n  mutate(ppcheck = map2(model, model_name, ~pp_check(.x, ndraws = 25) + labs(title = .y))) |> \n  mutate(loo = map(model, ~loo(.)))\n\n\nwrap_plots(models_brms$ppcheck)\n\n\n\n\n\n\n\n\n\nHow accurate are each model’s predictions?\nWe can check these with the 3 approaches from chapter 10.\n\nVisualization\nFor the models with fewer dimensions we can visualize their predictions, like with the location-only model and the model with the interaction\n\nrstanarmbrms\n\n\n\nweather_location_only_rstanarm |> \n  add_predicted_draws(newdata = weather_WU, ndraws = 100) |> \n  ggplot(aes(x = location)) +\n  stat_dotsinterval(aes(y = .prediction, color = location, fill = location), \n                    quantiles = 300, side = \"right\", slab_size = 0, scale = 0.4) +\n  stat_dotsinterval(aes(y = temp3pm, fill = location),\n                    side = \"left\", quantiles = 100, slab_size = 0, slab_shape = 23) +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  guides(color = \"none\") +\n  labs(title = \"Actual data on left; predictions on right\",\n       y = \"Predicted afternoon temperature\", x = NULL)\n\n\n\n\n\n\n\nweather_location_only_brms |> \n  add_predicted_draws(newdata = weather_WU, ndraws = 100) |> \n  ggplot(aes(x = location)) +\n  stat_dotsinterval(aes(y = .prediction, color = location, fill = location), \n                    quantiles = 300, side = \"right\", slab_size = 0, scale = 0.4) +\n  stat_dotsinterval(aes(y = temp3pm, fill = location),\n                    side = \"left\", quantiles = 100, slab_size = 0, slab_shape = 23) +\n  scale_fill_manual(values = c(clrs[2], clrs[1]), guide = \"none\") +\n  guides(color = \"none\") +\n  labs(title = \"Actual data on left; predictions on right\",\n       y = \"Predicted afternoon temperature\", x = NULL)\n\n\n\n\n\n\n\n\nrstanarmbrms\n\n\n\nweather_interaction_rstanarm |> \n  add_predicted_draws(newdata = weather_WU, ndraws = 100) |> \n  ggplot(aes(x = temp9am)) +\n  stat_interval(aes(y = .prediction, color = location, color_ramp = stat(level)), alpha = 0.25,\n                .width = c(0.5, 0.89, 0.95)) +\n  geom_point(aes(y = temp3pm, fill = location), size = 2, pch = 21, color = \"white\") +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  scale_fill_manual(values = c(clrs[2], clrs[1]))\n## Warning: `stat(level)` was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(level)` instead.\n\n\n\n\n\n\n\nweather_interaction_brms |> \n  add_predicted_draws(newdata = weather_WU, ndraws = 100) |> \n  ggplot(aes(x = temp9am)) +\n  stat_interval(aes(y = .prediction, color = location, color_ramp = stat(level)), alpha = 0.25,\n                .width = c(0.5, 0.89, 0.95)) +\n  geom_point(aes(y = temp3pm, fill = location), size = 2, pch = 21, color = \"white\") +\n  scale_color_manual(values = c(clrs[2], clrs[1])) +\n  scale_fill_manual(values = c(clrs[2], clrs[1]))\n\n\n\n\n\n\n\nWe can’t visualize the predictions from the full model without holding a bunch of stuff constant, though. So we use numbers instead!\n\n\nCross validation\nWe could run k-fold cross validation here, but we’d need to run 100 different Stan models (10 folds for each of the 10 models) and I don’t want to do that. The book has results from the rstanarm models. It shows that the smaller temp9am + location model is the most efficient of the bunch.\n\n\nELPD\nThe full model and temperature + location models have the highest ELPD, but they’re not significantly different from each other, so they perform about the same. The Bayes Rules! authors conclude:\n\nAgain, since [the temperature + location model] is simpler and achieves similar predictive accuracy, we’d personally choose to use it over the more complicated [full model].\n\n\nrstanarmbrms\n\n\n\nloo_rstanarm <- models_rstanarm |> \n  mutate(loo_stuff = map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |> \n  select(model_name, loo_stuff) |> \n  unnest(loo_stuff) |> \n  filter(statistic == \"elpd_loo\") |> \n  arrange(desc(Estimate))\nloo_rstanarm\n## # A tibble: 5 × 4\n##   model_name             statistic Estimate    SE\n##   <chr>                  <chr>        <dbl> <dbl>\n## 1 Full model             elpd_loo     -458. 22.0 \n## 2 Temperature + location elpd_loo     -461. 23.0 \n## 3 Temperature only       elpd_loo     -568.  8.57\n## 4 Temperature × location elpd_loo     -585.  9.99\n## 5 Location only          elpd_loo     -626.  9.52\n\n\nloo_rstanarm |> \n  mutate(model_name = fct_rev(fct_inorder(model_name))) |> \n  ggplot(aes(x = Estimate, y = model_name)) +\n  geom_pointrange(aes(xmin = Estimate - 2 * SE, xmax = Estimate + 2 * SE))\n\n\n\n\n\nmodels_rstanarm |> \n  pull(loo) |> \n  set_names(models_rstanarm$model_name) |> \n  loo_compare()\n##                        elpd_diff se_diff\n## Full model                0.0       0.0 \n## Temperature + location   -3.6       4.0 \n## Temperature only       -110.9      18.1 \n## Temperature × location -127.9      22.7 \n## Location only          -168.2      21.5\n\n\n\n\nloo_brms <- models_brms |> \n  mutate(loo_stuff = map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |> \n  select(model_name, loo_stuff) |> \n  unnest(loo_stuff) |> \n  filter(statistic == \"elpd_loo\") |> \n  arrange(desc(Estimate))\nloo_brms\n## # A tibble: 5 × 4\n##   model_name             statistic Estimate    SE\n##   <chr>                  <chr>        <dbl> <dbl>\n## 1 Full model             elpd_loo     -458. 22.0 \n## 2 Temperature + location elpd_loo     -461. 23.2 \n## 3 Temperature only       elpd_loo     -568.  8.74\n## 4 Temperature × location elpd_loo     -585. 10.2 \n## 5 Location only          elpd_loo     -626.  9.71\n\n\nloo_brms |> \n  mutate(model_name = fct_rev(fct_inorder(model_name))) |> \n  ggplot(aes(x = Estimate, y = model_name)) +\n  geom_pointrange(aes(xmin = Estimate - 2 * SE, xmax = Estimate + 2 * SE))\n\n\n\n\n\nmodels_brms |> \n  pull(loo) |> \n  set_names(models_brms$model_name) |> \n  loo_compare()\n##                        elpd_diff se_diff\n## Full model                0.0       0.0 \n## Temperature + location   -3.6       4.4 \n## Temperature only       -110.9      17.9 \n## Temperature × location -127.9      22.7 \n## Location only          -168.2      21.6"
  },
  {
    "objectID": "bayes-rules/12-chapter.html",
    "href": "bayes-rules/12-chapter.html",
    "title": "12: Poisson & negative binomial regression",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/12-chapter.html#the-general-setup",
    "href": "bayes-rules/12-chapter.html#the-general-setup",
    "title": "12: Poisson & negative binomial regression",
    "section": "The general setup",
    "text": "The general setup\nWe want to model the number of LGBTQ+ anti-discrimination laws in states based on how urban a state is and its historical partisan voting patterns. Here’s the general relationship. A regular straight OLS line doesn’t fit the data well, but because the outcome is a count, and because the general relationship is curvy, Poisson regression will work.\n\nggplot(equality, aes(x = percent_urban, y = laws)) +\n  geom_point(aes(fill = historical), pch = 21, size = 4, color = \"white\") +\n  geom_smooth(aes(color = \"Poisson regression\"), se = FALSE, method = \"glm\", \n              method.args = list(family = \"poisson\")) +\n  geom_smooth(aes(color = \"Normal regression\"), se = FALSE, method = \"lm\") +\n  scale_fill_manual(values = c(clrs[6], clrs[3], clrs[2])) +\n  scale_color_manual(values = c(clrs[5], clrs[4])) +\n  labs(x = \"Percent urban\", y = \"Count of laws\", color = NULL, fill = \"Party\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "bayes-rules/12-chapter.html#building-the-poisson-regression-model",
    "href": "bayes-rules/12-chapter.html#building-the-poisson-regression-model",
    "title": "12: Poisson & negative binomial regression",
    "section": "12.1: Building the Poisson regression model",
    "text": "12.1: Building the Poisson regression model\n\nPrelude: Intuition behind GLM links\nIn Poisson models, the \\(\\lambda\\) rate must be positive. But if you model \\(\\lambda\\) with a regression model like\n\\[\n\\lambda_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots,\n\\]\nthe predicted values for \\(\\lambda\\) could be negative. So instead we can force \\(\\lambda\\) to be positive by using a log link function, or\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots\n\\]\n\np1 <- augment(lm(laws ~ percent_urban + historical, data = equality), \n              newdata = expand_grid(percent_urban = seq(0, 100, 1),\n                                    historical = c(\"dem\", \"gop\", \"swing\"))) |> \n  ggplot(aes(x = percent_urban, y = .fitted, color = historical)) +\n  geom_textline(aes(label = historical), linewidth = 1, hjust = 0.7) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2]), guide = \"none\") +\n  labs(x = \"Percent urban\", y = \"Predicted count of laws\",\n       title = \"Linear model of laws\", subtitle = \"Model predicts negative laws\") +\n  theme_minimal() +\n  theme(axis.line = element_line(),\n        legend.position = \"bottom\") +\n  coord_axes_inside()\n\np2 <- augment(glm(laws ~ percent_urban + historical, data = equality,\n                  family = poisson(link = \"log\")), \n              newdata = expand_grid(percent_urban = seq(0, 100, 1),\n                                    historical = c(\"dem\", \"gop\", \"swing\")),\n              type.predict = \"link\") |> \n  ggplot(aes(x = percent_urban, y = .fitted, color = historical)) +\n  geom_textline(aes(label = historical), linewidth = 1, hjust = 0.7) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2]), guide = \"none\") +\n  labs(x = \"Percent urban\", y = \"Predicted count of laws (log scale)\",\n       title = \"Poisson model of laws, log scale\",\n       subtitle = \"Log link forces λ to be > 0\") +\n  theme_minimal() +\n  theme(axis.line = element_line(),\n        legend.position = \"bottom\") +\n  coord_axes_inside(ylim = c(0, 4))\n\np3 <- augment(glm(laws ~ percent_urban + historical, data = equality,\n                  family = poisson(link = \"log\")), \n              newdata = expand_grid(percent_urban = seq(0, 100, 1),\n                                    historical = c(\"dem\", \"gop\", \"swing\")),\n              type.predict = \"response\") |> \n  ggplot(aes(x = percent_urban, y = .fitted, color = historical)) +\n  geom_textline(aes(label = historical), linewidth = 1, hjust = 0.7) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2]), guide = \"none\") +\n  labs(x = \"Percent urban\", y = \"Predicted count of laws\",\n       title = \"Poisson model of laws, response scale\",\n       subtitle = \"Backtransformed link keeps predictions above 0\") +\n  theme_minimal() +\n  theme(axis.line = element_line(),\n        legend.position = \"bottom\") +\n  coord_axes_inside(ylim = c(0, 30))\n\np1 | p2 | p3\n\n\n\n\n\n\nPrelude II: How to interpret Poisson coefficients\nBefore specifying priors, it’s helpful to know what these actual coefficients mean. Here’s a basic frequentist model, with coefficients logged and exponentiated:\n\nmodel_basic <- glm(laws ~ percent_urban + historical, data = equality,\n                   family = poisson(link = \"log\"))\ntidy(model_basic)\n## # A tibble: 4 × 5\n##   term            estimate std.error statistic  p.value\n##   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)       1.72     0.305        5.65 1.62e- 8\n## 2 percent_urban     0.0163   0.00357      4.56 5.15e- 6\n## 3 historicalgop    -1.51     0.135      -11.2  4.39e-29\n## 4 historicalswing  -0.609    0.105       -5.78 7.52e- 9\ntidy(model_basic, exponentiate = TRUE)\n## # A tibble: 4 × 5\n##   term            estimate std.error statistic  p.value\n##   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        5.60    0.305        5.65 1.62e- 8\n## 2 percent_urban      1.02    0.00357      4.56 5.15e- 6\n## 3 historicalgop      0.220   0.135      -11.2  4.39e-29\n## 4 historicalswing    0.544   0.105       -5.78 7.52e- 9\n\n\nFor the intercept \\(\\beta_0\\), this is the intercept on the logged scale when percent urban is 0 in historically Democratic states (since it’s the omitted base case). We can backtransform this to the response/count scale by exponentiating it: \\(e^{1.7225} = 5.599\\). That means that in a historically Democratic non-urban state, we’d expect to see 5.6 anti-discrimination laws.\nBut the most un-urban Democratic states are Maine and Vermont, each at 38% urban, so the intercept isn’t super important here.\nFor the percent urban \\(\\beta_1\\) coefficient, this is the slope of the line on the log scale. We can expect the logged number of laws in states to increase by 0.0163 for every additional percentage point of urban-ness. To make that more interpretable we can exponentiate it (\\(e^{0.0163} = 1.0164\\)), which means that a 1 percentage point increase in urban-ness is associated with 1.0164 times more anti-discrimination laws (or 1.64%)\nFor the party/historical \\(\\beta_2\\) and \\(\\beta_3\\) coefficients, these are the shifts in the logged Democratic intercept (again because it’s the omitted base case). We’d thus expect the logged number of laws in GOP states to be 1.5 lower on average. That makes no sense when logged, but if we exponentiate it (\\(e^{-1.5145} = 0.2199\\)), we find that GOP states should have 22% as many anti-discrimination laws as a Democratic state (or only 22% of what a typical Democratic state would have).\n\nOr even better, we can look at the average marginal effects for these coefficients and get an overall average slope and change in intercept across the whole range of the fitted line. Here, the average slope is 0.17 laws (not logged laws) as urban-ness increases; the average GOP difference is 14.7 laws (that’s huge!).\n\nmfx_basic <- marginaleffects(model_basic)\ntidy(mfx_basic)\n##       type          term    contrast    estimate  std.error  statistic\n## 1 response percent_urban       dY/dX   0.1718771 0.03894868   4.412913\n## 2 response    historical   gop - dem -14.7199505 1.33083188 -11.060714\n## 3 response    historical swing - dem  -8.6083989 1.47239777  -5.846517\n##        p.value     conf.low   conf.high\n## 1 1.019892e-05   0.09553912   0.2482152\n## 2 1.945431e-28 -17.32833305 -12.1115680\n## 3 5.019718e-09 -11.49424546  -5.7225522\n\n…or we can look at the average marginal effects at user-specified or representative values, like in prototypical urban and rural Democratic and Republican states:\n\nmfx_basic_typical <- model_basic |> \n  marginaleffects(newdata = datagrid(percent_urban = c(40, 90),\n                                     historical = c(\"dem\", \"gop\")),\n                  variables = \"percent_urban\",\n                  by = c(\"percent_urban\", \"historical\"))\ntidy(mfx_basic_typical)\n##       type          term    contrast percent_urban historical   estimate\n## 1 response percent_urban mean(dY/dX)            40        dem 0.17440682\n## 2 response percent_urban mean(dY/dX)            40        gop 0.03835624\n## 3 response percent_urban mean(dY/dX)            90        dem 0.39318726\n## 4 response percent_urban mean(dY/dX)            90        gop 0.08647131\n##     std.error statistic      p.value   conf.low  conf.high\n## 1 0.015033937 11.600875 4.078865e-31 0.14494084 0.20387279\n## 2 0.006152249  6.234507 4.532025e-10 0.02629806 0.05041443\n## 3 0.098768546  3.980896 6.865611e-05 0.19960447 0.58677006\n## 4 0.027393506  3.156635 1.596007e-03 0.03278103 0.14016159\n\nThis is neat! For Democratic states the backtransformed slope/effect is fairly large: a one percentage point increase (or rather, an infinitely small increase, since we’re working with instantaneous partial derivatives here), is associated with 0.17 more anti-discrimination laws in rural states and 0.39 in urban states. In Republican states, the effect is small, with 0.04 and 0.09 more laws in rural and urban states.\n\n\nPrelude III: Poisson assumptions\nPoisson models have a few important assumptions:\n\nStructure of the data: Conditioned on predictors \\(X\\), the observed data \\(Y_i\\) for each case \\(i\\) is independent of other cases like case \\(j\\)\nStructure of \\(Y\\): The outcome is a discrete count of events\nStructure of the relationship: The logged average \\(Y\\) value can be written as a linear combination of the predictors: \\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 X_{i1} + \\dots\\)\nStructure of the variability in \\(Y\\): The mean and variance in Poisson distributions is the same, so there should be more spread around the fitted line for higher values of \\(Y\\)\n\nThe first three are all straightforward and standard for GLM-type models: \\(Y\\) needs to be independent, \\(Y\\) needs to be a count, and \\(\\log(Y)\\) has to be modelable with a linear model.\nThe fourth is unique to Poisson models, though. In the Poisson distribution, the mean and the variance are the same thing, both when looking at \\(Y\\) by itself and when conditioning it on other things:\n\\[\n\\begin{aligned}\nE(Y) &= \\operatorname{Var}(Y) = \\lambda \\text{, and} \\\\\nE(Y \\mid X) &= \\operatorname{Var}(Y \\mid X) = \\lambda\n\\end{aligned}\n\\]\nWe can check that with the count of laws, both overall:\n\\[\nE(\\text{Laws}) = \\operatorname{Var}(\\text{Laws})\n\\]\n\nequality |> \n  summarise(mean = mean(laws),\n            variance = sd(laws))\n## # A tibble: 1 × 2\n##    mean variance\n##   <dbl>    <dbl>\n## 1  10.6     10.3\n\nAnd across different levels of urban-ness:\n\\[\nE(\\text{Laws} \\mid \\text{Percent urban}) = \\operatorname{Var}(\\text{Laws}\\mid \\text{Percent urban})\n\\]\n\nequality_across_urban <- equality |> \n  mutate(urban_bins = santoku::chop_quantiles(percent_urban, \n                                              c(0.25, 0.5, 0.75))) |> \n  group_by(urban_bins) |> \n  summarise(mean = mean(laws),\n            variance = sd(laws)) |> \n  mutate(percent_urban = quantile(equality$percent_urban, c(0, 0.25, 0.5, 0.75) + 0.125), \n         .after = urban_bins)\nequality_across_urban\n## # A tibble: 4 × 4\n##   urban_bins  percent_urban  mean variance\n##   <fct>               <dbl> <dbl>    <dbl>\n## 1 [0%, 25%)            56.7  6.17     6.22\n## 2 [25%, 50%)           70.2  3.5      3.03\n## 3 [50%, 75%]           77.9 12        9.37\n## 4 (75%, 100%]          90.6 20.5     11.5\n\nThat’s magical. In general, the assumptions hold pretty well. It gets a little off (underdispersed) for higher values of percent urban, but overall, the mean and variance are the same!\n\nequality |> \n  ggplot(aes(x = percent_urban, y = laws)) +\n  geom_point(size = 1, color = \"grey60\") +\n  geom_smooth(se = FALSE, method = \"glm\", method.args = list(family = \"poisson\"),\n              size = 0.5, color = \"grey40\") +\n  geom_pointrange(data = equality_across_urban, \n                  aes(y = mean, ymin = mean - variance, ymax = mean + variance),\n                  color = clrs[4])\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\nDefining the priors\nOkay cool. Now that we’ve checked the Poisson assumptions and these coefficients make sense, we can set good logical priors for the different parameters in our Poisson model.\nFor our priors, we’ll say that we think that number of anti-discrimination laws in a typical state is 7. The log of that is 2ish (\\(\\log(7) \\approx 1.95\\)). We’ll also say that this logged intercept could range ±1 around that mean, so 1–3. In the unlogged world, that means a typical state would have between 3 and 20 laws (\\(e^1 \\approx 3; e^3 \\approx 20\\)). Our prior for \\(\\beta_0\\) is thus normal(2, 0.5).\nIn the book they specify a vague normal(0, 2.5) prior for all the other coefficients and rely on rstanarm’s autoscaling to make them reflect the data better. Here for fun I’ll use brms to be more specific about the priors.\nFor urban-ness (\\(\\beta_1\\)), I think that there’s definitely a positive relationship, but it’s not going to be massive. A 10% increase in urban-ness in a typical state will probably add a couple more laws. The percent change from going from 7 (our prior intercept) to 9 is 0.3 (\\(\\frac{9 - 7}{7} = 0.286\\)). Scaling that down to the result of a 1% increase gives us a change of 0.0286, or if we think about it multiplicatively it would be 1.0286. The logged version of 1.0286 is 0.0282, so we’re looking for coefficients around that.\n\n(pct_change_10 <- (9 - 7) / 7)\n## [1] 0.2857143\n(pct_change_1 <- pct_change_10 * 0.1)\n## [1] 0.02857143\nlog(1 + pct_change_1)\n## [1] 0.02817088\n\nTo get a sense for the range around that mean, let’s pretend a typical state goes from 7 to 40 laws as it becomes a little bit more urban. That’s a huge (and probably unlikely) jump! What does that look like in logged coefficients?\n\n(pct_change_10 <- (40 - 7) / 7)\n## [1] 4.714286\n(pct_change_1 <- pct_change_10 * 0.1)\n## [1] 0.4714286\nlog(1 + pct_change_1)\n## [1] 0.3862337\n\nAn effect that big would have a coefficient of 0.386. So in general, the range of plausible coefficients doesn’t ever get too high. Like, a coefficient of 2 would imply that there would be 7.4 times the number of anti-discrimination laws (or an increase of 740%!) with just a 1 percentage point increase in urban-ness. That’s wild.\n\nexp(2)\n## [1] 7.389056\n\nSo we’ll set the prior average at 0, with a small range around it so that it goes from -2 to 2. For kicks and giggles, we’ll use a t-distribution instead of a normal distribution since the t-distribution has fatter tails and makes large coefficients more possible (maybe some states do see huge jumps? idk). You can see the fatter tails here with the blue t-distribution. Our official prior for \\(\\beta_1\\) is thus student_t(2, 0, 1).\n\n# R's built-in dt() function for t-distributions doesn't use mu and sigma, but\n# extraDistr::dlst() does. We'll set df arbitrarily to 2 here since that's what\n# McElreath did in his 7th video on robust regression :shrug:\nggplot() +\n  geom_function(fun = ~dlst(., df = 2, mu = 0, sigma = 1),\n                size = 1.5, color = clrs[1]) +\n  geom_function(fun = ~dnorm(., 0, 1), color = clrs[2], size = 0.5) +\n  xlim(c(-4, 4))\n\n\n\n\nFor the party-specific changes in intercept (\\(\\beta_2\\) and \\(\\beta_3\\)), we can conceptualize this as the number of GOP state laws as a percentage of Democratic laws. For example, we’ve already said that the typical zero-urban Democratic state has around 7 laws. Based on background knowledge of how GOP states have dealt with LGBTQ+ issues, I’m guessing that there’s a big difference in those states. What if a GOP state has just one expected law? That would be 14% (\\(\\frac{1}{7} \\approx 0.143\\)) of a typical Democratic state. With 6 laws, it would have 86% as many laws (\\(\\frac{6}{7} \\approx 0.857\\)), and so on. We’ll assume that GOP states on average will generally have fewer laws on average than Democratic states (meaning the ratio would be less than 1; if a GOP state had 8 laws compared to 7 in a Democratic state, the ratio would be 1.142). Let’s say that on average GOP states will have 60% of the laws a Democratic state would—that would imply that compared to a Democratic state with 7 laws, a GOP state would have 4 (\\(7 \\times 0.6 = 4.2\\)). The ratio could be as low as 10%, and it could maybe be positive sometimes, like 110% or 150%, but it can never be below 0. Something like this half-t-distribution:\n\ntibble(x = seq(0.1, 5, by = 0.01)) |> \n  mutate(y = dlst(x, df = 2, mu = 0.6, sigma = 2)) |>\n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[3]) +\n  geom_vline(xintercept = 1, color = \"grey50\", linetype = 21) +\n  annotate(geom = \"label\", x = 0.4, y = 0.05, \n           label = \"Fewer laws relative\\nto Democratic states\", size = 3) +\n  annotate(geom = \"label\", x = 1.6, y = 0.05, \n           label = \"More laws relative\\nto Democratic states\", size = 3) +\n  scale_x_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, 5, 0.5)) +\n  labs(x = \"% of laws in identical Democratic state\")\n\n\n\n\nThat’s cool, but we can’t use that distribution in Stan because we’re actually modelling the logged ratio. To get an idea of the general shape of the logged distribution we can log the t-distribution:\n\ntibble(x = seq(0.1, 6, by = 0.01)) |> \n  mutate(y = dlst(x, df = 2, mu = 0.6, sigma = 2)) |>\n  mutate(x = log(x)) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[3]) +\n  geom_vline(xintercept = 0, color = \"grey50\", linetype = 21) +\n  annotate(geom = \"label\", x = -0.5, y = 0.05, \n           label = \"Fewer laws relative\\nto Democratic states\", size = 3) +\n  annotate(geom = \"label\", x = 0.5, y = 0.05, \n           label = \"More laws relative\\nto Democratic states\", size = 3)\n\n\n\n\nIt’s doing some weird things on the left side of the plot because of how logs work with zero. The closer we get to 0, the bigger the logged value becomes:\n\nlog(0.1)\n## [1] -2.302585\nlog(0.01)\n## [1] -4.60517\nlog(0.00001)\n## [1] -11.51293\nlog(1e-10)\n## [1] -23.02585\n\nIt’s super unlikely that we’ll ever see a GOP state with 0.00000001% of the laws of a Democratic state, so a value like -23 on the logged scale is super implausible. A GOP state with just 1% of the laws of a Democratic state would have a logged value of -4.6051702; any lower than that is extreme.\nSo our average of 0.6 is -0.511 on the log scale. We’ll use a t-distribution again (for fat tails), and use a sigma of 2 which creates this kind of distribution with most values below 0 (so the unlogged ratio is less than 100%):\n\ntibble(x = seq(-3, 2, by = 0.01)) |> \n  mutate(y = dlst(x, df = 2, mu = log(0.6), sigma = 2)) |>\n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[3]) +\n  geom_vline(xintercept = 0, color = \"grey50\", linetype = 21) +\n  annotate(geom = \"label\", x = -0.7, y = 0.09, \n           label = \"Fewer laws relative\\nto Democratic states\", size = 3) +\n  annotate(geom = \"label\", x = 0.7, y = 0.09, \n           label = \"More laws relative\\nto Democratic states\", size = 3)\n\n\n\n\nThis still doesn’t make a ton of sense with logged values, so we can exponentiate it just to see what it looks like on the original scale of ratios:\n\ntibble(x = seq(-3, 1, by = 0.01)) |> \n  mutate(y = dlst(x, df = 2, mu = log(0.6), sigma = 2)) |>\n  mutate(x = exp(x)) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[3]) +\n  geom_vline(xintercept = 1, color = \"grey50\", linetype = 21) +\n  annotate(geom = \"label\", x = 0.7, y = 0.09, \n           label = \"Fewer laws relative\\nto Democratic states\", size = 3) +\n  annotate(geom = \"label\", x = 1.3, y = 0.09, \n           label = \"More laws relative\\nto Democratic states\", size = 3) +\n  scale_x_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, 5, 0.5)) +\n  labs(x = \"% of laws in identical Democratic state\")\n\n\n\n\nThat’s not identical to the half-t-distribution we made up earlier, and it makes tiny ratios like 1% very unlikely, but the bulk of the distribution is still around 60% as expected, so we’ll go with it. Our final prior for \\(\\beta_2\\) on the log scale is thus student_t(2, -0.5, 2).\nHistorical swing states behave a little differently. Some of them might have more laws than a typical Democratic state (like 8/7, or 1.14 or 114%); some might have fewer (like 6/7, or 0.86, or 86%). In this case we don’t know much about the direction of the distance, so we’ll say that the average ratio is 100% ± some amount:\n\ntibble(x = seq(-2, 1.5, by = 0.01)) |> \n  mutate(y = dlst(x, df = 2, mu = 0, sigma = 2)) |>\n  mutate(x = exp(x)) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[2]) +\n  geom_vline(xintercept = 1, color = \"grey50\", linetype = 21) +\n  annotate(geom = \"label\", x = 0.6, y = 0.11, \n           label = \"Fewer laws\\nrelative to\\nDemocratic\\nstates\", size = 3) +\n  annotate(geom = \"label\", x = 1.4, y = 0.11, \n           label = \"More laws\\nrelative to\\nDemocratic\\nstates\", size = 3) +\n  scale_x_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, 5, 0.5)) +\n  labs(x = \"% of laws in identical Democratic state\")\n\n\n\n\nOn a logged scale this nice and symmetrical around 0:\n\ntibble(x = seq(-2, 2, by = 0.01)) |> \n  mutate(y = dlst(x, df = 2, mu = 0, sigma = 2)) |>\n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[2]) +\n  geom_vline(xintercept = 0, color = \"grey50\", linetype = 21) +\n  annotate(geom = \"label\", x = -0.6, y = 0.11, \n           label = \"Fewer laws relative\\nto Democratic states\", size = 3) +\n  annotate(geom = \"label\", x = 0.6, y = 0.11, \n           label = \"More laws relative\\nto Democratic states\", size = 3)\n\n\n\n\nA student_t(2, 0, 2) distribution looks reasonable and vague enough, so our final prior for \\(\\beta_3\\) is student_t(2, 0, 2).\n\n\nFinally, the formal model\nPHEW OKAY so with all of that, here’s our official model and priors:\n\\[\n\\begin{aligned}\n\\text{Laws}_i &\\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\beta_0 + \\beta_1\\ \\text{Percent urban}_i + \\beta_2\\ \\text{GOP}_i + \\beta_3\\ \\text{Swing}_i \\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(2, 0.5) \\\\\n\\beta_1 &\\sim \\operatorname{Student t}(\\nu = 2, \\mu = 0, \\sigma = 1) \\\\\n\\beta_2 &\\sim \\operatorname{Student t}(\\nu = 2, \\mu = -0.5, \\sigma = 2) \\\\\n\\beta_3 &\\sim \\operatorname{Student t}(\\nu = 2, \\mu = 0, \\sigma = 2) \\\\\n\\end{aligned}\n\\]\nHow reasonable are all these priors when they’re all working together? Let’s simulate it!\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(2, 0.5), class = Intercept),\n            prior(student_t(2, 0, 1), class = b, coef = \"percent_urban\"),\n            prior(student_t(2, -0.5, 2), class = b, coef = \"historicalgop\"),\n            prior(student_t(2, 0, 2), class = b, coef = \"historicalswing\"))\n\nmodel_equality_prior_brms <- brm(\n  bf(laws ~ percent_urban + historical),\n  data = equality,\n  family = poisson(),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\nIt’s all over the place with different slopes across different historical parties, which is good:\n\nprior_draws_brms <- equality |> \n  group_by(historical) |> \n  summarize(min = min(percent_urban),\n            max = max(percent_urban)) |> \n  mutate(percent_urban = map2(min, max, ~seq(.x, .y, 1))) |> \n  unnest(percent_urban) |> \n  add_epred_draws(model_equality_prior_brms, ndraws = 100)\n\nprior_draws_brms |> \n  ggplot(aes(x = percent_urban, y = .epred)) +\n  geom_line(aes(group = paste(historical, .draw), color = historical), \n            alpha = 0.5, size = 0.5) +\n  coord_cartesian(ylim = c(0, 100)) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2])) +\n  labs(x = \"Percent urban\", y = \"Predicted number of laws\", color = \"Party\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nWe can’t specify individual parameter priors with rstanarm (???), so we’ll just do what the book does and use normal(0, 2.5) with magical autoscaling:\n\nequality_model_prior <- stan_glm(\n  laws ~ percent_urban + historical, \n  data = equality, \n  family = poisson,\n  prior_intercept = normal(2, 0.5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  chains = 4, iter = 4000, seed = 84735, refresh = 0,\n  prior_PD = TRUE\n)\n\nWhat priors did rstanarm decide were good?\n\nprior_summary(equality_model_prior)\n## Priors for model 'equality_model_prior' \n## ------\n## Intercept (after predictors centered)\n##  ~ normal(location = 2, scale = 0.5)\n## \n## Coefficients\n##   Specified prior:\n##     ~ normal(location = [0,0,0], scale = [2.5,2.5,2.5])\n##   Adjusted prior:\n##     ~ normal(location = [0,0,0], scale = [0.17,4.97,5.60])\n## ------\n## See help('prior_summary.stanreg') for more details\n\nIt decided on \\(\\mathcal{N}(0, 0.17)\\), \\(\\mathcal{N}(0, 4.97)\\), and \\(\\mathcal{N}(0, 5.6)\\), which is a lot wider than what I decided on above :shrug:.\nThose wider priors give a larger range of possible values than the narrow models earlier:\n\nprior_draws_rstanarm <- equality |> \n  group_by(historical) |> \n  summarize(min = min(percent_urban),\n            max = max(percent_urban)) |> \n  mutate(percent_urban = map2(min, max, ~seq(.x, .y, 1))) |> \n  unnest(percent_urban) |> \n  add_epred_draws(equality_model_prior, ndraws = 100)\n\nprior_draws_rstanarm |> \n  ggplot(aes(x = percent_urban, y = .epred)) +\n  geom_line(aes(group = paste(historical, .draw), color = historical), \n            alpha = 0.5, size = 0.5) +\n  coord_cartesian(ylim = c(0, 100)) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2])) +\n  labs(x = \"Percent urban\", y = \"Predicted number of laws\", color = \"Party\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "bayes-rules/12-chapter.html#simulating-the-posterior",
    "href": "bayes-rules/12-chapter.html#simulating-the-posterior",
    "title": "12: Poisson & negative binomial regression",
    "section": "12.2: Simulating the posterior",
    "text": "12.2: Simulating the posterior\nWith these informative-ish priors, we can finally fit the actual model and play with the posterior.\n\nRun the model\nFOR FUN AND EXCITEMENT AND LEARNING I wrote the model in Stan here, but I’m not going to work with its posterior samples or anything for the rest of the notebook. I just wanted to try writing a non-OLS model in Stan. It is definitely not optimized or efficient or anything, but it works and it’s neat.\n\nbrmsrstanarmRaw StanRaw Stan (GLM shortcut)\n\n\n\npriors <- c(prior(normal(2, 0.5), class = Intercept),\n            prior(student_t(2, 0, 1), class = b, coef = \"percent_urban\"),\n            prior(student_t(2, -0.5, 2), class = b, coef = \"historicalgop\"),\n            prior(student_t(2, 0, 2), class = b, coef = \"historicalswing\"))\n\nmodel_equality_brms <- brm(\n  bf(laws ~ percent_urban + historical),\n  data = equality,\n  family = poisson(),\n  prior = priors,\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\n\n\nequality_model <- stan_glm(\n  laws ~ percent_urban + historical, \n  data = equality, \n  family = poisson,\n  prior_intercept = normal(2, 0.5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  chains = 4, iter = 4000, seed = 84735, refresh = 0\n)\n\n\n\nThere are different ways to model GLMs in Stan. First we can use the more traditional mathy approach of calculating \\(\\lambda\\) as a function of the intercept and all the slopes multiplied by all the Xs, then exponentiating the \\(\\lambda\\), then feeding the unlogged \\(\\lambda\\) to poisson() in Stan. This is precisely what the mathematical model says to do, but it involves manual matrix multiplication.\n\n\n\n12-stan/equality-manual.stan\n\ndata {\n  int<lower=0> n;  // Number of rows\n  int<lower=0> k;  // Number of predictors\n  matrix[n,k] X;   // Predictors\n  array[n] int Y;  // Outcome variable\n}\n\nparameters {\n  real alpha;\n  vector[k] beta;\n}\n\ntransformed parameters {\n  array[n] real log_lambda;\n  array[n] real<lower=0> lambda;\n  \n  for (i in 1:n) {\n    // We can be super explicit about the whole equation, expanding it to \n    // beta1*x1 + beta2*x2 + ..., or alternatively, we can use dot_product() to \n    // multiply all the betas and Xs at once\n    log_lambda[i] = alpha + beta[1] * X[i,1] + beta[2] * X[i,2] + beta[3] * X[i,3];\n    // log_lambda[i] = alpha + dot_product(X[i], beta);\n    \n    lambda[i] = exp(log_lambda[i]);\n  }\n}\n\nmodel {\n  alpha ~ normal(2, 0.5);\n  beta[1] ~ student_t(2, 0, 1);\n  beta[2] ~ student_t(2, -0.5, 2);\n  beta[3] ~ student_t(2, 0, 2);\n  \n  Y ~ poisson(lambda);\n}\n\ngenerated quantities {\n  array[n] int Y_rep;\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n    log_lik[i] = poisson_lpmf(Y[i] | lambda[i]);\n    Y_rep[i] = poisson_rng(lambda[i]);\n  }\n}\n\n\n\nequality_stan_manual <- cmdstan_model(\"12-stan/equality-manual.stan\")\n\n\n# Build a matrix of predictors for Stan\nX <- model.matrix(~ 1 + percent_urban + historical, data = equality)[,-1]\n\nequality_samples_manual <- equality_stan_manual$sample(\n  data = list(n = nrow(equality), \n              Y = equality$laws, \n              X = X,\n              k = ncol(X)),\n  parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 1.4 seconds.\n## Chain 3 finished in 1.5 seconds.\n## Chain 2 finished in 1.9 seconds.\n## Chain 4 finished in 1.9 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 1.7 seconds.\n## Total execution time: 2.0 seconds.\n\nI’m not going to work with these Stan models in the rest of the notebook here because it’s a hassle, but just to check that they worked, here are the coefficients, the LOO stats, and pp_check():\n\nequality_samples_manual$print(\n  variables = c(\"alpha\", \"beta[1]\", \"beta[2]\", \"beta[3]\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable  mean median   sd  2.5% 97.5%\n##   alpha    1.78   1.78 0.26  1.27  2.28\n##   beta[1]  0.02   0.02 0.00  0.01  0.02\n##   beta[2] -1.53  -1.52 0.13 -1.79 -1.27\n##   beta[3] -0.61  -0.61 0.10 -0.82 -0.42\n\nequality_samples_manual$loo()\n## \n## Computed from 20000 by 49 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -194.2 20.2\n## p_loo        17.7  4.6\n## looic       388.5 40.3\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     47    95.9%   1013      \n##  (0.5, 0.7]   (ok)        2     4.1%   127       \n##    (0.7, 1]   (bad)       0     0.0%   <NA>      \n##    (1, Inf)   (very bad)  0     0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\n\nequality_samples_manual |> \n  spread_draws(Y_rep[i]) |> \n  slice_sample(n = 25) |> \n  mutate(id = 1:n()) |> \n  ggplot(aes(x = Y_rep)) +\n  geom_density(aes(group = id), color = \"lightblue\", size = 0.25) +\n  geom_density(data = equality, aes(x = laws), color = \"darkblue\", size = 1)\n\n\n\n\n\n\nInstead of manually doing the matrix multiplication, Stan has shortcut functions specifically for running. The poisson_log_glm() function, for instance, takes a matrix of predictors, the intercept, and the coefficients, and deals with all the math and multiplication automatically.\n\n\n\n12-stan/equality.stan\n\ndata {\n  int<lower=0> n;  // Number of rows\n  int<lower=0> k;  // Number of predictors\n  matrix[n,k] X;   // Predictors\n  array[n] int Y;  // Outcome variable\n}\n\nparameters {\n  real alpha;\n  vector[k] beta;\n}\n\nmodel {\n  // Priors\n  alpha ~ normal(2, 0.5);\n  beta[1] ~ student_t(2, 0, 1);\n  beta[2] ~ student_t(2, -0.5, 2);\n  beta[3] ~ student_t(2, 0, 2);\n  \n  // Model\n  Y ~ poisson_log_glm(X, alpha, beta);\n}\n\ngenerated quantities {\n  array[n] int Y_rep;\n  vector[n] log_lik;\n\n  vector[n] lambda_hat = alpha + X * beta;\n  \n  for (i in 1:n) {\n    // We can use the shortcut poisson_log_glm_lpmf, which works just like \n    // poisson_log_glm from earlier\n    log_lik[i] = poisson_log_glm_lpmf({Y[i]} | X[i,], alpha, beta);\n\n    // Or we can use poisson_log_lpmf and feed it lambda_hat\n    // log_lik[i] = poisson_log_lpmf(Y[i] | lambda_hat[i]);\n\n    // Posterior predictive distribution\n    Y_rep[i] = poisson_log_rng(lambda_hat[i]);\n  }\n}\n\n\n\nequality_stan <- cmdstan_model(\"12-stan/equality.stan\")\n\n\nX <- model.matrix(~ 1 + percent_urban + historical, data = equality)[,-1]\n\nequality_samples <- equality_stan$sample(\n  data = list(n = nrow(equality), \n              Y = equality$laws, \n              X = X,\n              k = ncol(X)),\n  parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.7 seconds.\n## Chain 3 finished in 0.7 seconds.\n## Chain 2 finished in 0.9 seconds.\n## Chain 4 finished in 0.9 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.8 seconds.\n## Total execution time: 1.0 seconds.\n\n\nequality_samples$print(\n  variables = c(\"alpha\", \"beta[1]\", \"beta[2]\", \"beta[3]\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable  mean median   sd  2.5% 97.5%\n##   alpha    1.78   1.79 0.26  1.28  2.27\n##   beta[1]  0.02   0.02 0.00  0.01  0.02\n##   beta[2] -1.52  -1.52 0.13 -1.79 -1.27\n##   beta[3] -0.61  -0.61 0.10 -0.82 -0.41\nequality_samples$loo()\n## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.\n## \n## Computed from 20000 by 49 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -194.5 20.3\n## p_loo        18.0  4.8\n## looic       389.0 40.7\n## ------\n## Monte Carlo SE of elpd_loo is NA.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     48    98.0%   963       \n##  (0.5, 0.7]   (ok)        0     0.0%   <NA>      \n##    (0.7, 1]   (bad)       1     2.0%   38        \n##    (1, Inf)   (very bad)  0     0.0%   <NA>      \n## See help('pareto-k-diagnostic') for details.\n\nequality_samples |> \n  spread_draws(Y_rep[i]) |> \n  slice_sample(n = 25) |> \n  mutate(id = 1:n()) |> \n  ggplot(aes(x = Y_rep)) +\n  geom_density(aes(group = id), color = \"lightblue\", size = 0.25) +\n  geom_density(data = equality, aes(x = laws), color = \"darkblue\", size = 1)\n\n\n\n\n\n\n\n\n\nRegular diagnostics\nBefore looking at the coefficients/parameters and predictions, let’s check the diagnostics:\n\nbrmsrstanarm\n\n\n\nTrace plots\nFUZZY.\n\nmodel_equality_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n\n\n\n\n\n\nTrank plots\nNice and random\n\nmodel_equality_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank, color = factor(.chain))) +\n  stat_bin(geom = \"step\", binwidth = 250, position = position_identity(), boundary = 0) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nPosterior predicive plots\nIt seems to be overpredicting values < 10, but it does follow the general shape of the data, so that’s reassuring.\n\npp_check(model_equality_brms, ndraws = 50)\n\n\n\n\n\n\nLOO, PSIS, and WAIC\nWe don’t have too many issues with influential points with overly high Pareto k values, and loo() is generally happy:\n\nloo(model_equality_brms)\n## \n## Computed from 8000 by 49 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -195.7 20.1\n## p_loo        19.7  5.1\n## looic       391.4 40.3\n## ------\n## Monte Carlo SE of elpd_loo is 0.2.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     46    93.9%   978       \n##  (0.5, 0.7]   (ok)        3     6.1%   126       \n##    (0.7, 1]   (bad)       0     0.0%   <NA>      \n##    (1, Inf)   (very bad)  0     0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\n\nFor fun, we can recreate Figure 7.10 from Rethinking to see which points are causing some outlier weirdness:\n\nmodel_equality_brms <- add_criterion(model_equality_brms, \n                                     criterion = c(\"loo\", \"waic\"))\n## Warning: \n## 13 (26.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nbrms_diagnostics <- tibble(\n  psis = model_equality_brms$criteria$loo$diagnostics$pareto_k,\n  p_waic = model_equality_brms$criteria$waic$pointwise[, \"p_waic\"],\n  state = pull(equality, state)) |> \n  mutate(highlight = psis > 0.5 | p_waic > 1)\n\nbrms_diagnostics |> \n  ggplot(aes(x = psis, y = p_waic)) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(data = filter(brms_diagnostics, highlight),\n                  aes(label = state), seed = 1234, direction = \"y\") +\n  geom_vline(xintercept = 0.5, linetype = 32) +\n  scale_color_manual(values = c(\"grey40\", clrs[4]), guide = \"none\") +\n  labs(x = \"PSIS Pareto k\", y = \"WAIC penalty\")\n\n\n\n\n\n\n\n\nTrace plots\nStill fuzzy here too:\n\nequality_model |> \n  gather_draws(`(Intercept)`, percent_urban, historicalgop, historicalswing) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n\n\n\n\n\n\nTrank plots\nGreat\n\nequality_model |> \n  gather_draws(`(Intercept)`, percent_urban, historicalgop, historicalswing) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank, color = factor(.chain))) +\n  stat_bin(geom = \"step\", binwidth = 250, position = position_identity(), boundary = 0) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nPosterior predicive plots\nLovely\n\npp_check(equality_model, n = 50)\n\n\n\n\n\n\nLOO, PSIS, and WAIC\nInterestingly, rstanarm finds that 3 observations have bad Pareto k scores!\n\nrstanarm_loo <- loo(equality_model)\n## Warning: Found 3 observation(s) with a pareto_k > 0.7. We recommend calling 'loo' again with argument 'k_threshold = 0.7' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 3 times to compute the ELPDs for the problematic observations directly.\nrstanarm_loo\n## \n## Computed from 8000 by 49 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -196.1 20.2\n## p_loo        20.2  5.3\n## looic       392.2 40.3\n## ------\n## Monte Carlo SE of elpd_loo is NA.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     46    93.9%   851       \n##  (0.5, 0.7]   (ok)        0     0.0%   <NA>      \n##    (0.7, 1]   (bad)       3     6.1%   32        \n##    (1, Inf)   (very bad)  0     0.0%   <NA>      \n## See help('pareto-k-diagnostic') for details.\n\nFor whatever reason, Maine and Vermont are super outliers now in the rstanarm model :shrug:\n\nrstanarm_diagnostics <- tibble(\n  psis = rstanarm_loo$pointwise[, \"influence_pareto_k\"],\n  p_waic = waic(equality_model)$pointwise[, \"p_waic\"],\n  state = pull(equality, state)) |> \n  mutate(highlight = psis > 0.5 | p_waic > 1)\n## Warning: \n## 13 (26.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nrstanarm_diagnostics |> \n  ggplot(aes(x = psis, y = p_waic)) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(data = filter(rstanarm_diagnostics, highlight),\n                  aes(label = state), seed = 1234, direction = \"y\") +\n  geom_vline(xintercept = 0.5, linetype = 32) +\n  scale_color_manual(values = c(\"grey40\", clrs[4]), guide = \"none\") +\n  labs(x = \"PSIS Pareto k\", y = \"WAIC penalty\")\n\n\n\n\n\n\n\n\n\n\nELPD\nFor fun, we can compare the ELPD for the two models (more specific priors in brms; autoscaled priors in rstanarm) and see if one model performs better than the other. They’re basically identical.\n\ntribble(\n  ~model, ~stats,\n  \"Default auto-scaled priors (rstanarm)\", as_tibble(rstanarm_loo$estimates, rownames = \"statistic\"),\n  \"Careful priors (brms)\", as_tibble(model_equality_brms$criteria$loo$estimates, rownames = \"statistic\")\n) |> \n  unnest(stats) |> \n  filter(statistic == \"elpd_loo\") |> \n  ggplot(aes(x = Estimate, y = model, color = model)) +\n  geom_pointrange(aes(xmin = Estimate - 2 * SE, xmax = Estimate + 2 * SE)) +\n  scale_y_discrete(labels = scales::label_wrap(15)) +\n  scale_color_manual(values = c(clrs[5], clrs[1]), guide = \"none\") +\n  labs(x = \"ELPD\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/12-chapter.html#interpreting-the-posterior",
    "href": "bayes-rules/12-chapter.html#interpreting-the-posterior",
    "title": "12: Poisson & negative binomial regression",
    "section": "12.3: Interpreting the posterior",
    "text": "12.3: Interpreting the posterior\n\nCoefficients / parameters\nSo what do these coefficients all actually mean? We can look at the fitted draws to see the predicted count of laws across a range of urban-ness and state political party\n\nbrmsrstanarm\n\n\n\nequality %>%\n  add_epred_draws(model_equality_brms, ndraws = 50) %>%\n  ggplot(aes(x = percent_urban, y = laws, color = historical)) +\n  geom_point(data = equality, size = 1) +\n  geom_line(aes(y = .epred, group = paste(historical, .draw)), \n            size = 0.5, alpha = 0.3) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2])) +\n  labs(x = \"Percent urban\", y = \"Count of laws\", color = \"Party\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nequality %>%\n  add_epred_draws(equality_model, ndraws = 50) %>%\n  ggplot(aes(x = percent_urban, y = laws, color = historical)) +\n  geom_point(data = equality, size = 1) +\n  geom_line(aes(y = .epred, group = paste(historical, .draw)), \n            size = 0.5, alpha = 0.3) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2])) +\n  labs(x = \"Percent urban\", y = \"Count of laws\", color = \"Party\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nLike we thought with our priors, Democratic states have more laws on average than GOP or swing states, and swing states have more than GOP states. The Democratic-GOP gap is substantial. Based just on the plot of predictions ↑ there, there’s like a 15–20 law gap! Also, the count of laws is higher in urban states, also as expected.\nWe can look at the posterior distributions of the parameters/coefficients to get a more precise picture:\n\nbrmsrstanarm\n\n\nLog-scale coefficients:\n\n# There's a weird bug in broom.mixed or brms or somewhere that makes brms\n# Poisson models lose the term column here??? idk why??? tidy() works fine with\n# the rstanarm model, and parameters::parameters(model_equality_brms) shows the\n# terms fine. So here I just add them in manually with get_variables()\ncoefs_brms <- tidy(model_equality_brms) |> \n  select(-c(effect, component, group)) |> \n  mutate(term = get_variables(model_equality_brms)[1:4])\ncoefs_brms\n## # A tibble: 4 × 5\n##   term              estimate std.error conf.low conf.high\n##   <chr>                <dbl>     <dbl>    <dbl>     <dbl>\n## 1 b_Intercept         1.70     0.303    1.09       2.28  \n## 2 b_percent_urban     0.0164   0.00355  0.00967    0.0235\n## 3 b_historicalgop    -1.51     0.135   -1.78      -1.25  \n## 4 b_historicalswing  -0.609    0.104   -0.813     -0.405\n\nmodel_equality_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  mutate(.variable = factor(.variable, \n                            levels = c(\"b_Intercept\", \"b_percent_urban\", \n                                       \"b_historicalgop\", \"b_historicalswing\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[3], clrs[2]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n\n\n\n\nUnlogged coefficients:\n\ncoefs_brms |> \n  mutate(across(c(estimate, conf.low, conf.high), ~exp(.)))\n## # A tibble: 4 × 5\n##   term              estimate std.error conf.low conf.high\n##   <chr>                <dbl>     <dbl>    <dbl>     <dbl>\n## 1 b_Intercept          5.49    0.303      2.96      9.78 \n## 2 b_percent_urban      1.02    0.00355    1.01      1.02 \n## 3 b_historicalgop      0.220   0.135      0.169     0.288\n## 4 b_historicalswing    0.544   0.104      0.444     0.667\n\nmodel_equality_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  mutate(.value = exp(.value)) |> \n  mutate(.variable = factor(.variable, \n                            levels = c(\"b_Intercept\", \"b_percent_urban\", \n                                       \"b_historicalgop\", \"b_historicalswing\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[3], clrs[2]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nLog-scale coefficients:\n\ncoefs_rstanarm <- tidy(equality_model, conf.int = TRUE)\ncoefs_rstanarm\n## # A tibble: 4 × 5\n##   term            estimate std.error conf.low conf.high\n##   <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)       1.71     0.307     1.19      2.19  \n## 2 percent_urban     0.0165   0.00361   0.0108    0.0224\n## 3 historicalgop    -1.51     0.137    -1.74     -1.29  \n## 4 historicalswing  -0.608    0.103    -0.783    -0.439\n\nequality_model |> \n  gather_draws(`(Intercept)`, percent_urban, historicalgop, historicalswing) |>\n  mutate(.variable = factor(.variable, \n                            levels = c(\"(Intercept)\", \"percent_urban\", \n                                       \"historicalgop\", \"historicalswing\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[3], clrs[2]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n\n\n\n\nUnlogged coefficients:\n\ncoefs_rstanarm |> \n  mutate(across(c(estimate, conf.low, conf.high), ~exp(.)))\n## # A tibble: 4 × 5\n##   term            estimate std.error conf.low conf.high\n##   <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)        5.50    0.307      3.28      8.95 \n## 2 percent_urban      1.02    0.00361    1.01      1.02 \n## 3 historicalgop      0.220   0.137      0.175     0.274\n## 4 historicalswing    0.544   0.103      0.457     0.645\n\nequality_model |> \n  gather_draws(`(Intercept)`, percent_urban, historicalgop, historicalswing) |>\n  mutate(.value = exp(.value)) |> \n  mutate(.variable = factor(.variable, \n                            levels = c(\"(Intercept)\", \"percent_urban\", \n                                       \"historicalgop\", \"historicalswing\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[3], clrs[2]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\n\nInterpretation\nInterpretation time!\n\nThe intercept (\\(\\beta_0\\)): On the logged scale, this is the intercept when percent urban is 0 in historically Democratic states. On its own it’s meaningless; exponentiating it gives us an expected count of laws in a completely rural Democratic state. The mean unlogged posterior value here is 5.5 laws, with a 95% credible interval ranging from 3 to 9.8.\nThe percent urban coefficient (\\(\\beta_1\\)): This is the slope of the line on the logged scale. We should expect the logged number of laws in a state to increase by that amount for each additional percentage point of urban-ness. The mean posterior value is 0.0164, with a 95% credible interval ranging from 0.0097 to 0.0235. That seems really tiny. We can make it more interpretable by exponentiating it, where a 1 percentage point increase in urban-ness is associated with an average of 1.0166 times (or 1.66%) more laws, with a 95% credible interval ranging from 1.0097 to 1.0238 (0.97% to 2.38%).\nWe can make this a little more interpretable if we think in larger changes. In Bayes Rules! they say to imagine that the urban population in one state is 25 percentage points higher than another state. If that’s the case, we would expect \\(e^{25 \\times 0.0164} \\approx 1.51\\) or 51% more laws, or 1.5 times the number of laws. If that rural state had 10 laws, we’d expect 15 laws in the more urban state.\nThe historically GOP coefficient (\\(\\beta_2\\)): This is the shift in the logged intercept for historically Republican states. The logged number of laws is lower by 1.5 on average, but that doesn’t make sense on its own. After exponentiation, we can think about ratios of GOP laws to Democratic laws. Here, that ratio has a posterior mean of 0.22 with a 95% credible interval of 0.169 to 0.288, meaning that Republican states have 22% (or 16.9%–28.8%) of the count of laws in similar Democratic states. That’s a sizable difference! Compared to a Democratic state with 20 laws, a Republican state is predicted to only have 3–5 laws (20 × 0.169, 20 × 0.288). Oof.\nThe historically swing state coefficient (\\(\\beta_3\\)): This is the shift in the logged intercept for historically swing states. We interpret it the same way as the GOP shift. On the log scale it makes little sense; exponentiated, it has a posterior mean of 0.544 with a 95% credible interval of 0.444 to 0.667. That means that swing states have between 44.4%–66.67% of the laws of a Democratic state. Compared to a Democratic state with 20 laws, a swing state is predicted to have 9–13 laws (20 × 0.444, 20 × 0.667).\n\n\n\n\nMarginal effects\nUnlogging these coefficients makes interpretation a lot easier, but it would be nice to work with counts directly too. We already did that in each of those interpretation paragraphs, translating the percent-level effects to counts in hypothetical situations (rural state with 10 laws, Democratic state with 20 laws, etc.). We can be more systematic about these conversions to counts by calculating marginal effects.\nBecause this model is curvy, the slope of the fitted line changes depending on the value of percent urban. Additionally, the size of the party-specific intercept shift changes across different values of the count of laws. So instead, we can look at the overall average slope and change in intercept across the whole range of values. And because we’re working with posterior distributions, we actually get posterior distributions of marginal effects too!\nIf we look at overall marginal effect, we find that the posterior mean of the partial derivative for percent_urban on the count (or response) scale is 0.173, with a 95% credible interval of 0.1 to 0.25. That means that on an average, increasing urban-ness by one percentage point is associated with 0.1–0.25 additional laws, on average. The group contrasts are also helpful (and already on the count scale!). The average overall difference between Republican and Democratic states has a posterior mean of -14.64 laws, with a 95% credible interval of 12–17 laws, while the difference between swing states and Democratic states has a posterior mean of -8.5 laws (5.73–11.5).\n\nmfx_brms <- marginaleffects(model_equality_brms, type = \"response\")\ntidy(mfx_brms)\n##       type          term    contrast   estimate  conf.low   conf.high\n## 1 response percent_urban       dY/dX   0.173300   0.10019   0.2504355\n## 2 response    historical   gop - dem -14.635854 -17.31778 -12.0961604\n## 3 response    historical swing - dem  -8.561223 -11.50006  -5.7257059\n\nThat’s the marginal effect for the average of the whole range of urban-ness, but it’s maybe even more useful to look at the marginal effect at each possible level of percent urban.\nThis ends up being really neat! For the percent urban effect, it is small in rural states across all parties—down at 50% urban, a 1 percentage point increase in urban-ness is associated with 0.2 additional laws for Democratic states, 0.1ish for swing states, and nearly 0 for Republican states.\n\nmfx_brms_typical <- model_equality_brms |> \n  marginaleffects(newdata = datagrid(percent_urban = seq(40, 90, by = 5),\n                                     historical = c(\"dem\", \"gop\", \"swing\")),\n                  variables = c(\"percent_urban\", \"historical\")) |> \n  posteriordraws()\n\nmfx_brms_typical |> \n  filter(term != \"historical\" | !(historical %in% c(\"gop\", \"swing\"))) |> \n  mutate(historical = ifelse(term == \"historical\", \"contrast\", as.character(historical))) |> \n  mutate(term = fct_inorder(term)) |> \n  ggplot(aes(x = percent_urban, y = draw, color = historical, fill = historical)) +\n  stat_lineribbon(alpha = 0.25) +\n  scale_color_manual(values = c(clrs[6], clrs[3], clrs[2], clrs[1]),\n                     breaks = c(\"dem\", \"gop\", \"swing\"), na.value = clrs[1]) +\n  scale_fill_manual(values = c(clrs[6], clrs[3], clrs[2], clrs[1]),\n                    breaks = c(\"dem\", \"gop\", \"swing\"), na.value = clrs[1]) +\n  facet_nested_wrap(vars(term, contrast), scales = \"free_y\") +\n  labs(color = NULL, fill = NULL, x = \"Percent urban\",\n       y = \"Marginal effect or ∆ in group means\\nCount of laws\") +\n  theme(legend.position = \"bottom\")\n## Warning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\n\n\n\nMore automatic marginal effects plots\n\n\n\n\n\n↑ that’s all fancy with nested facets and manual ggplot work; we can also make these same plots with the plot_cme() function in marginaleffects:\n\nplot_cme(model_equality_brms, \n         effect = \"percent_urban\",\n         condition = c(\"percent_urban\", \"historical\"))\n\nplot_cme(model_equality_brms, \n         effect = \"historical\",\n         condition = \"percent_urban\")"
  },
  {
    "objectID": "bayes-rules/12-chapter.html#posterior-prediction",
    "href": "bayes-rules/12-chapter.html#posterior-prediction",
    "title": "12: Poisson & negative binomial regression",
    "section": "12.4: Posterior prediction",
    "text": "12.4: Posterior prediction\nWe can check how well this model predicts data by comparing the actual data of some state to the posterior predictive distribution for that state. In the book they use Minnesota, historically Democratic, fairly urban, with just 4 laws:\n\nequality |> filter(state == \"minnesota\")\n## # A tibble: 1 × 6\n##   state     region  gop_2016  laws historical percent_urban\n##   <fct>     <fct>      <dbl> <dbl> <fct>              <dbl>\n## 1 minnesota midwest     44.9     4 dem                 73.3\n\nWhat does the model predict? We can use posterior_predict() to find that out. It will return actual integer counts (since posterior predictions are on the original scale of the data):\n\nbrmsbrms without posterior_predict()rstanarm\n\n\n\nmn_pred_brms <- model_equality_brms |> \n  predicted_draws(newdata = filter(equality, state == \"minnesota\"))\n\nmn_pred_brms |> \n  ggplot(aes(x = .prediction)) +\n  stat_histinterval(slab_fill = clrs[4], slab_color = \"white\", outline_bars = TRUE,\n                    slab_size = 0.5) +\n  geom_vline(xintercept = 4)\n\n\n\n\n\n\nJust for fun, the book creates the posterior predictive distribution by hand by making the linear predictor \\(\\lambda\\) for Minnesota, then using rpois() to simulate the number of laws based on that \\(\\lambda\\). It’s the same!\n\nmodel_equality_brms |> \n  spread_draws(`^b_.*`, regex = TRUE) |> \n  mutate(log_lambda = b_Intercept + b_percent_urban*73.3 + \n           b_historicalgop*0 + b_historicalswing*0,\n         lambda = exp(log_lambda),\n         y_new = rpois(n(), lambda = lambda)) |> \n  ggplot(aes(x = y_new)) +\n  stat_histinterval(slab_fill = clrs[4], slab_color = \"white\", outline_bars = TRUE,\n                    slab_size = 0.5) +\n  geom_vline(xintercept = 4)\n\n\n\n\n\n\n\nmn_pred_rstanarm <- equality_model |> \n  predicted_draws(newdata = filter(equality, state == \"minnesota\"))\n\nmn_pred_rstanarm |> \n  ggplot(aes(x = .prediction)) +\n  stat_histinterval(slab_fill = clrs[4], slab_color = \"white\", outline_bars = TRUE,\n                    slab_size = 0.5) +\n  geom_vline(xintercept = 4)"
  },
  {
    "objectID": "bayes-rules/12-chapter.html#model-evaluation",
    "href": "bayes-rules/12-chapter.html#model-evaluation",
    "title": "12: Poisson & negative binomial regression",
    "section": "12.5: Model evaluation",
    "text": "12.5: Model evaluation\n\n1. How fair is the model?\nIt’s good.\n\n\n2. How wrong is the model?\npp_check() earlier showed that it overpredict values < 10, but it does follow the general shape of the data:\n\npp_check(model_equality_brms, ndraws = 50)\n\n\n\n\n\n\n3. How accurate are the model’s predictions?\nWe checked that with LOO, ELPD, and PSIS stuff earlier. It’s all pretty good."
  },
  {
    "objectID": "bayes-rules/12-chapter.html#negative-binomial-regression-for-overdispersed-counts",
    "href": "bayes-rules/12-chapter.html#negative-binomial-regression-for-overdispersed-counts",
    "title": "12: Poisson & negative binomial regression",
    "section": "12.6: Negative binomial regression for overdispersed counts",
    "text": "12.6: Negative binomial regression for overdispersed counts\nThis Poisson stuff is neat as long as the assumptions hold, in particular the requirement that the mean and variance of \\(Y\\) are the same. If that’s not the case, things break.\nFor instance, we can model the number of books people read per year based on whether they would like to be wise but unhappy, or happy but unwise.\n\ndata(pulse_of_the_nation, package = \"bayesrules\")\n\npulse <- pulse_of_the_nation |> \n  filter(books < 100)\n\nThe number of books people read looks Poisson-y, but with a lot more super low values than we might expect from a regular Poisson distribution\n\npulse |> \n  ggplot(aes(x = books)) +\n  geom_histogram(binwidth = 5, boundary = 0, color = \"white\", size = 0.25)\n\n\n\n\nThe mean and variance aren’t the same:\n\npulse |> \n  summarise(mean = mean(books),\n            variance = sd(books))\n## # A tibble: 1 × 2\n##    mean variance\n##   <dbl>    <dbl>\n## 1  10.9     14.1\n\nAnd they’re not the same across a range of ages and wise/unwise responses:\n\npulse |> \n  mutate(age_bins = santoku::chop_quantiles(age, \n                                            c(0.25, 0.5, 0.75))) |> \n  group_by(age_bins, wise_unwise) |> \n  summarise(mean = mean(books),\n            variance = sd(books))\n## # A tibble: 8 × 4\n## # Groups:   age_bins [4]\n##   age_bins    wise_unwise       mean variance\n##   <fct>       <fct>            <dbl>    <dbl>\n## 1 [0%, 25%)   Happy but Unwise  9.10     12.0\n## 2 [0%, 25%)   Wise but Unhappy 14.8      15.2\n## 3 [25%, 50%)  Happy but Unwise  8.34     10.3\n## 4 [25%, 50%)  Wise but Unhappy 11.4      15.3\n## 5 [50%, 75%]  Happy but Unwise  9.09     13.5\n## 6 [50%, 75%]  Wise but Unhappy 12.3      15.1\n## 7 (75%, 100%] Happy but Unwise 11.7      15.5\n## 8 (75%, 100%] Wise but Unhappy 10.9      14.4\n\nThis means that \\(Y\\) here is overdispersed, or that it has too much variability.\nWe can throw it at a Poisson model and it’ll fit just fine:\n\nmodel_pulse_poisson <- brm(\n  bf(books ~ age + wise_unwise),\n  data = pulse,\n  family = poisson(),\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\nBut if we look at a posterior predictive check, we have serious problems\n\n# oh no\npp_check(model_pulse_poisson)\n\n\n\n\nWith negative binomial models, we get to estimate two parameters: \\(\\mu\\), which is like Poisson’s \\(\\lambda\\), and \\(r\\), which is a non-negative “reciprocal dispersion” thing:\n\\[\n\\begin{aligned}\nY_u &\\sim \\operatorname{NegBin}(\\mu_i, r) \\\\\n\\log(\\mu_i) &= \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots \\\\\n\\\\\n\\beta_0, \\beta_1, \\beta_2, \\beta_{\\dots} &\\sim \\text{Some prior} \\\\\nr &\\sim \\text{Some prior} > 0\n\\end{aligned}\n\\]\n\nmodel_pulse_negbinom <- brm(\n  bf(books ~ age + wise_unwise),\n  data = pulse,\n  family = negbinomial(),\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\nThis fits almost perfectly now!\n\n# oh no\npp_check(model_pulse_negbinom)\n\n\n\n\nThe coefficients are interpreted just like Poisson ones, since negative binomial uses a log link. Here are the different scales everything:\n\nposterior_predict(), or \\(Y\\): Integers of counts of outcome\nposterior_linpred(), or \\(\\log{\\mu}\\): Logged predicted outcome\nposterior_epred() or posterior_linpred(transform = TRUE), or \\(\\mu\\): Unlogged (exponentiated) predicted outcome\n\nHere are the coefficients, both logged and unlogged:\n\ncoefs_negbinom <- tidy(model_pulse_negbinom) |> \n  select(-c(effect, component, group)) |> \n  mutate(term = get_variables(model_pulse_negbinom)[1:3])\n\ncoefs_negbinom |> \n  mutate(term = fct_inorder(term),\n         scale = \"Logged\", .before = 1) |> \n  bind_rows(\n    coefs_negbinom |> \n      mutate(across(c(estimate, conf.low, conf.high), ~exp(.)),\n             scale = \"Unlogged\")\n  ) |> \n  arrange(term)\n## # A tibble: 6 × 6\n##   scale    term                        estimate std.error conf.low conf.high\n##   <chr>    <chr>                          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 Logged   b_age                       0.000326   0.00240 -0.00443   0.00500\n## 2 Unlogged b_age                       1.00       0.00240  0.996     1.01   \n## 3 Logged   b_Intercept                 2.24       0.134    1.98      2.50   \n## 4 Unlogged b_Intercept                 9.36       0.134    7.24     12.2    \n## 5 Logged   b_wise_unwiseWisebutUnhappy 0.266      0.0800   0.113     0.424  \n## 6 Unlogged b_wise_unwiseWisebutUnhappy 1.30       0.0800   1.12      1.53\n\nWe’ll just plot the unlogged ones because thinking about logs is weird.\n\nmodel_pulse_negbinom |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  mutate(.value = exp(.value)) |> \n  mutate(.variable = factor(.variable, \n                            levels = c(\"b_Intercept\", \"b_age\", \n                                       \"b_wise_unwiseWisebutUnhappy\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[1], clrs[2], clrs[3]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\nGeneral interpretation, just to get used to working with these coefficients:\n\nThe unlogged intercept shows the expected count of books read when age is 0 and people want to be unwise but happy. The mean posterior value here is 9.3, with a 95% credible interval of 7.2–12.2.\nThe unlogged coefficient for age shows the multiplicative change in the number of books read. A one-year increase in age is associated with a posterior average 0.03% increase in books read (or 1.0003 times), with a 95% credible interval of 0.9956–1.005. That’s hardly anything. There’s mostly likely not any sort of relationship here.\nThe unlogged coefficient for wise but unhappy shows the percent or ratio of books read compared to the happy but unwise comparison group. The posterior mean is 1.3 with a 95% credible interval of 1.12–1.53, meaning that people who want to be wise but unhappy read 1.3 times (or 130%) as many books as their unwise happy counterparts. If a happy unwise person reads 10 books a year, a comparable unhappy wise person would be expected to read 13 books (or rather, somewhere between 11.2 and 15.3 books).\n\nHere’s what all these moving parts look like simultaneously:\n\npulse %>%\n  add_epred_draws(model_pulse_negbinom, ndraws = 50) %>%\n  ggplot(aes(x = age, y = books, color = wise_unwise)) +\n  geom_point(data = pulse, size = 0.5, alpha = 0.8) +\n  geom_line(aes(y = .epred, group = paste(wise_unwise, .draw)), \n            size = 0.5, alpha = 0.3) +\n  scale_color_manual(values = c(clrs[4], clrs[3])) +\n  labs(x = \"Age\", y = \"Count of books\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nThe slope is flat, so age doesn’t matter, but there is a recognizable gap in the happy/wise question—those who want to be wise read more books.\nFor fun, we can also look at the marginal effect of the happy/wise variable so we don’t have to work with percentages like 130%.\nOn average, the marginal effect of wanting to be wise over wanting to be happy is associated with almost 3 more books read per year (1.2–4.7 in the 95% credible interval):\n\nmfx_neg_binom <- marginaleffects(model_pulse_negbinom, type = \"response\")\ntidy(mfx_neg_binom)\n##       type        term                            contrast    estimate\n## 1 response         age                               dY/dX 0.003665476\n## 2 response wise_unwise Wise but Unhappy - Happy but Unwise 2.884521215\n##      conf.low  conf.high\n## 1 -0.04831247 0.05471462\n## 2  1.21990466 4.68279566\n\n\nmfx_neg_binom |> \n  posteriordraws() |> \n  filter(contrast != \"dY/dX\") |> \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = clrs[3])\n\n\n\n\nWe can also look at the gap across the full range of ages, though it’s not that exciting or interesting since the age slope is completeley flat. The 3-book boost happens at every possible age.\n\nmfx_pulse_ages <- model_pulse_negbinom |> \n  marginaleffects(newdata = datagrid(age = seq(18, 99, by = 1),\n                                     wise_unwise = c(\"Wise but Unhappy\")),\n                  variables = c(\"wise_unwise\")) |> \n  posteriordraws()\n\nmfx_pulse_ages |> \n  ggplot(aes(x = age, y = draw, color = wise_unwise, fill = wise_unwise)) +\n  stat_lineribbon(alpha = 0.25) +\n  scale_color_manual(values = c(clrs[4], clrs[3])) +\n  scale_fill_manual(values = c(clrs[4], clrs[3])) +\n  labs(color = NULL, fill = NULL, x = \"Age\",\n       y = \"∆ in group means\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`."
  },
  {
    "objectID": "bayes-rules/13-chapter.html",
    "href": "bayes-rules/13-chapter.html",
    "title": "13: Logistic regression",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/13-chapter.html#the-general-setup",
    "href": "bayes-rules/13-chapter.html#the-general-setup",
    "title": "13: Logistic regression",
    "section": "The general setup",
    "text": "The general setup\nIn this example we want to model if it’s going to rain tomorrow (raintomorrow) based on three predictors:\n\n\\(X_1\\) (humidity9am): today’s humidity at 9 AM\n\\(X_2\\) (humitidy3pm): today’s humidity at 3 PM\n\\(X_3\\) (`raintoday``): binary indicator if it rained today"
  },
  {
    "objectID": "bayes-rules/13-chapter.html#pause-odds-and-probability",
    "href": "bayes-rules/13-chapter.html#pause-odds-and-probability",
    "title": "13: Logistic regression",
    "section": "13.1: Pause: odds and probability",
    "text": "13.1: Pause: odds and probability\nProbability is the percent chance that something will happen, or \\(\\pi\\).\nThe odds of an event is the ratio of the probability that it will happen vs. the probability that it won’t happen, or\n\\[\n\\text{odds} = \\frac{\\pi}{1 - \\pi}\n\\]\nProbabilities are limited to 0–1. Odds can range from 0 to infinity. This makes interpretation a little weird.\nIf the probability of rain tomorrow is \\(\\pi = 2/3\\), the probability that it doesn’t rain is \\(1 - \\pi\\), or \\(1/3\\). The odds are thus\n\\[\n\\frac{2/3}{1/3} = 2\n\\]\n…which means that it’s twice as likely to rain than not rain.\nIf the probability of rain tomorrow is \\(\\pi = 1/3\\), the probability that it doesn’t rain is \\(1 - \\pi = 2/3\\), so the odds are\n\\[\n\\frac{1/3}{2/3} = \\frac{1}{2}\n\\]\n…which means that it’s half as likely to rain than not rain\nAnd if the probability is \\(\\pi = 1/2\\), the probability that it doesn’t rain is \\(1 - \\pi = 1/2\\), so the odds are\n\\[\n\\frac{1/2}{1/2} = 1\n\\] In general:\n\nIf odds are < 1, \\(\\pi < 0.5\\)\nIf odds are > 1, \\(\\pi > 0.5\\)\nIf odds are 1, \\(\\pi = 0.5\\)"
  },
  {
    "objectID": "bayes-rules/13-chapter.html#building-the-logistic-regression-model",
    "href": "bayes-rules/13-chapter.html#building-the-logistic-regression-model",
    "title": "13: Logistic regression",
    "section": "13.2: Building the logistic regression model",
    "text": "13.2: Building the logistic regression model\n\nPrelude: Intuition behind GLM links\nIn general GLMs let us model non-linear data in linear ways by transforming the outcome to a more linear scale. With Poisson, we logged \\(\\lambda\\) to force it to be above zero. With logistic regression, raw probabilities tend to be really curvy and sigmoidal, and they’re bound between 0 and 1, and linear regression can’t handle bounds like that.\nWe can convert the probabilities to odds to get out of the 0–1 world, but we still face substantial nonlinearities (since high probabilities approach infinity and low probabilities approach zero). We can log the odds to force the data to be more linear. Our GLM link is thus a logit function:\n\\[\n\\log \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots\n\\]\nOr more simply:\n\\[\n\\operatorname{logit}(\\pi_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots\n\\]\n\nlogit_df <- tibble(humidity9am = seq(0, 100, length.out = 101),\n                   logits = seq(-4, 6, length.out = 101)) |> \n  mutate(odds = exp(logits)) |> \n  mutate(probs = plogis(logits))\n\np1 <- ggplot(logit_df, aes(x = humidity9am, y = probs)) +\n  geom_line(size = 1, color = clrs[3]) +\n  labs(title = \"Probabilities\", \n       subtitle = \"Can't be modeled linearly; too curvy and bound between 0–1\",\n       x = \"9 AM humidity\",\n       y = TeX(\"$\\\\pi$\")) +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title.y = element_text(angle = 0, hjust = 0))\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\np2 <- ggplot(logit_df, aes(x = humidity9am, y = odds)) +\n  geom_line(size = 1, color = clrs[2]) +\n  labs(title = \"Odds\", \n       subtitle = \"No longer bound between 0–1, but still too curvy for linear models\",\n       x = \"9 AM humidity\",\n       y = TeX(\"$\\\\frac{\\\\pi}{1 - \\\\pi}$\")) +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title.y = element_text(angle = 0, hjust = 0))\n\np3 <- ggplot(logit_df, aes(x = humidity9am, y = logits)) +\n  geom_line(size = 1, color = clrs[1]) +\n  labs(title = \"Log odds (logits)\", \n       subtitle = \"Linear models are comfy and happy here\",\n       x = \"9 AM humidity\",\n       y = TeX(\"$log \\\\left( \\\\frac{\\\\pi}{1 - \\\\pi} \\\\right)$\")) +\n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title.y = element_text(angle = 0, hjust = 0))\n\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\nPrelude II: How to interpret logistic regression coefficients\n\n\n\n\n\n\nTip\n\n\n\nSee my guide here or my other guide here or this by Steven Miller for more on logistic regression coefficients.\n\n\nLogit model coefficients can be reported as log odds (or logits) or odds (or odds ratios). We can get actual probabilities if use plogis() to calculate the inverse logit of the log odds, or we can calculate marginal effects.\n\nfake_logit_data <- logit_df |> \n  mutate(raintomorrow_fake = map(probs, ~rbinom(15, 1, .))) |> \n  unnest(raintomorrow_fake)\n\nggplot(fake_logit_data, aes(x = humidity9am, y = raintomorrow_fake)) +\n  geom_dots(aes(side = ifelse(raintomorrow_fake == 1, \"bottom\", \"top\")), \n            pch = 19, color = \"grey70\", scale = 0.2) +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link = \"logit\")),\n              size = 1, color = clrs[5], se = FALSE)\n\n\n\n\n\nmodel_example <- glm(raintomorrow_fake ~ humidity9am, data = fake_logit_data,\n                     family = binomial(link = \"logit\"))\ntidy(model_example)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -3.65     0.201       -18.2 1.21e-73\n## 2 humidity9am   0.0936   0.00461      20.3 1.12e-91\ntidy(model_example, exponentiate = TRUE)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   0.0260   0.201       -18.2 1.21e-73\n## 2 humidity9am   1.10     0.00461      20.3 1.12e-91\n\nThe intercept \\(\\beta_0\\) is the log odds of the outcome when all other covariates are set to 0. In the fake data here, it’s -3.651. That makes no sense by itself, but we can exponentiate it (\\(e^{-3.651}\\)) and get an odds ratio of 0.026. That still doesn’t make sense for me (maybe some people do think in odds ratios?). We can convert the log odds value to a probability with \\(\\frac{e^{-3.651}}{1 + e^{-3.651}}\\), or plogis(-3.651) in R: 0.025. That means that in this simulated data, there’s a 2.5% probability of rain when humidity is 0%.\n\nLog odds: \\(-3.651\\)\nOdds: \\(e^{-3.651} = 0.026\\)\nProbability: \\(\\frac{e^{-3.651}}{1 + e^{-3.651}} = 0.025\\)\n\nThe slope \\(\\beta_1\\) is the slope of the line on the log odds scale. A 1 percentage point increase in humidity is associated with a 0.094 increase in the log odds of rain. That makes no d**n sense (it compels me though).\nWe can exponentiate the coefficient (\\(e^{0.094}\\)) to get an odds ratio of 1.098, or the multiplicative change in odds. This is a little more usable (though not much): a 1 percentage point increase in humidity increases the odds of rain by 9.8%.\nWe can convert this coefficient to the probability scale with a little bit of fancy math. We can’t just stick the log odds into plogis() like we did with the intercept. We need to include details from the intercept in the calculation:\n\nplogis(intercept + coefficient) - plogis(intercept)\n\nIf we do that with these coefficients like plogis(-3.651 + 0.094) - plogis(-3.651), we get 0.0024, which means that the probability of rain increases by 0.24 percentage points for every 1 percentage point increase in humidity, on average.\nThe slope is different across the whole range of 9 AM temperatures though. Averaged across the whole range of humidity, the probability of rain increases by 0.24 percentage points, but the fitted curve is flat at the edges and steep in the middle. We can use marginaleffects() to find the slope at different values of humidity:\n\nmfx_example <- marginaleffects(model_example, \n                               newdata = datagrid(humidity9am = c(0, 30, 60, 90)),\n                               by = \"humidity9am\")\nsummary(mfx_example)\n##          Term    Contrast humidity9am    Effect Std. Error z value   Pr(>|z|)\n## 1 humidity9am mean(dY/dX)           0 0.0023107  0.0003398   6.801 1.0395e-11\n## 2 humidity9am mean(dY/dX)          30 0.0196926  0.0008739  22.535 < 2.22e-16\n## 3 humidity9am mean(dY/dX)          60 0.0100879  0.0006424  15.704 < 2.22e-16\n## 4 humidity9am mean(dY/dX)          90 0.0007781  0.0001500   5.188 2.1232e-07\n##       2.5 %   97.5 %\n## 1 0.0016448 0.002977\n## 2 0.0179798 0.021405\n## 3 0.0088289 0.011347\n## 4 0.0004841 0.001072\n## \n## Model type:  glm \n## Prediction type:  response\n\nWhen humidity is 0, we get the same slope we found manually with plogis(), or 0.0023. As humidity increases, the slope increases too. When humidity is 30%, for instance, moving from 30% to 31% increases the probability of rain by 1.97 percentage points, while moving from 60% to 61% humidity increases the probability of rain by 1.01 percentage points. For fun we can plot the whole range of marginal effects:\n\n# This does the same thing automatically:\n# plot_cme(model_example, effect = \"humidity9am\", condition = \"humidity9am\")\n\nmodel_example |> \n  marginaleffects(\n    newdata = datagrid(humidity9am = seq(0, 100, by = 1)),\n    by = \"humidity9am\") |> \n  mutate(across(c(dydx, conf.low, conf.high), ~ . * 100)) |> \n  ggplot(aes(x = humidity9am, y = dydx)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.3, fill = clrs[1]) +\n  geom_line(size = 1, color = clrs[1]) +\n  labs(x = \"9 AM humidity\", \n       y = \"Percentage point increase in probability of rain\\n(marginal effect)\")\n\n\n\n\n\n\nDefining the priors\nTo make the intercept more interpretable and to make Stan happier, we’ll work with a centered version of humidity rather than the 0–100 scale.\nWe’ll say that based on our prior knowledge about weather in Perth, there’s generally a 20% chance of rain on a given day. Since the intercept here is centered, it represents a typical day with an average level of morning humidity, so we just need to convert 20% to the log odds scale, either manually or with qlogis(0.2):\n\\[\n\\log \\left( \\frac{0.2}{1 - 0.2} \\right) = -1.39\n\\]\n\nqlogis(0.2)\n## [1] -1.386294\n\nWe need a range around that number. In the book they use 0.7 for \\(\\sigma\\), making a range of -1.39 ± (2 * 0.7) = -2.786 and 0.014. If we convert those to probabilities, we get a range from 5% to 50%:\n\nplogis(qlogis(0.2) - (2 * 0.7))\n## [1] 0.05806931\nplogis(qlogis(0.2) + (2 * 0.7))\n## [1] 0.5034264\n\nThat seems fine. Here’s what that looks like in plot form:\n\np1 <- ggplot() +\n  geom_function(fun = ~dnorm(., mean = -1.39, sd = 0.7),\n                size = 1, color = clrs[2]) +\n  scale_x_continuous(limits = c(-4, 1)) +\n  scale_y_continuous(labels = NULL, breaks = NULL) +\n  labs(x = \"Log odds\", y = NULL)\n\np2 <- tibble(x = seq(-4, 1, by = 0.01)) |> \n  mutate(y = dnorm(x, mean = -1.39, sd = 0.7)) |> \n  mutate(x_prob = plogis(x)) |> \n  ggplot(aes(x = x_prob, y = y)) +\n  geom_line(size = 1, color = clrs[2]) +\n  scale_y_continuous(labels = NULL, breaks = NULL) +\n  labs(x = \"Probability\", y = NULL)\n\np1 | p2\n\n\n\n\nFor the \\(\\beta_1\\) coefficient, we have a general idea that more humidity today will increase the probability of rain tomorrow, but no idea how big that effect actually is. In the book they say that the slope is around 0.07 with a \\(\\sigma\\) of 0.035, so 0.07 ± 0.07, or 0–0.14.\nLog odds make no sense so we can convert that range to odds ratios:\n\nexp(0.07 - 0.07)\n## [1] 1\nexp(0.07 + 0.07)\n## [1] 1.150274\n\nThe odds of rain the next day thus increase by 0–15% for each percentage point increase in humidity. Here’s what that looks like:\n\np1 <- ggplot() +\n  geom_function(fun = ~dnorm(., mean = 0.07, sd = 0.07),\n                size = 1, color = clrs[3]) +\n  scale_x_continuous(limits = c(-0.35, 0.35)) +\n  scale_y_continuous(labels = NULL, breaks = NULL) +\n  labs(x = \"Log odds\", y = NULL)\n\np2 <- tibble(x = seq(-0.35, 0.35, by = 0.01)) |> \n  mutate(y = dnorm(x, mean = 0.07, sd = 0.07)) |> \n  mutate(x_odds = exp(x)) |> \n  ggplot(aes(x = x_odds, y = y)) +\n  geom_line(size = 1, color = clrs[3]) +\n  scale_y_continuous(labels = NULL, breaks = NULL) +\n  labs(x = \"Odds ratio\", y = NULL)\n\np1 | p2\n\n\n\n\n\n\nOfficial formal model\nHere’s the formal model then:\n\\[\n\\begin{aligned}\n\\text{Rain tomorrow}_i &\\sim \\operatorname{Bernoulli}(\\pi_i) \\\\\n\\operatorname{logit}(\\pi_i) &= \\beta_{0c} + \\beta_1\\ \\text{9 AM humidity}_i \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(-1.39, 0.7) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0.07, 0.035)\n\\end{aligned}\n\\]\nWe should get a sense of how reasonable these priors are by simulating them. These plots seem good and fine.\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(-1.39, 0.7), class = Intercept),\n            prior(normal(0.07, 0.035), class = b, coef = \"humidity9am_c\"))\n\nmodel_weather_prior_brms <- brm(\n  bf(raintomorrow ~ humidity9am_c),\n  data = weather,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\np1 <- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |> \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |> \n  add_epred_draws(model_weather_prior_brms, ndraws = 100) |> \n  ggplot(aes(x = humidity9am, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.5, size = 0.5, color = clrs[6]) +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\np2 <- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |> \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |> \n  add_predicted_draws(model_weather_prior_brms, ndraws = 100) |> \n  group_by(.draw) |> \n  summarize(proportion_rain = mean(.prediction == 1)) |> \n  ggplot(aes(x = proportion_rain)) +\n  geom_histogram(binwidth = 0.02, color = \"white\", fill = clrs[1]) +\n  labs(x = \"Proportion of rainy days in each draw\", y = \"Count\")\n\np1 | p2\n\n\n\n\n\n\n\nmodel_weather_prior_rstanarm <- stan_glm(\n  raintomorrow ~ humidity9am_c,\n  data = weather,\n  family = binomial(link = \"logit\"),\n  prior_intercept = normal(-1.39, 0.7),\n  prior = normal(0.07, 0.035),\n  chains = 4, iter = 4000, seed = 84735, refresh = 0,\n  prior_PD = TRUE\n)\n\n\np1 <- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |> \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |>\n  add_epred_draws(model_weather_prior_rstanarm, ndraws = 100) |> \n  ggplot(aes(x = humidity9am, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.5, size = 0.5, color = clrs[6]) +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\np2 <- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |> \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |> \n  add_predicted_draws(model_weather_prior_rstanarm, ndraws = 100) |> \n  group_by(.draw) |> \n  summarize(proportion_rain = mean(.prediction == 1)) |> \n  ggplot(aes(x = proportion_rain)) +\n  geom_histogram(binwidth = 0.02, color = \"white\", fill = clrs[1]) +\n  labs(x = \"Proportion of rainy days in each draw\", y = \"Count\")\n\np1 | p2"
  },
  {
    "objectID": "bayes-rules/13-chapter.html#simulating-the-posterior",
    "href": "bayes-rules/13-chapter.html#simulating-the-posterior",
    "title": "13: Logistic regression",
    "section": "13.3: Simulating the posterior",
    "text": "13.3: Simulating the posterior\n\nLook at the data\nTime to look at the actual data:\n\nrain_prob <- weather |> \n  mutate(humidity_bracket = cut(humidity9am, seq(10, 100, by = 10))) |> \n  group_by(humidity_bracket) |> \n  summarize(prop_rain = mean(raintomorrow == \"Yes\")) |> \n  mutate(mid = seq(15, 95, by = 10))\n  \nweather |> \n  ggplot(aes(x = humidity9am / 100, y = raintomorrow_num)) +\n  geom_dots(aes(color = raintomorrow, \n                shape = \"Data\",\n                side = ifelse(raintomorrow_num == 1, \"bottom\", \"top\")),\n            scale = 0.7, alpha = 0.5) +\n  scale_color_manual(values = c(clrs[2], clrs[3]), name = \"Rain tomorrow\",\n                     guide = guide_legend(override.aes = list(shape = 19))) +\n  scale_shape_manual(values = c(19, 15), name = NULL,\n                     guide = guide_legend(override.aes = list(linetype = 0))) +\n  geom_point(data = rain_prob, \n             aes(x = mid / 100, y = prop_rain,\n                 shape = \"Proportion of rainy days\")) +\n  # Add a logistic regression curve with a second color scale for optional bonus fun\n  ggnewscale::new_scale_color() +\n  geom_smooth(mapping = aes(color = \"Probability of rain\"),\n              method = \"glm\", method.args = list(family = binomial(link = \"logit\")),\n              size = 1, se = FALSE) +\n  scale_color_manual(values = clrs[5], name = NULL) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\n\n\n\nIn theory, the fitted model should look like the quantile-based proportions there ↑\n\n\nRun the model\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(-1.39, 0.7), class = Intercept),\n            prior(normal(0.07, 0.035), class = b, coef = \"humidity9am_c\"))\n\nmodel_weather_brms <- brm(\n  bf(raintomorrow ~ humidity9am_c),\n  data = weather,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\n\n\nmodel_weather_rstanarm <- stan_glm(\n  raintomorrow ~ humidity9am_c,\n  data = weather,\n  family = binomial(link = \"logit\"),\n  prior_intercept = normal(-1.39, 0.7),\n  prior = normal(0.07, 0.035),\n  chains = 4, iter = 4000, seed = 84735, refresh = 0\n)\n\n\n\n\n\n\nDiagnostics\n\nbrmsrstanarm\n\n\n\nTrace plots\n\nmodel_weather_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n\n\n\n\n\n\nTrank plots\n\nmodel_weather_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank, color = factor(.chain))) +\n  stat_bin(geom = \"step\", binwidth = 250, position = position_identity(), boundary = 0) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nPosterior predictive plots\n\npp_check(model_weather_brms, ndraws = 50)\n\n\n\n\n\n\nLOO, PSIS, and WAIC\n\nloo(model_weather_brms)\n## \n## Computed from 8000 by 1000 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -437.0 19.0\n## p_loo         2.2  0.2\n## looic       874.1 37.9\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n\n\n\n\n\nTrace plots\n\nmodel_weather_rstanarm |> \n  gather_draws(`(Intercept)`, humidity9am_c) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n\n\n\n\n\n\nTrank plots\n\nmodel_weather_rstanarm |> \n  gather_draws(`(Intercept)`, humidity9am_c) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank, color = factor(.chain))) +\n  stat_bin(geom = \"step\", binwidth = 250, position = position_identity(), boundary = 0) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\nPosterior predictive plots\n\npp_check(model_weather_rstanarm, n = 50)\n\n\n\n\n\n\nLOO, PSIS, and WAIC\n\nloo(model_weather_rstanarm)\n## \n## Computed from 8000 by 1000 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -437.0 19.0\n## p_loo         2.1  0.2\n## looic       874.0 38.0\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details."
  },
  {
    "objectID": "bayes-rules/13-chapter.html#interpreting-the-posterior",
    "href": "bayes-rules/13-chapter.html#interpreting-the-posterior",
    "title": "13: Logistic regression",
    "section": "13.3½: Interpreting the posterior",
    "text": "13.3½: Interpreting the posterior\nThe book is more focused on classification and prediction here and doesn’t go (much) into coefficient interpretation or (at all) into marginal effects, but I will, just for fun.\n\nCoefficients / parameters\n\nbrmsrstanarm\n\n\n\nweather |> \n  add_epred_draws(model_weather_brms, ndraws = 50) |> \n  ggplot(aes(x = humidity9am)) +\n  geom_dots(data = weather, \n            aes(y = raintomorrow_num, color = raintomorrow,\n                side = ifelse(raintomorrow_num == 1, \"bottom\", \"top\")), \n            pch = 19, scale = 0.5, alpha = 0.5) +\n  geom_line(aes(y = .epred, group = .draw),\n            size = 0.5, alpha = 0.3, color = clrs[6]) +\n  scale_color_manual(values = c(clrs[2], clrs[3]), name = \"Rain tomorrow\", guide = \"none\") +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\n\n\n\n\n\n\nweather |> \n  add_epred_draws(model_weather_rstanarm, ndraws = 50) |> \n  ggplot(aes(x = humidity9am)) +\n  geom_dots(data = weather, \n            aes(y = raintomorrow_num, color = raintomorrow,\n                side = ifelse(raintomorrow_num == 1, \"bottom\", \"top\")), \n            pch = 19, scale = 0.5, alpha = 0.5) +\n  geom_line(aes(y = .epred, group = .draw),\n            size = 0.5, alpha = 0.3, color = clrs[6]) +\n  scale_color_manual(values = c(clrs[2], clrs[3]), name = \"Rain tomorrow\", guide = \"none\") +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\n\n\n\n\n\n\n\nbrmsrstanarm\n\n\nLogit/log odds-scale coefficients:\n\n# I normally do this with broom.mixed::tidy() since I use broom::tidy() for\n# everything ordinarily, but (1) I want to try out parameters::model_parameters()\n# since it includes lots of other neat things like PD and ROPE, and (2)\n# broom.mixed::tidy() requires the `effects` argument to work and I always\n# forget to include it (.e.g. tidy(model_weather_brms, effects = \"fixed\"))\nmodel_parameters(model_weather_brms, centrality = \"mean\", dispersion = TRUE, \n                 test = FALSE, verbose = FALSE, ci_method = \"hdi\", priors = TRUE)\n## Parameter     |  Mean |       SD |         95% CI |  Rhat |     ESS |                  Prior\n## --------------------------------------------------------------------------------------------\n## (Intercept)   | -1.68 |     0.10 | [-1.85, -1.48] | 1.000 | 3986.00 | Normal (-1.39 +- 0.70)\n## humidity9am_c |  0.05 | 5.24e-03 | [ 0.04,  0.06] | 1.001 | 4570.00 |  Normal (0.07 +- 0.04)\n\nOdds ratio-scale coefficients:\n\nmodel_parameters(model_weather_brms, centrality = \"mean\", dispersion = TRUE, \n                 test = FALSE, verbose = FALSE, ci_method = \"hdi\", priors = TRUE,\n                 exponentiate = TRUE) |> \n  print(digits = 3, zap_small = TRUE)\n## Parameter     |  Mean |    SD |       95% CI |  Rhat |      ESS |                  Prior\n## ----------------------------------------------------------------------------------------\n## (Intercept)   | 0.187 | 0.095 | [0.16, 0.23] | 1.000 | 3986.000 | Normal (-1.39 +- 0.70)\n## humidity9am_c | 1.049 | 0.005 | [1.04, 1.06] | 1.001 | 4570.000 |  Normal (0.07 +- 0.04)\n\n\nmodel_weather_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  mutate(.value = exp(.value)) |> \n  mutate(.variable = factor(.variable, \n                            levels = c(\"b_Intercept\", \"b_humidity9am_c\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nLogit/log odds-scale coefficients:\n\nmodel_parameters(model_weather_rstanarm, centrality = \"mean\", dispersion = TRUE, \n                 test = FALSE, verbose = FALSE, ci_method = \"hdi\", priors = TRUE)\n## Parameter     |  Mean |       SD |         95% CI |  Rhat |     ESS |                  Prior\n## --------------------------------------------------------------------------------------------\n## (Intercept)   | -1.68 |     0.09 | [-1.87, -1.50] | 1.000 | 3956.00 | Normal (-1.39 +- 0.70)\n## humidity9am_c |  0.05 | 5.20e-03 | [ 0.04,  0.06] | 1.000 | 3960.00 |  Normal (0.07 +- 0.04)\n\nOdds ratio-scale coefficients:\n\nmodel_parameters(model_weather_rstanarm, centrality = \"mean\", dispersion = TRUE, \n                 test = FALSE, verbose = FALSE, ci_method = \"hdi\", priors = TRUE,\n                 exponentiate = TRUE) |> \n  print(digits = 3, zap_small = TRUE)\n## Parameter     |  Mean |    SD |       95% CI |  Rhat |      ESS |                  Prior\n## ----------------------------------------------------------------------------------------\n## (Intercept)   | 0.187 | 0.092 | [0.15, 0.22] | 1.000 | 3956.000 | Normal (-1.39 +- 0.70)\n## humidity9am_c | 1.049 | 0.005 | [1.04, 1.06] | 1.000 | 3960.000 |  Normal (0.07 +- 0.04)\n\n\nmodel_weather_rstanarm |> \n  gather_draws(`(Intercept)`, humidity9am_c) |>\n  mutate(.value = exp(.value)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\n\n\nInterpretation and marginal effects\nThe intercept \\(\\beta_0\\) has a posterior mean of -1.676, and it is the log odds of raining tomorrow when humidity at 9 AM is at its average value (60.9%). That number makes no sense, but if we exponentiate it (\\(e^{-1.676}\\)) we get an odds ratio of 0.187, which still doesn’t make sense for intercepts. So instead we’ll actually just convert the log odds to a probability with \\(\\frac{e^{-1.676}}{1 + e^{-1.676}}\\), or plogis(-1.676) in R, which is 0.158. This means that when 9 AM humidity is average (again, 60.9%), there’s a mean posterior probability of raining tomorrow of 15.8%, with a 95% credible interval of 13.4%–18.4%.\nThe posterior mean of the \\(\\beta_1\\) parameter for 9 AM humidity is 0.048, which again, by itself, doesn’t make much sense. Exponentiating it (\\(e^{0.048}\\)) gives us a mean posterior odds ratio of 1.049, with a 95% credible interval of 1.039–1.06, which is interpretable. A 1 percentage point increase in humidity today makes it 4.9% more likely to rain tomorrow (with a 95% credible interval of 3.9%–6.0%)\nIn addition to working with odds ratios, we can work with changes in probabilities more directly if we look at marginal effects. Since the fitted line is curvy here, we don’t have a single slope—the slope changes across the whole range of 9 AM humidity.\nAt low values of humidity, increasing it a little bit doesn’t change the slope much at all, but at higher values, like ≈75%, a 1 percentage point increase in humidity increases the probability of rain tomorrow by 1 percentage point or more.\n\nmfx_brms <- model_weather_brms |> \n  marginaleffects(newdata = datagrid(humidity9am_c = seq(0, 100, by = 1) -\n                                       unscaled$humidity9am_centered$scaled_center),\n                  variables = \"humidity9am_c\",\n                  type = \"response\") |> \n  posteriordraws() |> \n  mutate(humidity9am = humidity9am_c + unscaled$humidity9am_centered$scaled_center)\n\n\nmfx_brms |> \n  filter(humidity9am %in% c(0, 25, 50, 75, 90)) |> \n  group_by(humidity9am) |> \n  summarize(mean_slope = mean(draw),\n            conf.low = mean(conf.low),\n            conf.high = mean(conf.high)) |> \n  mutate(across(c(mean_slope, conf.low, conf.high), ~ . * 100))\n## # A tibble: 5 × 4\n##   humidity9am mean_slope conf.low conf.high\n##         <dbl>      <dbl>    <dbl>     <dbl>\n## 1           0     0.0484   0.0275    0.0750\n## 2          25     0.150    0.113     0.189 \n## 3          50     0.430    0.367     0.497 \n## 4          75     0.948    0.712     1.20  \n## 5          90     1.18     0.888     1.46\n\n\nmfx_brms |> \n  ggplot(aes(x = humidity9am, y = draw * 100)) +\n  stat_lineribbon(alpha = 0.25, fill = clrs[6], color = clrs[6]) +\n  labs(x = \"9 AM humidity\", \n       y = \"Percentage point increase in\\nprobability of rain tomorrow\")\n## Warning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`."
  },
  {
    "objectID": "bayes-rules/13-chapter.html#prediction-and-classification",
    "href": "bayes-rules/13-chapter.html#prediction-and-classification",
    "title": "13: Logistic regression",
    "section": "13.4: Prediction and classification",
    "text": "13.4: Prediction and classification\nNext we can see how well the model predicts things. In the book they predict the probability that it will rain tomorrow given that there is 99% humidity at 9 AM this morning, so we’ll do that here too, both automatically with posterior_predict() and manually with rbinom():\n\nbrmsrstanarm\n\n\nAutomatic way:\n\nhumidity_99_c <- 99 - unscaled$humidity9am_centered$scaled_center\n\nprediction_brms <- model_weather_brms |> \n  predicted_draws(newdata = tibble(humidity9am_c = humidity_99_c))\n\nprediction_brms |> \n  count(.prediction) |> \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n  ggplot(aes(x = factor(.prediction), y = n)) + \n  geom_col(aes(fill = factor(.prediction))) +\n  geom_label(aes(label = prop_nice), nudge_y = -500) +\n  scale_fill_manual(values = c(clrs[2], clrs[3]), guide = \"none\")\n\n\n\n\nManual way:\n\nhumidity_99_c <- 99 - unscaled$humidity9am_centered$scaled_center\n\nprediction_brms_manual <- model_weather_brms |> \n  spread_draws(b_Intercept, b_humidity9am_c) |> \n  mutate(log_odds = b_Intercept + b_humidity9am_c * humidity_99_c,\n         odds = exp(log_odds),\n         prob = odds / (1 + odds),\n         prob_auto = plogis(log_odds),  # Alternatively do this\n         y_new = rbinom(n(), size = 1, prob = prob))\n\nprediction_brms_manual |> \n  count(y_new) |> \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n  ggplot(aes(x = factor(y_new), y = n)) + \n  geom_col(aes(fill = factor(y_new))) +\n  geom_label(aes(label = prop_nice), nudge_y = -500) +\n  scale_fill_manual(values = c(clrs[2], clrs[3]), guide = \"none\")\n\n\n\n\n\n\n\nhumidity_99_c <- 99 - unscaled$humidity9am_centered$scaled_center\n\nprediction_rstanarm <- model_weather_rstanarm |> \n  predicted_draws(newdata = tibble(humidity9am_c = humidity_99_c))\n\nprediction_rstanarm |> \n  count(.prediction) |> \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n  ggplot(aes(x = factor(.prediction), y = n)) + \n  geom_col(aes(fill = factor(.prediction))) +\n  geom_label(aes(label = prop_nice), nudge_y = -500) +\n  scale_fill_manual(values = c(clrs[2], clrs[3]), guide = \"none\")\n\n\n\n\nManual way:\n\nhumidity_99_c <- 99 - unscaled$humidity9am_centered$scaled_center\n\nprediction_rstanarm_manual <- model_weather_rstanarm |> \n  spread_draws(`(Intercept)`, humidity9am_c) |> \n  mutate(log_odds = `(Intercept)` + humidity9am_c * humidity_99_c,\n         odds = exp(log_odds),\n         prob = odds / (1 + odds),\n         prob_auto = plogis(log_odds),  # Alternatively do this\n         y_new = rbinom(n(), size = 1, prob = prob))\n\nprediction_rstanarm_manual |> \n  count(y_new) |> \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n  ggplot(aes(x = factor(y_new), y = n)) + \n  geom_col(aes(fill = factor(y_new))) +\n  geom_label(aes(label = prop_nice), nudge_y = -500) +\n  scale_fill_manual(values = c(clrs[2], clrs[3]), guide = \"none\")\n\n\n\n\n\n\n\nBased on these predictions, if it’s 99% humidity this morning, it’s more likely to rain tomorrow than not (54ish% chance):\n\nprediction_brms |> \n  count(.prediction) |> \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop))\n## # A tibble: 2 × 6\n## # Groups:   humidity9am_c, .row [1]\n##   humidity9am_c  .row .prediction     n  prop prop_nice\n##           <dbl> <int>       <int> <int> <dbl> <chr>    \n## 1          38.1     1           0  3738 0.467 46.7%    \n## 2          38.1     1           1  4262 0.533 53.3%"
  },
  {
    "objectID": "bayes-rules/13-chapter.html#model-evaluation",
    "href": "bayes-rules/13-chapter.html#model-evaluation",
    "title": "13: Logistic regression",
    "section": "13.5 Model evaluation",
    "text": "13.5 Model evaluation\n\nHow fair is the model?\nIt’s fine.\n\n\nHow wrong is the model?\nThis is where we’d use pp_check(). We did that earlier and got this neat graph:\n\npp_check(model_weather_brms, ndraws = 50)\n\n\n\n\nThat shows that the model at least fits the shape of the underlying data, though it has some optical illusions with the density plot there, since the data is actually all only 0s and 1s.\nWe can get a more appropriate pp_check by changing the type to bars:\n\nbrmsrstanarm\n\n\n\npp_check(model_weather_brms, ndraws = 100,\n         type = \"bars\")\n\n\n\n\n\n\n\npp_check(model_weather_rstanarm, n = 100,\n         plotfun = \"bars\")\n\n\n\n\n\n\n\nAlternatively, in the book they use a histogram that shows the proportion of rainy days in each simulated dataset and compare that histogram with the observed proportion of rainy days. In this plot it shows that most simulated posterior datasets saw rain on 18% of the days, just like the real data, with some as low as 15% and others as high as 23%:\n\nbrmsrstanrm\n\n\n\npp_check(model_weather_brms, ndraws = 500,\n         type = \"stat\") +\n  labs(x = \"Probability of rain\")\n\n\n\n\n\n\n\npp_check(model_weather_rstanarm, nreps = 500,\n         plotfun = \"stat\") +\n  labs(x = \"Probability of rain\")\n\n\n\n\n\n\n\n\n\nHow accurate are the model’s posterior classifications?\nIn other types of models, we answered this by looking at the difference between actual \\(Y\\) and its posterior predictions. That’s a little different here with binary data—our posterior predictions are either right or wrong and don’t have any sort of measurable distance. So we want to know how often we’re right, not how close we are to being right.\nHere we’ll get posterior predicted draws for each row in the original dataset (8,000 draws per 1,000 rows). These are a bunch of 1s and 0s, so we’ll take the average of those 1s and 0s to get the overall proportion of predicted 1s for that day, which will be the predicted probability of tomorrow’s rain for that day. We’ll then choose some cutoff point, like 50%, to classify whether we will count that as a prediction of rain tomorrow or not.\n\nprediction_brms_all <- weather |> \n  add_predicted_draws(model_weather_brms)\n\nrain_classifications <- prediction_brms_all |> \n  group_by(.row) |> \n  summarize(rain_prob_that_day = mean(.prediction),\n            raintomorrow_actual = raintomorrow[1]) |> \n  mutate(raintomorrow_pred = rain_prob_that_day >= 0.5)\nrain_classifications\n## # A tibble: 1,000 × 4\n##     .row rain_prob_that_day raintomorrow_actual raintomorrow_pred\n##    <int>              <dbl> <fct>               <lgl>            \n##  1     1             0.128  No                  FALSE            \n##  2     2             0.078  No                  FALSE            \n##  3     3             0.163  Yes                 FALSE            \n##  4     4             0.117  No                  FALSE            \n##  5     5             0.181  No                  FALSE            \n##  6     6             0.361  No                  FALSE            \n##  7     7             0.0912 No                  FALSE            \n##  8     8             0.107  No                  FALSE            \n##  9     9             0.0842 Yes                 FALSE            \n## 10    10             0.432  Yes                 FALSE            \n## # … with 990 more rows\n\nIn these first 10 rows, none of the days predict rain the following day since their predicted probabilities are all under 0.5. But rows 3, 9, and 10 did actually rain the next day, so those predictions were wrong.\nWe can get an overall count of times the model predicted rain and was right or wrong by creating what the machine learning world calls a confusion matrix. We can do this with dplyr’s count() and tidyr’s pivot_wider(), or we can use janitor’s tabyl(). Both do the same thing.\n\nrain_classifications |> \n  count(raintomorrow_actual, raintomorrow_pred) |> \n  pivot_wider(names_from = \"raintomorrow_pred\", values_from = \"n\")\n## # A tibble: 2 × 3\n##   raintomorrow_actual `FALSE` `TRUE`\n##   <fct>                 <int>  <int>\n## 1 No                      805      9\n## 2 Yes                     172     14\n\nrain_classifications |> \n  janitor::tabyl(raintomorrow_actual, raintomorrow_pred) |> \n  janitor::adorn_totals(c(\"row\", \"col\"))\n##  raintomorrow_actual FALSE TRUE Total\n##                   No   805    9   814\n##                  Yes   172   14   186\n##                Total   977   23  1000\n\nWe can think of this confusion matrix more generally with letters assigned to specific cells, where \\(Y\\) represents the observed outcome and \\(\\hat{Y}\\) represents the predicted outcome:\n\n\n\n\n\\(\\hat{Y} = 0\\)\n\\(\\hat{Y} = 1\\)\n\n\n\n\n\\(Y = 0\\)\na\nb\n\n\n\\(Y = 1\\)\nc\nd\n\n\n\nBased on this, we correctly classified 805 No/No cases (cell \\(a\\)) and 14 Yes/Yes cases(cell d), resulting in 819 total correct predictions. That implies an overall classification accuracy rate of 81.9%. More formally, this is \\(\\frac{a + d}{a + b + c + d}\\).\nWe can also look at the true negative rate, or specificity. Out of the 814 (805 in a + 9 in b) days where it didn’t rain, we correctly classified 805, or 98.9%. Formally the specificity (true negative rate) is \\(\\frac{a}{a + b}\\).\nFinally we can also look at the true positive rate, or sensitivity. Out of the 186 (172 in c + 14 in d) days where it does rain, we correctly classified 14, or 7.5%. Oooof. We’re really good at guessing when it won’t rain; we’re awful at guessing when it will rain. Formally sensitivity (true positive rate) is \\(\\frac{c}{c + d}\\).\nWe don’t have to stick with a 50% cutoff for classification. We can increase the sensitivity by saying that we’ll predict rain if there’s a 20+% probability in a given day.\n\nrain_classifications_20 <- prediction_brms_all |> \n  group_by(.row) |> \n  summarize(rain_prob_that_day = mean(.prediction),\n            raintomorrow_actual = raintomorrow[1]) |> \n  mutate(raintomorrow_pred = rain_prob_that_day >= 0.2)\n\nrain_classifications_20 |> \n  janitor::tabyl(raintomorrow_actual, raintomorrow_pred) |> \n  janitor::adorn_totals(c(\"row\", \"col\"))\n##  raintomorrow_actual FALSE TRUE Total\n##                   No   579  235   814\n##                  Yes    66  120   186\n##                Total   645  355  1000\n\nNow we have different accuracy rates:\n\nOverall accuracy (\\(\\frac{a + d}{a + b + c + d}\\)): 69.9%\nSpecificity, or true negative rate (\\(\\frac{a}{a + b}\\)): 71.1%\nSensitivity, or true positive rate (\\(\\frac{c}{c + d}\\)): 64.5%\n\nThe sensitivity rate is waaay better than before now! But that’s at the cost of specificity:\n\nYet this improvement is not without consequences. In lowering the cut-off, we make it more difficult to predict when it won’t rain… and we’ll carry around an umbrella more often than we need to."
  },
  {
    "objectID": "bayes-rules/13-chapter.html#extending-the-model",
    "href": "bayes-rules/13-chapter.html#extending-the-model",
    "title": "13: Logistic regression",
    "section": "13.6: Extending the model",
    "text": "13.6: Extending the model\nIn the book they run a new model that also includes 3 PM humidity and an indicator for whether or not it rained that day. I’ll do that here too, but not with rstanarm. I’ll also do it with raw Stan for fun and learning.\n\nSpecify the model and priors\nWe’ll use the same model as before, but now with a couple new coefficients for 3 PM humidity and rain today. In the book they used vague priors for all the coefficients, so I’ll do that here too. (It seems like these all came from rstanarm’s autoscaled priors.)\n\\[\n\\begin{aligned}\n\\text{Rain tomorrow}_i \\sim&\\ \\operatorname{Bernoulli}(\\pi_i) \\\\\n\\operatorname{logit}(\\pi_i) =&\\ \\beta_{0c} + \\beta_1\\ \\text{9 AM humidity}_i + \\\\\n&\\ \\beta_2\\ \\text{3 PM humidity}_i + \\beta_3\\ \\text{Rain today}_i \\\\\n\\\\\n\\beta_{0c} \\sim&\\ \\mathcal{N}(-1.39, 0.7) \\\\\n\\beta_1 \\sim&\\ \\mathcal{N}(0, 0.14) \\\\\n\\beta_2 \\sim&\\ \\mathcal{N}(0, 0.15) \\\\\n\\beta_3 \\sim&\\ \\mathcal{N}(0, 6.45)\n\\end{aligned}\n\\]\n\n\nSimulate the posterior\n\nbrmsRaw Stan\n\n\n\npriors <- c(prior(normal(-1.39, 0.7), class = Intercept),\n            prior(normal(0, 0.14), class = b, coef = \"humidity9am_c\"),\n            prior(normal(0, 0.15), class = b, coef = \"humidity3pm_c\"),\n            prior(normal(0, 6.45), class = b, coef = \"raintodayYes\"))\n\nmodel_weather_full_brms <- brm(\n  bf(raintomorrow ~ humidity9am_c + humidity3pm_c + raintoday),\n  data = weather,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\n\n\n\n\n\n13-stan/weather-full.stan\n\ndata {\n  int<lower=0> n;  // Number of rows\n  int<lower=0> k;  // Number of predictors\n  matrix[n,k] X;   // Predictors\n  array[n] int Y;  // Outcome variable\n}\n\nparameters {\n  real alpha;\n  vector[k] beta;\n}\n\nmodel {\n  // Priors\n  alpha ~ normal(-1.4, 0.7);\n  beta[1] ~ normal(0, 0.14);\n  beta[2] ~ normal(0, 0.15);\n  beta[3] ~ normal(0, 6.45);\n  \n  // Model\n  Y ~ bernoulli_logit_glm(X, alpha, beta);\n}\n\ngenerated quantities {\n  array[n] int Y_rep;\n  vector[n] log_lik;\n\n  vector[n] pi_hat = alpha + X * beta;\n  \n  for (i in 1:n) {\n    // We can use the shortcut bernoulli_logit_glm_lpmf, which works just like \n    // bernoulli_logit_glm from earlier\n    log_lik[i] = bernoulli_logit_glm_lpmf({Y[i]} | X[i,], alpha, beta);\n\n    // Or we can use bernoulli_logit_lpmf and feed it pi_hat\n    // log_lik[i] = bernoulli_logit_lpmf(Y[i] | pi_hat[i]);\n\n    // Posterior predictive distribution\n    Y_rep[i] = bernoulli_logit_rng(pi_hat[i]);\n  }\n}\n\n\n\nweather_stan <- cmdstan_model(\"13-stan/weather-full.stan\")\n\n\nX <- model.matrix(~ 1 + humidity9am_c + humidity3pm_c + raintoday, data = weather)[,-1]\n\nweather_samples <- weather_stan$sample(\n  data = list(n = nrow(weather), \n              Y = weather$raintomorrow_num, \n              X = X,\n              k = ncol(X)),\n  parallel_chains = 4, iter_warmup = 2000, iter_sampling = 2000, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 1.9 seconds.\n## Chain 2 finished in 2.0 seconds.\n## Chain 3 finished in 1.9 seconds.\n## Chain 4 finished in 1.9 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 1.9 seconds.\n## Total execution time: 2.0 seconds.\n\nIt works! I’m not going to work with any of these results, though, since it’s a hassle. But it’s exciting that it works!\n\nweather_samples$print(\n  variables = c(\"alpha\", \"beta[1]\", \"beta[2]\", \"beta[3]\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable  mean median   sd  2.5% 97.5%\n##   alpha   -2.23  -2.22 0.13 -2.49 -1.99\n##   beta[1] -0.01  -0.01 0.01 -0.02  0.01\n##   beta[2]  0.08   0.08 0.01  0.06  0.10\n##   beta[3]  1.14   1.14 0.22  0.72  1.57\n\nweather_samples$loo()\n## \n## Computed from 8000 by 1000 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -356.8 20.7\n## p_loo         4.1  0.3\n## looic       713.7 41.5\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n\nyrep_draws <- weather_samples$draws(\"Y_rep\") |> \n  posterior::as_draws_matrix()\n\nbayesplot::ppc_bars(y = weather$raintomorrow_num, \n                    yrep = yrep_draws[sample(nrow(yrep_draws), 100), ])\n\n\n\n\n\n\n\n\n\nInterpretation and marginal effects\nLogit/log odds-scale coefficients:\n\nmodel_parameters(model_weather_full_brms, centrality = \"mean\", dispersion = TRUE, \n                 test = FALSE, verbose = FALSE, ci_method = \"hdi\", priors = TRUE)\n## Parameter     |      Mean |       SD |         95% CI |  Rhat |     ESS |                  Prior\n## ------------------------------------------------------------------------------------------------\n## (Intercept)   |     -2.24 |     0.13 | [-2.50, -1.99] | 1.000 | 5564.00 | Normal (-1.39 +- 0.70)\n## humidity9am_c | -6.91e-03 | 7.51e-03 | [-0.02,  0.01] | 1.000 | 6201.00 |  Normal (0.00 +- 0.14)\n## humidity3pm_c |      0.08 | 8.76e-03 | [ 0.06,  0.10] | 1.001 | 5268.00 |  Normal (0.00 +- 0.15)\n## raintodayYes  |      1.15 |     0.22 | [ 0.70,  1.56] | 1.000 | 6809.00 |  Normal (0.00 +- 6.45)\n\nOdds ratio-scale coefficients:\n\nmodel_parameters(model_weather_full_brms, centrality = \"mean\", dispersion = TRUE, \n                 test = FALSE, verbose = FALSE, ci_method = \"hdi\", priors = TRUE,\n                 exponentiate = TRUE) |> \n  print(digits = 3, zap_small = TRUE)\n## Parameter     |  Mean |    SD |       95% CI |  Rhat |      ESS |                  Prior\n## ----------------------------------------------------------------------------------------\n## (Intercept)   | 0.107 | 0.130 | [0.08, 0.14] | 1.000 | 5564.000 | Normal (-1.39 +- 0.70)\n## humidity9am_c | 0.993 | 0.008 | [0.98, 1.01] | 1.000 | 6201.000 |  Normal (0.00 +- 0.14)\n## humidity3pm_c | 1.083 | 0.009 | [1.07, 1.10] | 1.001 | 5268.000 |  Normal (0.00 +- 0.15)\n## raintodayYes  | 3.169 | 0.219 | [2.02, 4.75] | 1.000 | 6809.000 |  Normal (0.00 +- 6.45)\n\n\nmodel_weather_full_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  mutate(.value = exp(.value)) |> \n  mutate(.variable = factor(.variable, \n                            levels = c(\"b_Intercept\", \"b_humidity9am_c\",\n                                       \"b_humidity3pm_c\", \"b_raintodayYes\"),\n                            ordered = TRUE)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[3], clrs[1]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\nAfter exponentiating it, the intercept \\(\\beta_0\\) has a posterior mean of 0.107 with a 95% credible interval of 0.08–0.14. This represents the probability of rain tomorrow (10.7%) when 9 AM humidity is average (60.87), when 3 PM humidity is average (45.8), and when there was no rain today.\nThe \\(\\beta_1\\) and \\(\\beta_2\\) coefficients show the effect of humidity. After after un-logiting them through exponentiation, morning humidity doesn’t do much anymore. It has a posterior mean of 0.993, with a 95% credible interval of 0.98–1.01. Afternoon humidity, on the other hand, does matter, with a posterior mean of 1.083 and a 95% credible interval of 1.07–1.10. This means that a 1 percentage point increase in afternoon humidity makes it 7–10% more likely to rain tomorrow, on average. The book makes the important point that 9 AM humidity’s loss of importance doesn’t mean it’s not a significant predictor of tomorrow’s rain—it was in the simpler model above. Instead,\n\nRather, humidity9am isn’t a significant predictor of tomorrow’s rain when controlling for afternoon humidity and whether or not it rains today. Put another way, if we already know today’s humidity3pm and rain status, then knowing humidity9am doesn’t significantly improve our understanding of whether or not it rains tomorrow. This shift in understanding about humidity9am … might not be much of a surprise – humidity9am is strongly associated with humidity3pm and raintoday, thus the information it holds about raintomorrow is somewhat redundant in [the larger model].\n\nAnd finally, the exponentiated \\(\\beta_3\\) coefficient for rain today has a posterior mean of 3.169 witih a 95% credible interval of 2.02–4.75, which means that rain today makes it 2 to almost-5 times more likely to rain tomorrow.\nFor fun, we can look at some of the moving predictions and marginal effects of this model. Because there are multiple variables, we need to hold some constant in order to plot these in two dimensions. We’ll hold 9 AM humidity constant at its average value (which is conveniently 0, since we centered it) and look at the predicted probabilities of weather across a range of 3 PM humidity.\n\n# Fast automatic version:\n# plot_cap(model_weather_full_brms, condition = c(\"humidity3pm_c\", \"raintoday\"))\n\nexpand_grid(humidity9am_c = 0,\n            humidity3pm = seq(0, 100, by = 1),\n            raintoday = c(\"No\", \"Yes\")) |> \n  mutate(humidity3pm_c = humidity3pm - unscaled$humidity3pm_centered$scaled_center) |> \n  add_epred_draws(model_weather_full_brms) |> \n  ggplot(aes(x = humidity3pm, y = .epred, color = raintoday)) +\n  geom_dots(data = weather, \n            aes(y = raintomorrow_num, \n                side = ifelse(raintomorrow_num == 1, \"bottom\", \"top\")), \n            pch = 19, color = \"grey70\", scale = 0.65, alpha = 0.75) +\n  stat_lineribbon(aes(fill = raintoday), alpha = 0.35) + \n  scale_color_manual(values = c(clrs[6], clrs[4])) +\n  scale_fill_manual(values = c(clrs[6], clrs[4])) +\n  labs(x = \"3 PM humidity\", y = \"Posterior probability of rain tomorrow\", \n       color = \"Rain today\", fill = \"Rain today\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\nThis is neat! When it rains, the probability of rain the next day is higher than it is for non-rainy days, regardless of the afternoon humidity. As afternoon humidity increases, the probability of rain also increases, and seems to be steepest at 60–70% humidity.\nWe can find the slopes of those fitted lines with marginaleffects():\n\n# Fast automatic version:\n# plot_cme(model_weather_full_brms, effect = \"humidity3pm_c\",\n#          condition = c(\"humidity3pm_c\", \"raintoday\"))\n\nmfx_full <- model_weather_full_brms |> \n  marginaleffects(newdata = datagrid(humidity9am_c = 0,\n                                     humidity3pm_c = seq(0, 100, by = 1) -\n                                       unscaled$humidity3pm_centered$scaled_center,\n                                     raintoday = c(\"No\", \"Yes\")),\n                  variables = c(\"humidity3pm_c\", \"raintoday\"),\n                  type = \"response\")\ntidy(mfx_full)\n##       type          term contrast    estimate    conf.low   conf.high\n## 1 response humidity3pm_c    dY/dX 0.009129328 0.008342123 0.009575226\n## 2 response     raintoday Yes - No 0.132734228 0.079558025 0.192418252\n\n\nmfx_full |> \n  posteriordraws() |> \n  mutate(humidity3pm = humidity3pm_c + unscaled$humidity3pm_centered$scaled_center) |> \n  filter(term != \"raintoday\" | raintoday != \"Yes\") |> \n  mutate(raintoday = ifelse(term == \"raintoday\", \"contrast\", as.character(raintoday))) |> \n  ggplot(aes(x = humidity3pm, y = draw, color = raintoday, fill = raintoday)) +\n  stat_lineribbon(alpha = 0.25) +\n  scale_color_manual(values = c(clrs[6], clrs[4], clrs[1]),\n                     breaks = c(\"No\", \"Yes\"), na.value = clrs[1]) +\n  scale_fill_manual(values = c(clrs[6], clrs[4], clrs[1]),\n                    breaks = c(\"No\", \"Yes\"), na.value = clrs[1]) +\n  labs(x = \"3 PM humidity\", \n       y = \"Marginal effect or ∆ in group means\", \n       color = \"Rain today\", fill = \"Rain today\") +\n  facet_wrap(vars(term, contrast), scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nPrediction and classification\nWe’ll use this more complex model to predict the probability of rain tomorrow given 99% humidity at 3 PM and both rain today and no rain today, holding morning humidity constant at its average, similar to what we did with the simple model.\n\nhumidity_99_c <- 99 - unscaled$humidity3pm_centered$scaled_center\n\nprediction_full <- model_weather_full_brms |> \n  predicted_draws(newdata = expand_grid(humidity9am_c = 0,\n                                        humidity3pm_c = humidity_99_c,\n                                        raintoday = c(\"No\", \"Yes\")))\n\nprediction_full |> \n  count(.prediction, raintoday) |> \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n  ggplot(aes(x = factor(.prediction), y = n)) + \n  geom_col(aes(fill = factor(.prediction))) +\n  geom_label(aes(label = prop_nice), nudge_y = -500) +\n  scale_fill_manual(values = c(clrs[2], clrs[3]), guide = \"none\") +\n  facet_wrap(vars(raintoday))\n\n\n\n\n\n\nModel evaluation\n\nHow fair is the model?\nGreat.\n\n\nHow wrong is the model?\nThe posterior predictive checks look fine:\n\npp_check(model_weather_full_brms, ndraws = 100,\n         type = \"bars\")\n\n\n\n\nMost simulated posterior datasets saw rain on 18ish% of days, ranging between 14% and 23%, and in reality the true value is around 18%, so that’s good:\n\npp_check(model_weather_full_brms, ndraws = 500,\n         type = \"stat\") +\n  labs(x = \"Probability of rain\")\n\n\n\n\n\n\nHow accurate are the model’s posterior classifications?\n\nprediction_brms_full <- weather |> \n  add_predicted_draws(model_weather_full_brms)\n\nrain_classifications_full <- prediction_brms_full |> \n  group_by(.row) |> \n  summarize(rain_prob_that_day = mean(.prediction),\n            raintomorrow_actual = raintomorrow[1]) |> \n  mutate(raintomorrow_pred = rain_prob_that_day >= 0.5)\n\nrain_classifications_full |> \n  janitor::tabyl(raintomorrow_actual, raintomorrow_pred) |> \n  janitor::adorn_totals(c(\"row\", \"col\"))\n##  raintomorrow_actual FALSE TRUE Total\n##                   No   783   31   814\n##                  Yes   108   78   186\n##                Total   891  109  1000\n\nHere are all the different accuracy rates:\n\nOverall accuracy (\\(\\frac{a + d}{a + b + c + d}\\)): 86.1%\nSpecificity, or true negative rate (\\(\\frac{a}{a + b}\\)): 96.2%\nSensitivity, or true positive rate (\\(\\frac{c}{c + d}\\)): 41.9%\n\nThe overall accuracy is a little better than the simple model using a 50% threshold (it was 82ish%; now it’s 86%). The specificity/true negative rate declined a tiny bit (it was 99%; now it’s 96%). The sensitivity/true positive rate is substantially better though (it was 7.5%; now it’s 42.5%!!)\n\n\nHow does this model compare with the simpler one?\nWe can get a formal measure of these two models’ predictive power by comparing their ELPD values. The more complex model is substantially better than the simple one.\n\nloo_stats <- tribble(\n  ~model_name, ~model,\n  \"Simple model\", model_weather_brms,\n  \"Complex model\", model_weather_full_brms\n) |> \n  mutate(loo = map(model, ~loo(.))) |> \n  mutate(loo_stuff = map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |> \n  select(model_name, loo_stuff) |> \n  unnest(loo_stuff) |> \n  filter(statistic == \"elpd_loo\") |> \n  arrange(desc(Estimate))\nloo_stats\n## # A tibble: 2 × 4\n##   model_name    statistic Estimate    SE\n##   <chr>         <chr>        <dbl> <dbl>\n## 1 Complex model elpd_loo     -357.  20.8\n## 2 Simple model  elpd_loo     -437.  19.0\n\n\nloo_stats |> \n  mutate(model_name = fct_rev(fct_inorder(model_name))) |> \n  ggplot(aes(x = Estimate, y = model_name)) +\n  geom_pointrange(aes(xmin = Estimate - 2 * SE, xmax = Estimate + 2 * SE))\n\n\n\n\n\nloo_compare(loo(model_weather_brms), \n            loo(model_weather_full_brms))\n##                         elpd_diff se_diff\n## model_weather_full_brms   0.0       0.0  \n## model_weather_brms      -80.1      13.5"
  },
  {
    "objectID": "bayes-rules/15-chapter.html",
    "href": "bayes-rules/15-chapter.html",
    "title": "15: Hierarchical models are exciting",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/15-chapter.html#the-general-setup",
    "href": "bayes-rules/15-chapter.html#the-general-setup",
    "title": "15: Hierarchical models are exciting",
    "section": "The general setup",
    "text": "The general setup\nThis running data has the race times for 36 different runners for a 10-mile in race that they’ve run for multiple years\nIn the book, Figure 15.1 is a boxplot showing each runner’s race time:\n\nrunning |> \n  ggplot(aes(x = runner, y = net)) +\n  geom_boxplot() +\n  labs(x = \"Runner number\", y = \"Race time\")\n\n\n\n\nThere’s variation across runners (e.g., runner 4 is consistently slower than runners 5 and 6), and there’s variation within runners (e.g., runner 6 has had roughly the same time every race; runner 17 ranges from 80ish to 120ish)\nWe can also see this if we look at patterns across age within each runner. People like runner 29 are consistently fast over time; runner 17 bounces around a lot.\n\nrunning |> \n  ggplot(aes(x = age, y = net)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1, color = clrs[4]) +\n  facet_wrap(vars(runner_nice), ncol = 4) +\n  labs(x = \"Age\", y = \"Race time\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\nWhen analyzing this kind of data, we have to do special stuff with it to incorporate our understanding of these different structures (i.e. repeated runners across time). Before getting into hierarchical models (the point of this unit), it’s helpful to look at two extreme ways of handling the data: complete pooling and no pooling."
  },
  {
    "objectID": "bayes-rules/15-chapter.html#complete-pooling",
    "href": "bayes-rules/15-chapter.html#complete-pooling",
    "title": "15: Hierarchical models are exciting",
    "section": "15.1: Complete pooling",
    "text": "15.1: Complete pooling\nWith complete pooling we lump all the observations together into one pool of information and basically treat each row as independent:\n\nrunning |> \n  ggplot(aes(x = age, y = net)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = clrs[4]) +\n  labs(x = \"Age\", y = \"Race time\")\n\n\n\n\nWe can then model this with a normal regression model with vague priors:\n\\[\n\\begin{aligned}\n\\text{Race time}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\, \\text{Age}_i \\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(0, 35) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/10)\n\\end{aligned}\n\\]\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(0, 35), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(0.1), class = sigma))\n\nmodel_complete_pooling_brms <- brm(\n  bf(net ~ age),\n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"15-manual_cache/complete-pooling-brms\"\n)\n\n\ntidy(model_complete_pooling_brms, effects = \"fixed\", conf.level = 0.8)\n## # A tibble: 2 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   <chr>  <chr>     <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  cond      (Intercept)   75.2      24.6     43.1     107.   \n## 2 fixed  cond      age            0.267     0.447   -0.306     0.850\n\n\n# Put the results in a list\nm_pool_complete <- model_complete_pooling_brms |> \n  tidy(effects = \"fixed\", conf.level = 0.8) |> \n  mutate(term = janitor::make_clean_names(term)) |> \n  split(~term)\n\n\n\n\nmodel_complete_pooling_rstanarm <- stan_glm(\n  net ~ age,\n  data = running,\n  family = gaussian,\n  prior_intercept = normal(0, 35),\n  prior = normal(0, 2.5),\n  prior_aux = exponential(0.1),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n\n\ntidy(model_complete_pooling_rstanarm, conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 2 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)   75.4      24.3     44.5     107.   \n## 2 age            0.263     0.439   -0.309     0.826\n\n\n\n\nBased on this, a one-year increase in age is associated with a race time that is 0.27 minutes longer, on average, but with an 80% credible interval of -0.31 to 0.85, so essentially there’s no effect. But that’s likely not the case—we would expect most people to get slower over time. And that is indeed the case if we look at the age trends for each runner. The pooled line is basically flat while the individual lines all have (generally upward) slopes.\n\nmodel_complete_pooling_brms |> \n  add_epred_draws(newdata = tibble(age = seq(min(running$age), \n                                             max(running$age), \n                                             length.out = 100))) |> \n  ggplot(aes(x = age, y = .epred)) +\n  geom_smooth(data = running,\n              aes(y = net, group = runner), method = \"lm\", se = FALSE, \n              color = \"grey60\", size = 0.5) +\n  stat_lineribbon(alpha = 0.25, fill = clrs[4], color = clrs[4]) +\n  labs(x = \"Age\", y = \"Race time\")\n\n\n\n\nlol this model isn’t great. It doesn’t keep any information about participants and treats every row as its own independent chunk of the population:\n\n\n\n\n\nComplete pooling"
  },
  {
    "objectID": "bayes-rules/15-chapter.html#no-pooling",
    "href": "bayes-rules/15-chapter.html#no-pooling",
    "title": "15: Hierarchical models are exciting",
    "section": "15.2: No pooling",
    "text": "15.2: No pooling\nInstead of lumping all the runners together, we can keep complete information about each runner by not lumping anything together at all. Note how here there’s no node for “Population” here like before—with the no pooling approach, we treat each runner as the complete population for that person:\n\n\n\n\n\nNo pooling\n\n\n\n\nMore formally, this entails running the linear model for each runner \\(j\\) in their \\(i\\)th race:\n\\[\n\\begin{aligned}\n\\text{Race time}_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma) \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j}\\, \\text{Age}_{i_j} \\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(0, 35) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/10)\n\\end{aligned}\n\\]\nThere’s some mathematical trickery we can do with interaction terms to just fit one model and get unique slopes and intercepts for each runner (net ~ age + runner + age*runner), but practically speaking, this no pooled approach entails running a single model for each runner.\nSo for fun and excitement and learning, we’ll do that here! The underlying Stan code for model_complete_pooling_brms is the same as what we need for this no pooling model, so we’ll just re-run that model over and over (with update()) with subsets of the data—one subset per runner.\n\nno_pooling <- tibble(runner = levels(running$runner)) |> \n  mutate(data = map(runner, ~filter(running, runner == .x))) |> \n  mutate(model = map(data, ~update(model_complete_pooling_brms, \n                                   newdata = .x)))\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## Warning: 1 of 8000 (0.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## Warning: 5 of 8000 (0.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n\nNow each runner has their own intercept and slope!\n\nno_pooling_results <- no_pooling |> \n  mutate(tidied = map(model, ~tidy(., effects = \"fixed\"))) |> \n  select(-data, -model) |> \n  unnest(tidied) |> \n  filter(term %in% c(\"(Intercept)\", \"age\")) |> \n  select(runner, term, estimate) |> \n  pivot_wider(names_from = \"term\", values_from = \"estimate\") |> \n  rename(beta0 = `(Intercept)`, beta1 = age)\n\nno_pooling_results\n## # A tibble: 36 × 3\n##    runner  beta0   beta1\n##    <chr>   <dbl>   <dbl>\n##  1 1      154.   -1.42  \n##  2 2       40.7   0.791 \n##  3 3       86.4   0.0585\n##  4 4       56.4   0.831 \n##  5 5        5.52  1.31  \n##  6 6       24.4   0.903 \n##  7 7       40.7   1.14  \n##  8 8        2.68  1.84  \n##  9 9       23.1   1.32  \n## 10 10      32.4   1.47  \n## # … with 26 more rows\n\nWe can even plot these runner-specific models:\n\nrunner_ages <- running |> \n  group_by(runner) |> \n  summarize(age_min = min(age), age_max = max(age)) |> \n  mutate(age_range = map2(age_min, age_max, ~tibble(age = seq(.x, .y, length.out = 100)))) |> \n  select(runner, age_range)\n\nno_pooling_epreds <- no_pooling |> \n  left_join(runner_ages, by = \"runner\") |> \n  mutate(epred = map2(model, age_range, ~epred_draws(.x, .y, ndraws = 100)))\n\nno_pooling_plots <- no_pooling_epreds |> \n  mutate(plot = map2(epred, data, ~{\n    ggplot(.x, aes(x = age, y = .epred)) +\n      geom_line(aes(group = .draw), size = 0.25, color = clrs[4], alpha = 0.5) +\n      geom_point(data = .y, aes(y = net), size = 0.75, color = clrs[6]) +\n      labs(x = NULL, y = NULL) +\n      coord_cartesian(xlim = c(50, 64), ylim = c(60, 132)) +\n      theme(panel.grid.minor = element_blank()) +\n      facet_wrap(vars(runner_nice))\n  }))\n\n\nwrap_plots(no_pooling_plots$plot, ncol = 4)\n\n\n\n\nBut that’s not really all that helpful at all. If we want to predict the race time for a new runner who is 58 years old (or any age), we can’t! As Bayes Rules! says:\n\nSince they’re tailored to the 36 individuals in our sample, the resulting 36 models don’t reliably extend beyond these individuals.\n\nThese individual models can’t “communicate” with each other and are too isolated and unwieldy."
  },
  {
    "objectID": "bayes-rules/15-chapter.html#partial-pooling-with-hierarchical-models",
    "href": "bayes-rules/15-chapter.html#partial-pooling-with-hierarchical-models",
    "title": "15: Hierarchical models are exciting",
    "section": "15.3 & 15.4: Partial pooling with hierarchical models",
    "text": "15.3 & 15.4: Partial pooling with hierarchical models\nThis data, however, has a natural hierarchical structure to it. Within our population, we have observations that are grouped by runner. There is variation within each runner, and variation across or between each runner. According to Bayes Rules!, we have:\n\n\nWithin-group variability: The degree of the variability among multiple observations within each group can be interesting on its own. For example, we can examine how consistent an individual’s running times are from year to year.\nBetween-group variability: Hierarchical data also allows us to examine the variability from group to group. For example, we can examine the degree to which running patterns vary from individual to individual.\n\n\nWe can visualize the structure like this:\n\n\n\n\n\nPartial pooling with hierarchical models\n\n\n\n\nAnd this chapter doesn’t cover how to run a model with this structure, but we’ll do it here for fun:\n\npriors <- c(prior(normal(0, 35), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(student_t(3, 0, 15), class = sd, lb = 0))\n\nmodel_partial_pooling_brms <- brm(\n  bf(net ~ age + (1 + age | runner)),\n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 8000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"15-manual_cache/partial-pooling-brms\"\n)\n\n\ntidy(model_partial_pooling_brms)\n## # A tibble: 6 × 8\n##   effect   component group    term               estim…¹ std.e…² conf.…³ conf.…⁴\n##   <chr>    <chr>     <chr>    <chr>                <dbl>   <dbl>   <dbl>   <dbl>\n## 1 fixed    cond      <NA>     (Intercept)         22.1    12.1   -1.51    46.0  \n## 2 fixed    cond      <NA>     age                  1.23    0.222  0.790    1.67 \n## 3 ran_pars cond      runner   sd__(Intercept)      8.24    6.11   0.334   23.0  \n## 4 ran_pars cond      runner   sd__age              0.269   0.117  0.0463   0.549\n## 5 ran_pars cond      runner   cor__(Intercept).…  -0.273   0.561 -0.972    0.890\n## 6 ran_pars cond      Residual sd__Observation      5.18    0.305  4.62     5.81 \n## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​conf.low,\n## #   ⁴​conf.high\n\nWe’ll get into how to interpret all these population-level (“fixed effects”) and group-level (“random effects”) things in future chapter. Or look at this guide of mine here.\nIn the meantime, for fun, here’s what this all looks like when plotted:\n\nmodel_partial_pooling_brms |> \n  epred_draws(newdata = unnest(runner_ages, age_range),\n              ndraws = 100,\n              re_formula = NULL) |> \n  ggplot(aes(x = age, y = .epred)) +\n  geom_line(aes(group = paste(runner, .draw)),\n            size = 0.1, color = clrs[4], alpha = 0.5) +\n  geom_point(data = running, aes(y = net), size = 0.75, color = clrs[6]) +\n  labs(x = \"Age\", y = \"Race time\") +\n  theme(panel.grid.minor = element_blank()) +\n  facet_wrap(vars(runner), ncol = 4,\n             labeller = as_labeller(function(x) glue::glue(\"Runner {x}\")))"
  },
  {
    "objectID": "bayes-rules/16-chapter.html",
    "href": "bayes-rules/16-chapter.html",
    "title": "16: Hierarchical models without predictors",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#the-general-setup",
    "href": "bayes-rules/16-chapter.html#the-general-setup",
    "title": "16: Hierarchical models without predictors",
    "section": "The general setup",
    "text": "The general setup\nThe data in this example comes from a sample of music from Spotify. There are 350 songs by 44 different artists:\n\nspotify |> \n  summarize(n_songs = n(),\n            n_artists = n_distinct(artist))\n## # A tibble: 1 × 2\n##   n_songs n_artists\n##     <int>     <int>\n## 1     350        44\n\nspotify |> \n  count(artist)\n## # A tibble: 44 × 2\n##    artist            n\n##    <fct>         <int>\n##  1 Mia X             4\n##  2 Chris Goldarg    10\n##  3 Soul&Roll         5\n##  4 Honeywagon        3\n##  5 Röyksopp          4\n##  6 Freestyle         3\n##  7 DA Image          3\n##  8 Jean Juan         5\n##  9 TV Noise         14\n## 10 Kid Frost         3\n## # … with 34 more rows\n\nThe main thing we care about in this example is song popularity and its variation within and between artists. We’ll answer three different questions:\n\nWhat’s the typical popularity of a typical Spotify song?\nHow much does popularity vary from artist to artist?\nFor any single artist, how much does popularity vary from song to song?\n\nPopularity is measured on a scale of 0–100, with 100 representing the highest popularity:\n\nspotify %>% \n  group_by(artist) %>% \n  summarize(count = n(), popularity = mean(popularity)) |> \n  arrange(popularity)\n## # A tibble: 44 × 3\n##    artist        count popularity\n##    <fct>         <int>      <dbl>\n##  1 Mia X             4       13.2\n##  2 Chris Goldarg    10       16.4\n##  3 Soul&Roll         5       24.2\n##  4 Honeywagon        3       31.7\n##  5 Röyksopp          4       33.2\n##  6 Freestyle         3       33.7\n##  7 DA Image          3       36.7\n##  8 Jean Juan         5       36.8\n##  9 TV Noise         14       38.1\n## 10 Kid Frost         3       40.7\n## # … with 34 more rows\n\nHere’s how much these artists’ songs vary in popularity. Some are consistently popular with little variation within the artist, like Lil Skies and Sufjan Stevens; others like Kendrik Lamar have enormous variation. There are also patterns across/between artists—Beyoncé is clearly more popular than Honeywagon.\n\nspotify |> \n  ggplot(aes(y = artist, x = popularity)) +\n  geom_point(color = \"grey70\", size = 0.75) +\n  stat_pointinterval(color = clrs[6], point_interval = \"mean_qi\") +\n  labs(x = \"Popularity\", y = NULL)\n## Warning: Using the `size` aesthietic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\nWe’ll analyze these questions with three different approaches:\n\nComplete pooling: Ignore artists and lump all songs together\nNo pooling: Analyze each artist separately and assume that data from one artist has no information about any other artist\nPartial pooling (with multilevel models): Incorporate the grouping structure into the analysis “so that even though artists differ in popularity, they might share valuable information about each other and about the broader population of artists”\n\nTo be super mathy and formal about the structure of the data, we’ll use \\(i\\) to represent a song and \\(j\\) to represent an artist, where \\(j \\in \\{1, 2, \\dots, 44\\}\\), and \\(n_j\\) represents the number of songs each artist has (like Mia X is \\(n_1 = 4\\), Chris Goldarg is \\(n_2 = 10\\), and so on).\nWe can calculate the total sample size by adding all the \\(n_j\\)s together:\n\\[\nn = \\sum_{j = 1}^{44} n_j = n_1 + n_2 + \\dots + n_{44} = 350\n\\]\nThat’s one level of the hierarchy—a count of artists with songs inside them.\nWe can also think about the songs themselves. Each song \\(Y\\) gets two subscripts for the song id and the artist id: \\(Y_{i,j}\\), where \\(i \\in \\{1, 2, \\dots, n_j\\}\\) and \\(j \\in \\{1, 2, \\dots, 44\\}\\)\nWhen we look at the data at a song level, we can think of \\(Y\\) as a collection of all the songs by the 44 different artists, or:\n\\[\nY = \\Bigl( (Y_{1_1}, Y_{2_1}, Y_{n_{1_1}}), (Y_{1_2}, Y_{2_2}, Y_{n_{2_2}}), \\dots, (Y_{1_{44}}, Y_{2_{44}}, Y_{n_{{44}_{44}}}) \\Bigr)\n\\]\nThe data follows a hierarchical structure, with songs nested inside artists:\n\n\n\n\n\nPartial hierarchical pooling"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#complete-pooled-model",
    "href": "bayes-rules/16-chapter.html#complete-pooled-model",
    "title": "16: Hierarchical models without predictors",
    "section": "16.1: Complete pooled model",
    "text": "16.1: Complete pooled model\nFirst we’ll do a pooled model where we ignore the fact that different (and often repeated) artists recorded these songs. We’ll pretend that these all came from 350 completely different independent artists. That means that instead of looking at the data like this:\n\\[\nY = \\Bigl( (Y_{1_1}, Y_{2_1}, Y_{n_{1_1}}), (Y_{1_2}, Y_{2_2}, Y_{n_{2_2}}), \\dots, (Y_{1_{44}}, Y_{2_{44}}, Y_{n_{{44}_{44}}}) \\Bigr)\n\\]\n…we’ll look at it like this:\n\\[\nY = ( Y_1, Y_2, \\dots, Y_{350} )\n\\]\nLumping everything together, here’s what the distribution of popularity looks like:\n\nspotify |> \n  ggplot(aes(x = popularity)) +\n  geom_density(fill = clrs[1], color = NA) +\n  labs(x = \"Popularity\", y = \"Density\")\n\n\n\n\nWe’ll use an intercept-only model to estimate the mean and variance of this distribution based on this official model and these priors:\n\nFor \\(\\mu\\), we’ll assume that an average song will have a popularity score of 50, but it could be somewhere between 10 and 90\nFor \\(\\sigma\\), we’ll assume that for all songs, popularity will vary with a standard deviation of 25 popularity points\n\n\\[\n\\begin{aligned}\n\\text{Popularity}_{i,j} &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/25)\n\\end{aligned}\n\\]\nWe’ll do it both with brms and rstanarm:\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(50, 20), class = Intercept),\n            prior(exponential(0.04), class = sigma))\n\nmodel_pooled_brms <- brm(\n  bf(popularity ~ 1),\n  data = spotify,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"16-manual_cache/model-pooled-brms\"\n)\n\n\nmodel_pooled_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  select(-component, -group)\n## # A tibble: 2 × 6\n##   effect   term            estimate std.error conf.low conf.high\n##   <chr>    <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed    (Intercept)         58.4     1.12      56.9      59.8\n## 2 ran_pars sd__Observation     20.7     0.782     19.7      21.7\n\n\n\n\nmodel_pooled_rstanarm <- stan_glm(\n  popularity ~ 1,\n  data = spotify,\n  family = gaussian,\n  prior_intercept = normal(50, 20),\n  prior_aux = exponential(0.04),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n\n\nmodel_pooled_rstanarm |> \n  tidy(effects = c(\"fixed\", \"aux\"),\n       conf.int = TRUE, conf.level = 0.8) |> \n  filter(term != \"mean_PPD\")\n## # A tibble: 2 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)     58.4     1.09      57.0      59.8\n## 2 sigma           20.7     0.785     19.7      21.7\n\n\n\n\nThese results mean that Spotify songs have an average popularity rating (\\(\\mu\\)) of 58.36 points. The \\(\\sigma\\) implies that the score bounces around from song to song with a variance of 20.71 points.\nWe can see how this model predicts popularity if we plug in the average popularities for each of the artists and see the predicted values. The point ranges here represent the posterior predictions for each artist; the red diamonds show the actual average popularity. This isn’t that great—the model treats every artist as the same, so there are no differences in predictions across artists.\n\npooled_predictions <- spotify |> \n  group_by(artist) |> \n  summarize(popularity = mean(popularity)) |> \n  add_predicted_draws(model_pooled_brms)\n\npooled_predictions |> \n  ggplot(aes(y = artist, x = .prediction)) +\n  stat_pointinterval(color = clrs[1]) +\n  stat_summary(data = spotify, aes(x = popularity), \n               geom = \"point\", fun = mean, color = clrs[3], size = 3, pch = 18) +\n  labs(x = \"Popularity\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#no-pooled-model",
    "href": "bayes-rules/16-chapter.html#no-pooled-model",
    "title": "16: Hierarchical models without predictors",
    "section": "16.2: No pooled model",
    "text": "16.2: No pooled model\nAs we saw above, the pooled model isn’t great because it erases artist effects. Average popularity and variability in popularity vary substantially across artists. So instead of pretending that there’s a global mean popularity level, we’ll look at group/artist-specific averages instead. Rather than looking at the data like this:\n\\[\nY = \\Bigl( (Y_{1_1}, Y_{2_1}, Y_{n_{1_1}}), (Y_{1_2}, Y_{2_2}, Y_{n_{2_2}}), \\dots, (Y_{1_{44}}, Y_{2_{44}}, Y_{n_{{44}_{44}}}) \\Bigr)\n\\]\n…we’ll look at it like this:\n\\[\n\\begin{aligned}\nY_{j = 1} &= ( Y_1, Y_2, \\dots, Y_{n, j = 1} ) \\\\\nY_{j = 2} &= ( Y_1, Y_2, \\dots, Y_{n, j = 2} ) \\\\\n& \\dots \\\\\nY_{j = 44} &= ( Y_1, Y_2, \\dots, Y_{n, j = 44} ) \\\\\n\\end{aligned}\n\\]\nThat gives us this formal model:\n\\[\n\\begin{aligned}\nY_{i, j} &\\sim \\mathcal{N}(\\mu_j, \\sigma) \\\\\n\\\\\n\\mu_j &\\sim \\mathcal{N}(50, 20) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/25)\n\\end{aligned}\n\\]\nThis implies that the popularity of all songs \\(i\\) is normally distributed around some group mean \\(\\mu_j\\) with standard deviation \\(\\sigma\\) within each artist.\nThis also implies that\n\nevery song \\(i\\) by the same artist \\(j\\) has the same mean \\(\\mu_j\\)\nsongs by different artists like \\(j\\) and \\(k\\) have different group averages \\(\\mu_j\\) and \\(\\mu_k\\), so each artist’s \\(\\mu_j\\) is unique and doesn’t tell us anything about other artists’ \\(\\mu_j\\)s\nthe \\(\\sigma\\) term is technically still pooled across all songs; we could use artist-specific \\(\\sigma_j\\) terms if we wanted, but that would make life too complex\n\nModeling time. This time instead of estimating 44 separate models like we did in the previous chapter, we can get rid of the global intercept term in the model formula by using popularity ~ 0 + artist (in the book they use popularity ~ artist - 1, but I like the 0 + artist syntax since the 0 is in the spot where \\(\\beta_0\\) would normally go and it feels like it lines up with the model syntax better).\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(50, 20), class = b),\n            prior(exponential(0.04), class = sigma))\n\nmodel_no_pooled_brms <- brm(\n  bf(popularity ~ 0 + artist),\n  data = spotify,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"16-manual_cache/model-no-pooled-brms\"\n)\n\n\nmodel_no_pooled_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  group_by(effect) |> \n  slice(1:3) |> \n  select(-component, -group)\n## # A tibble: 4 × 6\n## # Groups:   effect [2]\n##   effect   term               estimate std.error conf.low conf.high\n##   <chr>    <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed    artistMiaX             17.2     6.73      8.66      25.8\n## 2 fixed    artistChrisGoldarg     18.0     4.37     12.4       23.6\n## 3 fixed    artistSoul&Roll        26.5     5.94     18.9       34.1\n## 4 ran_pars sd__Observation        14.0     0.562    13.3       14.7\n\n\n\n\nmodel_no_pooled_rstanarm <- stan_glm(\n  popularity ~ 0 + artist,\n  data = spotify,\n  family = gaussian,\n  prior = normal(50, 20),\n  prior_aux = exponential(0.04),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n\n\nmodel_no_pooled_rstanarm |> \n  tidy(effects = c(\"fixed\", \"aux\"),\n       conf.int = TRUE, conf.level = 0.8) |> \n  filter(term != \"mean_PPD\") |> \n  slice(1:3, 45)\n## # A tibble: 4 × 5\n##   term                estimate std.error conf.low conf.high\n##   <chr>                  <dbl>     <dbl>    <dbl>     <dbl>\n## 1 artistMia X             17.3     6.66      8.67      25.8\n## 2 artistChris Goldarg     18.0     4.28     12.5       23.5\n## 3 artistSoul&Roll         26.5     5.92     18.9       34.0\n## 4 sigma                   14.0     0.565    13.3       14.7\n\n\n\n\nThese models give us one coefficient for each artist, representing the artist average, or \\(\\mu_j\\). In the output above I show just the first three artists’ means. We also get a global \\(\\sigma\\) of 13.99 points.\nAs before, we can see how this model predicts popularity if we plug in the average popularities for each of the artists and see the predicted values. Again, these point ranges represent the posterior predictions for each artist; the red diamonds show the actual average popularity. This time things seem to fit really well—that’s because the model is designed to predict the artist-specific \\(\\mu_j\\) averages.\nBut this great apparent fit comes at a cost:\n\nNo information is shared across these artists. This is equivalent to running 44 separate models and showing them all in one plot. The number of songs and artist has in the data could influence overall popularity, but since each artist is in an informational silo here, we can’t see if that’s the case.\nThe fit is hyper-specific to artists in the sample and can’t be used to predict the popularity of other artists.\n\n\nno_pooled_predictions <- spotify |> \n  group_by(artist) |> \n  summarize(popularity = mean(popularity)) |> \n  add_predicted_draws(model_no_pooled_brms)\n\nno_pooled_predictions |> \n  ggplot(aes(y = artist, x = .prediction)) +\n  stat_pointinterval(color = clrs[5], point_interval = \"mean_qi\") +\n  stat_summary(data = spotify, aes(x = popularity), \n               geom = \"point\", fun = mean, color = clrs[2], size = 3, pch = 18) +\n  labs(x = \"Popularity\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#building-the-hierarchical-model",
    "href": "bayes-rules/16-chapter.html#building-the-hierarchical-model",
    "title": "16: Hierarchical models without predictors",
    "section": "16.3: Building the hierarchical model",
    "text": "16.3: Building the hierarchical model\nWe can can overcome these shortcomings of the no pooled model by incorporating the group structure into the model and allowing information to be shared across different artist groups. The formal math gets a little bit more gnarly, and there are different ways of describing the models. Bayes Rules! shows two different approaches.\n\nLayer-based notation\nFirst they show a model with two layers: one layer that models the popularity of songs within artists and one layer that models the variability of popularity between arists:\n\\[\n\\begin{aligned}\nY_{i,j} &\\sim \\mathcal{N}(\\mu_j, \\sigma_y) & \\text{Individual songs within artist } j \\\\\n\\mu_j &= \\mathcal{N}(\\mu, \\sigma_{\\mu}) & \\text{Between-artist differences} \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) & \\text{Prior for global average} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/25) & \\text{Prior for within-artist variability} \\\\\n\\sigma_{\\mu} &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-artist variability}\n\\end{aligned}\n\\]\nIn the first layer, we’re modeling the popularity of individual songs within each artist \\(j\\). We have two parameters:\n\n\\(\\mu_j\\): mean song popularity for artist \\(j\\)\n\\(\\sigma_y\\): within-group variability, or the standard deviation of popularity within each artist\n\nIn the second layer, we’re modeling the variability of popularity across or between artists. We have two different parameters for this part of the model:\n\n\\(\\mu\\): global average of mean song popularity across all artists \\(j\\)\n\\(\\sigma_\\mu\\): between-group variability, or the standard deviation in mean popularity \\(\\mu_j\\) from artist to artist, or how much popularity bounces around as we move from artist to artist\n\n\n\nOffset-based notation\nI like this notation better (and it’s what I use here). It’s still focused on layers—the first line is the same in both model definitions. The second layer is different now, though:\n\\[\n\\begin{aligned}\nY_{i,j} &\\sim \\mathcal{N}(\\mu_j, \\sigma_y) & \\text{Individual songs within artist } j \\\\\n\\mu_j &= (\\mu + b_j)  & \\text{Between-artist differences} \\\\\nb_j &\\sim \\mathcal{N}(0, \\sigma_{\\mu}) & \\text{Random artist offsets} \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) & \\text{Prior for global average} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/25) & \\text{Prior for within-artist variability} \\\\\n\\sigma_{\\mu} &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-artist variability}\n\\end{aligned}\n\\]\nIn the second layer, we’re still modeling the variability across artists (\\(\\mu_j\\)), but we have a new parameter \\(b_j\\). This requires some short explanation to show where this is coming from.\nLet’s think about song popularity like a slope/intercept regression model with \\(\\beta\\) terms instead of with \\(\\mu\\). Since this is an intercept-only model, we just have a \\(\\beta_0\\) term:\n\\[\n\\text{Popularity}_j = \\beta_{0_\\text{Global artist mean}} + \\varepsilon\n\\]\nNote the \\(\\varepsilon\\) term here. We haven’t been using that throughout this book, but it’s always been lurking there. It’s the error term and represents all the variation in popularity that isn’t captured by the global artist mean. Right now the \\(\\beta_0\\) coefficient represents the global average of artist-specific averages. Any deviance from that is stuffed away in \\(\\varepsilon\\).\nHere’s an example to help illustrate this. \\(\\mu_j\\) represents artist-specific averages, and Beyoncé’s average popularity (\\(\\mu_\\text{Beyoncé}\\)) is 69.7:\n\nartist_averages <- spotify |> \n  group_by(artist) |> \n  summarise(mu_j = mean(popularity))\n\nartist_averages |> \n  filter(artist == \"Beyoncé\")\n## # A tibble: 1 × 2\n##   artist   mu_j\n##   <fct>   <dbl>\n## 1 Beyoncé  69.7\n\nThe global mean artist-specific popularity is the average of the mu_j column, or 52.1\n\nartist_averages |> \n  summarise(global_mu = mean(mu_j))\n## # A tibble: 1 × 1\n##   global_mu\n##       <dbl>\n## 1      52.1\n\nAn intercept-only model of popularity using only artist-specific averages would look something like this:\n\\[\n\\text{Popularity}_j = 52.1 + \\varepsilon\n\\]\nBeyoncé’s average popularity is 17.6 points higher than the global average of 52.1, but that’s lost in the error term. We can highlight it to show that it really exists:\n\\[\n\\text{Popularity}_\\text{Beyoncé} = 52.1 + (17.6_\\text{Beyoncé} + \\varepsilon)\n\\]\nWith some algebra, we can group that Beyoncé offset with the global mean:\n\\[\n\\text{Popularity}_\\text{Beyoncé} = (52.1 + 17.6_\\text{Beyoncé}) + \\varepsilon\n\\]\nNeat. Beyoncé’s artist-specific average is thus the global mean plus an artist-specific offset.\nWe can generalize this to any artist \\(j\\). We’ll call the offset term \\(b_j\\):\n\\[\n\\text{Popularity}_j = (\\beta_0 + b_j) + \\varepsilon\n\\]\nOr if we want to go back to the world of \\(\\mu\\) instead of \\(\\beta\\)s:\n\\[\n\\text{Popularity}_j = (\\mu + b_j)\n\\]\nPhew. I like this offset approach because it highlights the fact that these are “random” effects, or that we’re modeling the distribution of group-specific shifts above and below some global mean. Here’s the full model again, for reference:\n\\[\n\\begin{aligned}\nY_{i,j} &\\sim \\mathcal{N}(\\mu_j, \\sigma_y) & \\text{Individual songs within artist } j \\\\\n\\mu_j &= (\\mu + b_j)  & \\text{Between-artist differences} \\\\\nb_j &\\sim \\mathcal{N}(0, \\sigma_{\\mu}) & \\text{Random artist offsets} \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) & \\text{Prior for global average} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/25) & \\text{Prior for within-artist variability} \\\\\n\\sigma_{\\mu} &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-artist variability}\n\\end{aligned}\n\\]\n\n\n16.3.3: Within- vs between-group variability\nSince these multilevel/hierarchical/random effects/whatever-we-want-to-call-them models give us so much information about the variability within and between groups, we can do some neat things with that information.\nFor instance, the total variance in individual song popularity within and between artists is the sum of the two \\(\\sigma\\) terms we’ve estimated:\n\\[\n\\operatorname{Var}(Y_{i, j}) = \\sigma_y + \\sigma_\\mu\n\\]\nThis means we can calculate some ratios and percentages:\n\n\\(\\frac{\\sigma_y}{\\sigma_y + \\sigma_\\mu}\\): Proportion of total variance that is explained by within-artist differences\n\\(\\frac{\\sigma_\\mu}{\\sigma_y + \\sigma_\\mu}\\): Proportion of total variance that is explained by between-artist differences"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#posterior-analysis",
    "href": "bayes-rules/16-chapter.html#posterior-analysis",
    "title": "16: Hierarchical models without predictors",
    "section": "16.4: Posterior analysis",
    "text": "16.4: Posterior analysis\nK, finally we can calculate the posterior. To add a random intercept, or allow the intercept to vary by artist-specific offsets (yay for the offset notation!), we include a (1 | artist) term in both brms and rstanarm (and this table is indispensable for remembering how to set different random offsets for different nesting and grouping structures)\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(50, 20), class = Intercept),\n            prior(exponential(0.04), class = sigma),\n            prior(exponential(1), class = sd))\n\nmodel_multilevel_brms <- brm(\n  bf(popularity ~ (1 | artist)),\n  data = spotify,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"16-manual_cache/model-multilevel-brms\"\n)\n\n\nmodel_multilevel_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 3 × 7\n##   effect   group    term            estimate std.error conf.low conf.high\n##   <chr>    <chr>    <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed    <NA>     (Intercept)         52.5     2.17      49.8      55.3\n## 2 ran_pars artist   sd__(Intercept)     12.6     1.39      10.9      14.5\n## 3 ran_pars Residual sd__Observation     14.1     0.584     13.3      14.8\n\n\n# Put the results in a list\nm_mlm <- model_multilevel_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  mutate(term = janitor::make_clean_names(term)) |> \n  split(~term)\n\n\n\nThe decov() prior for \\(\\sigma_\\mu\\) here is weird—it’s equivalent to exponential(1) in the case since we have no predictors.\n\nmodel_multilevel_rstanarm <- stan_glmer(\n  popularity ~ (1 | artist),\n  data = spotify,\n  family = gaussian,\n  prior_intercept = normal(50, 20),\n  prior_aux = exponential(0.04),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n\n\nmodel_multilevel_rstanarm |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 3 × 6\n##   term                    estimate std.error conf.low conf.high group   \n##   <chr>                      <dbl>     <dbl>    <dbl>     <dbl> <chr>   \n## 1 (Intercept)                 52.4      2.40     49.2      55.6 <NA>    \n## 2 sd_(Intercept).artist       15.2     NA        NA        NA   artist  \n## 3 sd_Observation.Residual     14.0     NA        NA        NA   Residual\n\n\n\n\n(Weirdly rstanarm and brms give different results for the between-artist \\(\\sigma_mu\\) term sd_(Intercept). I don’t know why—probably a prior thing?)\n\nGlobal parameters (“fixed” effects)\nAt the global level of the model, we have these terms:\n\n\\(\\mu\\) or (Intercept): Global average artist-specific popularity\n\\(\\sigma_y\\) or sd__(Intercept) for artist: Within-artist variability\n\\(\\sigma^2_\\mu\\) or sd__Observation for Residual: Between-artist variability\n\nThe average artist has a posterior average (\\(\\mu\\)) of 52.5 popularity points, and there’s an 80% chance that their average popularity is between 49.8 and 55.3\nAccording to the posterior mean of \\(\\sigma_y\\) or sd__(Intercept), the between-artist variability is 12.6, which means that popularity bounces around by 12ish points as we move from artist to artist. The posterior mean of \\(\\sigma^2_\\mu\\) or sd__Observation shows that the within-artist variability is 14.1, which means that within any artist, popularity varies by 14ish points from song to song. Alternatively, this is the total residual variation, or the unexplained variation in song popularity.\nWe can play with the ratios of these types of variance too:\n\n# Proportion of song popularity explained by differences *between* artists\n(m_mlm$sd_intercept$estimate^2 / \n   (m_mlm$sd_intercept$estimate^2 + m_mlm$sd_observation$estimate^2)) |> \n  unname()\n## [1] 0.4450867\n\n# Proportion of song popularity explained by differences *within* artists\n(m_mlm$sd_observation$estimate^2 / \n    (m_mlm$sd_intercept$estimate^2 + m_mlm$sd_observation$estimate^2)) |> \n  unname()\n## [1] 0.5549133\n\n\n\nGroup parameters (“random” effects)\nAt the artist level of the model, we only have one term:\n\n\\(b_j\\) or (Intercept): Artist-specific offset from the global mean popularity \\(\\mu\\)\n\nWe can extract these offsets with the ranef() function, which returns a messy gross array, or we can use broom.mixed::tidy(..., effects = \"ran_vals\"). Let’s look at Beyoncé’s offset, since we used that as an example earlier. It should be somewhere around 17.6 (though not exactly that, since that was the value from the no pooling model—we have more information about artists this time, so that should influence the estimated artist average here).\n\nmodel_multilevel_brms |> \n  tidy(effects = \"ran_vals\", conf.level = 0.8) |> \n  filter(level == \"Beyoncé\") |> \n  select(-effect, -component)\n## # A tibble: 1 × 7\n##   group  level   term        estimate std.error conf.low conf.high\n##   <chr>  <chr>   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 artist Beyoncé (Intercept)     16.4      3.43     11.9      20.7\n\nThe offset is a little smaller (16ish) now that we’re partially pooling, and there’s an 80% chance that her offset is between 12 and 21ish popularity points from the global average.\nWe can look at everyone’s offsets and calculate artist-specific posterior group averages, or \\(\\mu_j\\) with some tidybayes wrangling:\n\nmodel_multilevel_brms |> \n  spread_draws(b_Intercept, r_artist[artist,]) |>  \n  mutate(mu_j = b_Intercept + r_artist) |>\n  ungroup() |> \n  mutate(artist = fct_reorder(artist, mu_j, .fun = mean)) |> \n  ggplot(aes(y = artist, x = mu_j)) +\n  stat_pointinterval(color = clrs[3]) +\n  labs(x = \"Popularity\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#posterior-prediction",
    "href": "bayes-rules/16-chapter.html#posterior-prediction",
    "title": "16: Hierarchical models without predictors",
    "section": "16.5: Posterior prediction",
    "text": "16.5: Posterior prediction\nBecause we used a hierarchical model, we aren’t limited to making predictions for only the artists that exist in the original data. We can use the model to make predictions both for existing artists and for brand new unseen artists\n\nPredictions for existing groups / artists\nThere are multiple forms of uncertainty in this model at both the global and artist levels. Like I explore in this guide to different posterior predictions, we have uncertainty in \\(mu_j\\), or the artist-specific average, and uncertainty in \\(Y\\), or the song-specific average.\nThe posterior predictions for \\(mu_j\\) will be narrow than those for \\(Y\\) because \\(mu_j\\) is only a function of the global \\(\\mu\\) and artist-specific offsets \\(b_j\\):\n\\[\n\\mu_j = \\mu + b_j\n\\]\nThis corresponds to posterior_linpred() or posterior_epred().\nThe posterior predictions for \\(Y\\) have much more uncertainty because \\(Y\\) is a function of both \\(\\mu_j\\) and \\(\\sigma_y\\), or the within-artist variability:\n\\[\nY_{i,j} \\sim \\mathcal{N}(\\mu_j, \\sigma_y)\n\\]\nThis corresponds to posterior_predict().\nWe can make these posterior predictions both manually and with the more automatic functions. We’ll predict Frank Ocean’s popularity since that’s what the book does.\nManually we need to get the intercept, offset, and sigma from the MCMC chains, add the intercept and offset to create \\(\\mu_j\\), and then randomly draw from a normal distribution with that \\(\\mu_j\\) and \\(\\sigma_y\\):\n\nManuallyAutomatically with postior_predict()\n\n\n\nocean_posterior_preds <- model_multilevel_brms %>%\n  spread_draws(b_Intercept, r_artist[artist,], sigma) |> \n  filter(artist == \"Frank.Ocean\") |> \n  mutate(mu_ocean = b_Intercept + r_artist,\n         y_ocean = rnorm(n(), mean = mu_ocean, sd = sigma))\n\nocean_posterior_preds |> \n  ggplot(aes(x = y_ocean)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Frank Ocean song\")\n\n\n\n\n\n\n\nmodel_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = \"Frank Ocean\")) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Frank Ocean song\")\n\n\n\n\n\n\n\nLooking at the posterior distribution of just \\(\\mu_j\\) gives us a narrower range, since this corresponds to our prediction of Frank Ocean’s underlying average song popularity, not the popularity of a single song:\n\nManuallyAutomatically wtih posterior_linpred()\n\n\n\nocean_posterior_preds |> \n  ggplot(aes(x = mu_ocean)) +\n  stat_halfeye(fill = clrs[2]) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Frank Ocean songs\")\n\n\n\n\n\n\n\nmodel_multilevel_brms |> \n  linpred_draws(newdata = tibble(artist = \"Frank Ocean\")) |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[2]) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Frank Ocean songs\")\n\n\n\n\n\n\n\n\n\nPredictions for unseen groups / artists\nIf we want to simulate a posterior for a completely unobserved artist, we can’t just extract their offset, add it to the global mean, and use rnorm(). There’s no offset to extract!\nSo instead we have to simulate the offset. We estimated a posterior distribution for \\(\\sigma_\\mu\\) for the variability in the offsets:\n\\[\nb_j \\sim \\mathcal{N}(0, \\sigma_{\\mu})\n\\]\n…so we can use that to generate a random offset, then add that to the global mean, and then make predictions like normal. In the book they make up an artist named “Mohsen Beats” so we’ll do that here too.\nWhen doing this automatically, we need to include re_formula = NULL and allow_new_levels = TRUE to incorporate the random effects into the predictions (see here for more about that). We also use sample_new_levels = \"gaussian\" to indicate that the random offsets we’re adding in for the new fake artist come from a normal distribution (just like we do with rnorm()). It’s also possible to use sample_new_levels = \"uncertainty\" (and I think that’s the default?), which doesn’t use rnorm() to generate the offsets but instead somehow uses the uncertainty and variation in the existing groups to create the offsets.\n\nManuallyAutomatically with posterior_predict(re_formula = NULL)\n\n\n\nmohsen_posterior_preds <- model_multilevel_brms %>%\n  spread_draws(b_Intercept, sd_artist__Intercept, sigma) |> \n  mutate(sigma_mu = sd_artist__Intercept,\n         mu_mohsen = b_Intercept + rnorm(n(), mean = 0, sd = sigma_mu),\n         y_mohsen = rnorm(n(), mean = mu_mohsen, sd = sigma))\n\nmohsen_posterior_preds |> \n  ggplot(aes(x = y_mohsen)) +\n  stat_halfeye(fill = colorspace::darken(clrs[3], 0.7)) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Mohsen Beats song\")\n\n\n\n\n\n\n\nmodel_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = \"Mohsen Beats\"),\n                  re_formula = NULL, allow_new_levels = TRUE,\n                  sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = colorspace::darken(clrs[3], 0.7)) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Mohsen Beats song\")\n\n\n\n\n\n\n\nWe can also get the posterior distribution of just the \\(\\mu_j\\) part for Mohsen Beats, providing a prediction of the artist’s average song popularity:\n\nManuallyAutomatically wtih posterior_linpred(re_formula = NULL)\n\n\n\nmohsen_posterior_preds |> \n  ggplot(aes(x = mu_mohsen)) +\n  stat_halfeye(fill = colorspace::darken(clrs[2], 0.5)) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Mohsen Beats songs\")\n\n\n\n\n\n\n\nmodel_multilevel_brms |> \n  linpred_draws(newdata = tibble(artist = \"Mohsen Beats\"),\n                re_formula = NULL, allow_new_levels = TRUE,\n                sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye(fill = colorspace::darken(clrs[2], 0.5)) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Mohsen Beats songs\")\n\n\n\n\n\n\n\n\n\nComparisons\nFor fun, we can compare all four of these posteriors:\n\np1 <- model_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = c(\"Frank Ocean\", \"Mohsen Beats\")),\n                  re_formula = NULL, allow_new_levels = TRUE,\n                  sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .prediction, y = artist, fill = artist)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[3], colorspace::darken(clrs[3], 0.7)),\n                    guide = \"none\") +\n  labs(x = \"Posterior predictions for individual songs (Yij)\\n(equivalent to posterior_predict()\",\n       y = NULL)\n\np2 <- model_multilevel_brms |> \n  linpred_draws(newdata = tibble(artist = c(\"Frank Ocean\", \"Mohsen Beats\")),\n                re_formula = NULL, allow_new_levels = TRUE,\n                sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .linpred, y = artist, fill = artist)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], colorspace::darken(clrs[2], 0.5)),\n                    guide = \"none\") +\n  labs(x = \"Posterior predictions for artist average (µj)\\n(equivalent to posterior_linpred()\",\n       y = NULL)\n\np1 | p2"
  },
  {
    "objectID": "bayes-rules/16-chapter.html#shrinkage-and-the-bias-variance-tradeoff",
    "href": "bayes-rules/16-chapter.html#shrinkage-and-the-bias-variance-tradeoff",
    "title": "16: Hierarchical models without predictors",
    "section": "16.6: Shrinkage and the bias-variance tradeoff",
    "text": "16.6: Shrinkage and the bias-variance tradeoff\nWe’ve been talking about how information is shared across artists in these hierarchical models, and that’s why they’re better than the no pooling approach, but what information actually gets shared? In these intercept-only models, there aren’t any other covariates that are spread across artists (like age of artist, number of years producing music, gender, income, whatever) that could inform between-artist variation. All we have is the artist name.\nEven with the artist name, we’re still sharing information across artists, but in a more subtle way. The average popularity for each artist is heavily informed by the number of songs each artist has. Artists with just a handful of songs might have high popularity, but that average is really unstable; artists with lots of songs and who are also highly popular have a much more stable average.\n(This is like Amazon ratings too: an item on Amazon with a 4.6 star average with 10,000 reviews is more trustworthy than an item with a 4.8 star average with 4 reviews)\nIn the book they compare the ratings and counts of Camila Cabello (38 songs) and Lil Skies (3 songs):\n\nspotify |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\")) |> \n  group_by(artist) |> \n  summarize(count = n(), popularity = mean(popularity))\n## # A tibble: 2 × 3\n##   artist         count popularity\n##   <fct>          <int>      <dbl>\n## 1 Camila Cabello    38       77.1\n## 2 Lil Skies          3       79.3\n\nIn a model without pooling, Lil Skies’s average remains high because it ignores the effect of artist-specific sample size, but hierarchical models care about the number of songs within each artist, and that influences the predicted average for artists with fewer songs. That’s the information sharing we get in an intercept-only model.\nWe can compare the predictions from the no pooling and multilevel models for these two to see the smaller prediction after sharing information in the hierarchical model:\n\nno_pooled_small <- no_pooled_predictions |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\")) |> \n  mutate(model_type = \"No pooling\") |> \n  ungroup() |> \n  select(artist, .row, .chain, .iteration, .draw, .prediction, model_type)\n\nmultilevel_small <-  model_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = unique(spotify$artist))) |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\")) |> \n  mutate(model_type = \"Multilevel\") |> \n  ungroup() |> \n  select(artist, .row, .chain, .iteration, .draw, .prediction, model_type)\n\nboth_small <- rbind(no_pooled_small, multilevel_small)\n\nactual_small <- spotify |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\"))\n\nboth_small |> \n  ggplot(aes(x = .prediction, y = fct_rev(artist), color = model_type)) +\n  stat_pointinterval(position = position_dodge(width = 0.4)) +\n  stat_summary(data = actual_small, \n               aes(x = popularity, color = \"Original data\"), \n               fun.data = \"mean_se\", fun.args = list(mult = 1.96),\n               geom = \"pointrange\", size = 1, pch = 18) +\n  scale_color_manual(values = c(clrs[1], \"grey50\", clrs[3]),\n                     breaks = c(\"No pooling\", \"Original data\", \"Multilevel\")) +\n  labs(x = \"Popularity\", y = NULL, color = NULL) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nFor Camila Cabello, the averages are all basically the same since she has so many songs (38). For Lil Skies, though, the predicted average in the multilevel model is smaller than either the no pooled data or the observed mean, since the partial pooling takes the small number of songs (3) into account and shrinks the prediction.\nWe can see this phenomenon if we look at everyone’s posterior predictions. For artists near the average level of popularity, the model’s predictions are basically perfectly inline with the observed average. For artists with more extreme levels of popularity, the multilevel model pulls their averages toward the middle, particularly when artists only have a handful of songs. You can see a sizable divergence in observed and predicted averages for artists like Lil Skies and Sean Kingston at the top end of popularity and Mia X and Soul & Roll at the bottom end. In an artist in the extremes has a lot of songs (like Camila Cabello or Beyoncé), there’s no shrinkage—the higher number of songs helps stabilize the prediction.\n\nmultilevel_predictions <- model_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = unique(spotify$artist)))\n\nmultilevel_predictions |> \n  ggplot(aes(y = artist, x = .prediction)) +\n  stat_pointinterval(color = clrs[3], point_interval = \"mean_qi\") +\n  stat_summary(data = spotify, aes(x = popularity), \n               geom = \"point\", fun = mean, color = clrs[2], size = 3, pch = 18) +\n  labs(x = \"Popularity\", y = NULL)"
  },
  {
    "objectID": "bayes-rules/17-chapter.html",
    "href": "bayes-rules/17-chapter.html",
    "title": "17: Hierarchical models with predictors",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "bayes-rules/17-chapter.html#the-general-setup",
    "href": "bayes-rules/17-chapter.html#the-general-setup",
    "title": "17: Hierarchical models with predictors",
    "section": "The general setup",
    "text": "The general setup\nIn this chapter we’re back to the running data from chapter 15, with race times nested in runners:\n\n\n\n\n\nPartial pooling with hierarchical models\n\n\n\n\nWe want to model race times (net) based on runner characteristics runner and age age. We’ll use \\(Y_{i,j}\\) to refer to race times and \\(X_{i,j}\\) to refer to age, where \\(i\\) refers individual races and \\(j\\) refers to runners."
  },
  {
    "objectID": "bayes-rules/17-chapter.html#first-steps-complete-pooling",
    "href": "bayes-rules/17-chapter.html#first-steps-complete-pooling",
    "title": "17: Hierarchical models with predictors",
    "section": "17.1: First steps: Complete pooling",
    "text": "17.1: First steps: Complete pooling\nWe’ll start by recreating the complete pooling model from chapter 15:\n\\[\n\\begin{aligned}\n\\text{Race time}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\, \\text{Age}_i \\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(0, 35) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/10)\n\\end{aligned}\n\\]\nI’ll just do it in brms here—for rstanarm, go look at either the book or my notes for chapter 15\n\npriors <- c(prior(normal(0, 35), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(0.1), class = sigma))\n\nmodel_complete_pooling_brms <- brm(\n  bf(net ~ age),\n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"17-manual_cache/complete-pooling-brms\"\n)\n\n\ntidy(model_complete_pooling_brms, effects = \"fixed\", conf.level = 0.8)\n## # A tibble: 2 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   <chr>  <chr>     <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  cond      (Intercept)   75.2      24.6     43.1     107.   \n## 2 fixed  cond      age            0.267     0.447   -0.306     0.850\n\nAgain, this isn’t the best model ever. It ignores within-runner trends in age. It implies that running time increases by 0.27 minutes for each additional year runners age, but with an 80% credible interval of -0.31–0.85 minutes. That means there’s no substantial relationship between age and race time, even though we’d assume that people get slower over time.\n\nmodel_complete_pooling_brms |> \n  add_epred_draws(newdata = tibble(age = seq(min(running$age), \n                                             max(running$age), \n                                             length.out = 100))) |> \n  ggplot(aes(x = age, y = .epred)) +\n  geom_smooth(data = running,\n              aes(y = net, group = runner), method = \"lm\", se = FALSE, \n              color = \"grey60\", size = 0.5) +\n  stat_lineribbon(alpha = 0.25, fill = clrs[4], color = clrs[4]) +\n  labs(x = \"Age\", y = \"Race time\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n## Warning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`."
  },
  {
    "objectID": "bayes-rules/17-chapter.html#hierarchical-model-with-varying-intercepts",
    "href": "bayes-rules/17-chapter.html#hierarchical-model-with-varying-intercepts",
    "title": "17: Hierarchical models with predictors",
    "section": "17.2: Hierarchical model with varying intercepts",
    "text": "17.2: Hierarchical model with varying intercepts\nFirst we’ll build a model that accounts for varying runner-specific average race times, but ignores runner-specific age trends in race times. That is, we’ll assume one global age trend, but each runner will have their own intercept for that trend based on runner-specific characteristics.\nTo do this we need to think about all three layers involved in this model:\n\nRunning times within each runner \\(j\\)\nHow average running times vary between runners\nGlobal parameters like average running time, variance, etc.\n\n\nLayer 1: Variability within runners\nIn this first layer, we can say that each runner’s race time varies around their mean (\\(\\mu_{i,j}\\)) with some within-runner variability \\(\\sigma_y\\). In previous chapters, we’ve just looked at \\(\\mu_j\\) as an average (average running time; average popularity), but we can actually model it using a linear model and incorporate age effects, resulting in a model like this:\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j}\n\\end{aligned}\n\\]\nWith this layer of the model, we have a few parameters:\n\n\\(\\beta_{0_j}\\) is the runner-specific intercept for runner \\(j\\). It is group-specific.\n\\(\\beta_1\\) is the global coefficient (i.e. no subscripts) for the effect of age on race time. It is global and shared across all runners.\n\\(\\sigma_y\\) is the within-runner variability for the regression model. It measures the strength of the relationship between an individual runner’s age and their race time. It is also global and shared across all runners.\n\n\n\nLayer 2: Variability between runners\nNext we need to think about the variability between runners. With only random intercepts, we’re just going to worry about \\(\\beta_0\\) in this stage.\n\\[\n\\beta_{0_j} \\sim \\mathcal{N}(\\beta_0, \\sigma_0)\n\\]\nWe have two parameters here:\n\n\\(\\beta_0\\) is the global average intercept across all runners, or the average runner’s race time\n\\(\\sigma_0\\) is the between-runner variability around that global average, or how much race times bounce around the average\n\n\n\nLayer 3: Global priors\nFinally we need priors for all the global parameters on the righthand side of these different layers:\n\n\\(\\beta_0\\): Average race time for all runners\n\\(\\beta_1\\): Effect of age on race time for all runners\n\\(\\sigma_y\\): Within-runner variability of race time\n\\(\\sigma_0\\): Between-runner variability of average runner race time\n\n\n\nCombined model\nPhew. With all these pieces, we can present the final formal model:\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Race times within runners } j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} & \\text{Linear model of within-runner variation} \\\\\n\\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0) & \\text{Variability in average times between runners} \\\\\n\\\\\n\\beta_0 &\\sim \\text{Some prior} & \\text{Global priors} \\\\\n\\beta_1 &\\sim \\text{Some prior} \\\\\n\\sigma_y &\\sim \\text{Some prior} \\\\\n\\sigma_0 &\\sim \\text{Some prior}\n\\end{aligned}\n\\]\n\n\nOther way of writing all this: offsets\nWe can also think about the between-runner variation as a series of offsets from the global mean, which I find easier to conceptualize (and the inner workings of brms treat them this way, since running ranef() gets you these offsets). Here, each runner-specific intercept is a combination of the global average and a runner-specific offset from that average:\n\\[\n\\beta_{0_j} = \\beta_0 + b_{0_j}\n\\]\nThese offsets come from a normal distribution with some standard deviation \\(\\sigma_0\\):\n\\[\nb_{0_j} \\sim \\mathcal{N}(0, \\sigma_0)\n\\]\nWe can add these offsets directly to the model:\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Race times within runners } j \\\\\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1 X_{i_j} & \\text{Linear model of within-runner variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random runner offsets} \\\\\n\\\\\n\\beta_0 &\\sim \\text{Some prior} & \\text{Global priors} \\\\\n\\beta_1 &\\sim \\text{Some prior} \\\\\n\\sigma_y &\\sim \\text{Some prior} \\\\\n\\sigma_0 &\\sim \\text{Some prior}\n\\end{aligned}\n\\]\n\n\nPrior simulation\nIt’s been a few chapters since we’ve done this, but it’s a good idea to set specific priors and then simulate from them to see if they’re reasonable. (In previous chapters the book used rstanarm’s magical autoscale thing; I chose my own reasonableish priors, but never simulated from them. Also, back in chapter 15 we didn’t center the intercept, but here we will because it makes interpretation and prior setting easier.)\nHere’s the reasoning they go through in the book:\n\nA typical runner in this age group runs between an 8-minute and 12-minute mile, and the race is 10 miles, so average race time should be 80–120 minutes, so we’ll use \\(\\mathcal{N}(100, 10)\\)\nWe think that race times increase as age increases. In the book they say it’s probably somewhere between 0.5–4.5 minutes a year, so \\(\\mathcal{N}(2.5, 1)\\)\nFor the variability between runners (\\(\\sigma_0\\), or the offsets) and the variability within runners (\\(\\sigma_y\\)), we don’t know much, other than the fact that they have to be positive. We’ll say that they vary moderately with a standard deviation of 10 minutes, so we’ll use \\(\\operatorname{Exponential}(1/10)\\)\n\nThat leaves us with this official model:\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Race times within runners } j \\\\\n\\mu_{i_j} &= (\\beta_{0c} + b_{0_j}) + \\beta_1 X_{i_j} & \\text{Linear model of within-runner variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random runner offsets} \\\\\n\\\\\n\\beta_{0_c} &\\sim \\mathcal{N}(100, 10) & \\text{Prior for global average} \\\\\n\\beta_1 &\\sim \\mathcal{N}(2.5, 1) & \\text{Prior for global age effect} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for within-runner variability} \\\\\n\\sigma_0 &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for between-runner variability}\n\\end{aligned}\n\\]\nAnd here’s what these priors look like:\n\np1 <- ggplot() +\n  stat_function(fun = ~dnorm(., 100, 10),\n                geom = \"area\", fill = clrs[1]) +\n  xlim(c(60, 140)) +\n  labs(x = \"**β<sub>0c</sub>**<br>Global average runner race time\") +\n  theme(axis.title.x = element_markdown(), axis.text.y = element_blank(), \n        axis.title.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\np2 <- ggplot() +\n  stat_function(fun = ~dnorm(., 2.5, 1),\n                geom = \"area\", fill = clrs[2]) +\n  xlim(c(-1, 6)) +\n  labs(x = \"**β<sub>1</sub>**<br>Global effect of age on race time\") +\n  theme(axis.title.x = element_markdown(), axis.text.y = element_blank(), \n        axis.title.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\np3 <- ggplot() +\n  stat_function(fun = ~dexp(., 1/10),\n                geom = \"area\", fill = clrs[3]) +\n  xlim(c(0, 60)) +\n  labs(x = \"**σ<sub>y</sub>** and **σ<sub>0</sub>**<br>Variation *within* individuals' race times<br>and *between* average runner race times\") +\n  theme(axis.title.x = element_markdown(), axis.text.y = element_blank(), \n        axis.title.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\np1 | p2 | p3\n\n\n\n\nLet’s simulate these priors, both with brms and with rstanarm:\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd))\n\nmodel_running1_brms_prior <- brm(\n  net ~ age_c + (1 | runner), \n  data = running,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"17-manual_cache/running1-brms-prior\"\n)\n\n\np1 <- running |> \n  add_predicted_draws(model_running1_brms_prior, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted race time\", y = \"Density\") +\n  coord_cartesian(xlim = c(-100, 300))\n\np2 <- running |> \n  add_linpred_draws(model_running1_brms_prior, ndraws = 6, \n                    seed = 12345) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = paste(runner, .draw)),\n            color = clrs[1], size = 0.25) +\n  labs(x = \"Age\", y = \"Race time\") +\n  coord_cartesian(ylim = c(50, 130)) +\n  facet_wrap(vars(.draw))\n\n(p1 | p2) + plot_layout(widths = c(0.3, 0.7))\n\n\n\n\n\n\n\nmodel_running1_rstanarm_prior <- stan_glmer(\n  net ~ age_c + (1 | runner), \n  data = running, family = gaussian,\n  prior_intercept = normal(100, 10),\n  prior = normal(2.5, 1), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, cores = 4, iter = 5000*2, seed = 84735, refresh = 0,\n  prior_PD = TRUE)\n\n\np1 <- running |> \n  add_predicted_draws(model_running1_rstanarm_prior, ndraws = 100) |> \n  ggplot(aes(x = .prediction, group = .draw)) +\n  geom_density(size = 0.25, color = clrs[3]) +\n  labs(x = \"Predicted race time\", y = \"Density\") +\n  coord_cartesian(xlim = c(-100, 300))\n\np2 <- running |> \n  add_linpred_draws(model_running1_rstanarm_prior, ndraws = 6, \n                    seed = 12345) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = paste(runner, .draw)),\n            color = clrs[1], size = 0.25) +\n  labs(x = \"Age\", y = \"Race time\") +\n  coord_cartesian(ylim = c(50, 130)) +\n  facet_wrap(vars(.draw))\n\n(p1 | p2) + plot_layout(widths = c(0.3, 0.7))\n\n\n\n\n\n\n\nThe plots from these prior simulations show us 100 plausible sets of overall race times, as well as six sets of 36 simulated runners. The race time predictions are all plausible-looking, clustered around 100 minutes with most variation between 50 and 150 minutes. The repeated 36 runners also look okay—the relationship between age and race time is consistently positive and never super steep, and there’s good random variation across the intercepts, as expected.\n\n\nPosterior simulation and analysis\nActual posterior time!\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd))\n\nmodel_running1_brms <- brm(\n  net ~ age_c + (1 | runner), \n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"17-manual_cache/running1-brms\"\n)\n\n\n\n\nmodel_running1_rstanarm <- stan_glmer(\n  net ~ age_c + (1 | runner), \n  data = running, family = gaussian,\n  prior_intercept = normal(100, 10),\n  prior = normal(2.5, 1), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, cores = 4, iter = 5000*2, seed = 84735, refresh = 0)\n\n\n\n\n\nGlobal analysis\nFirst we’ll look at the relationship between running time and age for the typical runner. This involves just the global \\(\\beta_0\\) and the global \\(\\beta_1\\), which don’t have any group-specific elements.\n\nbrmsrstanarm\n\n\n\nmodel_running1_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 2 × 6\n##   effect term        estimate std.error conf.low conf.high\n##   <chr>  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  (Intercept)    90.8      2.13     88.0      93.5 \n## 2 fixed  age_c           1.30     0.219     1.02      1.58\n\n\n\n\nmodel_running1_rstanarm |> \n  tidy(effects = c(\"fixed\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 2 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)    90.6      2.17     87.8      93.5 \n## 2 age_c           1.30     0.219     1.02      1.58\n\n\n\n\nWe’ve got two coefficients to interpret:\n\n\\(\\beta_0\\) is 90.77 is the overall global posterior mean race time for all runners. There’s an 80% chance that it’s between 88.03 and 93.48 minutes.\n\\(\\beta_1\\) shows that a typical runner slows down by a posterior mean of 1.3 minutes each year. There’s an 80% chance that the effect is between 1.02 and 1.58 minutes, which definitely isn’t zero, so we’re pretty confident that there’s a “significant” and substantial age effect.\n\nHere’s what this posterior relationship looks like. Again, this is for a typical runner.\n\nmodel_running1_brms |> \n  linpred_draws(running, ndraws = 200, re_formula = NA) |> \n  ggplot(aes(x = age, y = net)) +\n  stat_lineribbon(aes(y = .linpred), fill = clrs[4], color = clrs[4], alpha = 0.2) +\n  labs(x = \"Age\", y = \"Race time\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\nThis is neat! When we pooled all the data together, we didn’t have any “significant” effect. After taking runner-specific characteristics into account, we found an actual substantial, practical effect.\n\n\nRunner-specific analysis\nNext we can explore the model’s group details by looking at the different runner specific offsets from the global \\(\\beta_0\\) mean, or the different \\(b_{0_j}\\) terms:\n\\[\n\\beta_0 + b_{0_j}\n\\]\nFirst, we can look at just the intercepts and how much they differ from the global mean. Runner 10 is exceptionally slow compared to the general average; Runners 29 and 30 are exceptionally fast:\n\nglobal_b0 <- model_running1_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  filter(term == \"(Intercept)\")\n\nrunning1_runner_offsets <- model_running1_brms |> \n  spread_draws(b_Intercept, r_runner[runner,]) |> \n  mutate(runner_intercept = b_Intercept + r_runner) |> \n  ungroup() |> \n  mutate(runner = fct_reorder(factor(runner), runner_intercept, .fun = mean))\n\n# Instead of manually adding the offsets to the intercept, we could also use\n# posterior_linpred() to calculate the linear predictions for each runner when\n# all other covariates are set to 0; the results are identical\n# running1_runner_offsets <- model_running1_brms |> \n#   linpred_draws(tibble(runner = unique(running$runner),\n#                        age_c = 0)) |> \n#   ungroup() |> \n#   mutate(runner = fct_reorder(factor(runner), .linpred, .fun = mean))\n\nrunning1_runner_offsets |> \n  ggplot(aes(x = runner_intercept, y = runner)) +\n  annotate(geom = \"rect\", ymin = -Inf, ymax = Inf,\n           xmin = global_b0$conf.low, xmax = global_b0$conf.high,\n           fill = clrs[2], alpha = 0.4) +\n  geom_vline(xintercept = global_b0$estimate, color = clrs[2]) +\n  stat_pointinterval(color = clrs[3])\n\n\n\n\nNext we can see how these runner-specific offsets influence the age-race time relationship. In the book they compare posterior linear predictions for runners 4 and 5. Just shifting the intercept around helps a lot with model fit!\n\nrunning |> \n  filter(runner %in% c(\"4\", \"5\")) |> \n  add_linpred_draws(model_running1_brms, ndraws = 100) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = paste(runner, .draw), color = runner),\n            size = 0.3, alpha = 0.2) +\n  geom_point(aes(color = runner)) +\n  scale_color_manual(values = c(clrs[3], clrs[6])) +\n  labs(x = \"Age\", y = \"Race time\", color = \"Runner\")\n\n\n\n\nFinally, the book plots the posterior median slope and intercepts for all 36 runners. Most runners’ lines are clustered around the global median, as expected:\n\nrunning1_all_runner_lines <- model_running1_brms |> \n  linpred_draws(expand_grid(runner = unique(running$runner),\n                            age_c = c(50 - unscaled$age_c$scaled_center,\n                                      61 - unscaled$age_c$scaled_center))) |> \n  group_by(runner, age_c) |> \n  median_qi(.linpred) |> \n  mutate(age = age_c + unscaled$age_c$scaled_center)\n\nrunning1_global_trend <- running1_all_runner_lines |> \n  group_by(age_c) |> \n  median_qi(.linpred) |> \n  mutate(age = age_c + unscaled$age_c$scaled_center)\n\nrunning1_all_runner_lines |> \n  ggplot(aes(x = age, y = .linpred)) +\n  geom_line(aes(group = runner), color = \"grey70\") +\n  geom_line(data = running1_global_trend, color = clrs[5], size = 2) +\n  coord_cartesian(ylim = c(50, 135))\n\n\n\n\n\n\nWithin- and between-runner variability\nFinally we can look at the two \\(\\sigma\\) terms: \\(\\sigma_y\\) (sd__Observation) for the variability within runners and \\(\\sigma_0\\) (sd__(Intercept) for the runner group) for the variability between runners’ baseline averages, or the variability around the \\(b_{0_j}\\) offsets.\n\nbrmsrstanarm\n\n\n\nmodel_running1_brms |> \n  tidy(effects = c(\"ran_pars\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 2 × 7\n##   effect   group    term            estimate std.error conf.low conf.high\n##   <chr>    <chr>    <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 ran_pars runner   sd__(Intercept)    13.5      1.65     11.5      15.6 \n## 2 ran_pars Residual sd__Observation     5.21     0.306     4.83      5.62\n\n\n\n\nmodel_running1_rstanarm |> \n  tidy(effects = c(\"ran_pars\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 2 × 3\n##   term                    group    estimate\n##   <chr>                   <chr>       <dbl>\n## 1 sd_(Intercept).runner   runner      13.4 \n## 2 sd_Observation.Residual Residual     5.25\n\n\n\n\nThe \\(\\sigma_y\\) (or sd__Observation) term here is 5.21, which means that within any runner, their race times vary by 5 minutes around their individual race time average.\n\\(\\sigma_0\\) (or sd__(Intercept) for the runner group) on the other hand, is 13.48, which means that average runner speeds vary or bounce around by 13 minutes across runners. There’s thus more variation between runners than within individual runners.\nIf we square these terms and make ratios of the values, we can see how much of the total model variation comes from within-runner and between-runner variation:\n\nmodel_running1_brms |> \n  tidy(effects = c(\"ran_pars\"), conf.int = FALSE) |> \n  select(-component, -effect, -std.error) |> \n  mutate(sigma_2 = estimate^2) |> \n  mutate(props = sigma_2 / sum(sigma_2))\n## # A tibble: 2 × 5\n##   group    term            estimate sigma_2 props\n##   <chr>    <chr>              <dbl>   <dbl> <dbl>\n## 1 runner   sd__(Intercept)    13.5    182.  0.870\n## 2 Residual sd__Observation     5.21    27.2 0.130\n\nNeat! 87% of the variability in race times comes from between-runner differences, while 13% comes from variations within individual runners.\nWe can also use performance::icc() to calculate this ratio automatically:\n\nperformance::icc(model_running1_brms, by_group = TRUE)\n## # ICC by Group\n## \n## Group  |   ICC\n## --------------\n## runner | 0.870"
  },
  {
    "objectID": "bayes-rules/17-chapter.html#hierarchical-model-with-varying-intercepts-slopes",
    "href": "bayes-rules/17-chapter.html#hierarchical-model-with-varying-intercepts-slopes",
    "title": "17: Hierarchical models with predictors",
    "section": "17.3: Hierarchical model with varying intercepts & slopes",
    "text": "17.3: Hierarchical model with varying intercepts & slopes\nFun times so far, and even funner times ahead.\nHowever, we’ve just been working with varying intercepts, assuming that the relationship between age and speed is the same for each runner. In reality, though, that’s not the case. Some runners slow down faster or slower over time—some even get faster as they age. If we compare this random-intercepts-only model to the actual trends in the data we can see that we’ve missed those varying runner effects—every runner in the right panel here is perfectly parallel. To capture these different age-race relationships within runners, we can use both random intercepts and random slopes.\n\np1 <- running |> \n  ggplot(aes(x = age, y = net, group = runner)) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5, color = clrs[1]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = \"Race time\",\n       title = \"Observed data\",\n       subtitle = \"Basic per-runner OLS models\")\n\np2 <- running |> \n  add_linpred_draws(model_running1_brms, ndraws = 100) |> \n  summarize(.linpred = mean(.linpred)) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = runner),\n            size = 0.5, color = clrs[3]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = \"Race time\", \n       title = \"Random intercepts only\",\n       subtitle = \"Posterior means\")\n\np1 | p2\n\n\n\n\n\nModel building\nGetting random slopes into the model requires some tinkering with the formal model structure\nWith layer 1 (within-runner variation), we now use \\(\\beta_{1_j}\\) instead of the global \\(\\beta_1\\) term from before, showing that each runner \\(j\\) gets their own \\(\\beta_1\\):\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j}\n\\end{aligned}\n\\]\nIn the non-offset-based syntax, we can then say that both \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\) follow some random distribution with coefficient-specific variance (\\(\\sigma_0\\) and \\(\\sigma_1\\) now instead of just \\(\\sigma_0\\)):\n\\[\n\\begin{aligned}\n\\beta_{0_j} \\sim \\mathcal{N}(\\beta_0, \\sigma_0) \\\\\n\\beta_{1_j} \\sim \\mathcal{N}(\\beta_1, \\sigma_1)\n\\end{aligned}\n\\]\nLife gets a little trickier with these terms, though, because \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\) are correlated and move together within each runner. So instead of writing the models for \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\) as two separate lines, we have to consider them together:\n\\[\n\\left(\n  \\begin{array}{c}\n  \\beta_{0_j} \\\\\n  \\beta_{1_j}\n  \\end{array}\n\\right)\n\\sim \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\end{array}\n  \\right)\n  , \\,\\Sigma\n\\right)\n\\]\nWritten like this, we can draw values for \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\) from a multivariate (or joint) normal distribution with a shared covariance \\(\\Sigma\\), which includes both the variability and the correlation between \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\):\n\\[\n\\Sigma =\n\\left(\n  \\begin{array}{cc}\n     \\text{Var}_{\\beta_0} & \\text{Cov}_{\\beta_0, \\beta_1} \\\\\n     \\text{Cov}_{\\beta_0, \\beta_1} & \\text{Var}_{\\beta_1}\n  \\end{array}\n\\right)\n\\]\nWe can transform the variance term into Greek letters by replacing \\(\\text{Var}_{\\beta_0}\\) with \\(\\sigma^2_{\\beta_0}\\). The covariance term can change to \\(\\sigma_{\\beta_0, \\beta_1}\\), but that’s a little trickier to work with because thinking about covariance values is less intuitive than thinking about standard deviations or correlation. So to make life easier, we can do a little algebra to rewrite the covariance as a function of the correlation \\(\\rho_{\\beta_0, \\beta_1}\\) and the two standard deviations \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\).\n\\[\n\\begin{aligned}\n\\rho_{\\beta_0, \\beta_1} &= \\frac{\\sigma_{\\beta_0, \\beta_1}}{\\sigma_{\\beta_0} \\sigma_{\\beta_0}} \\\\\n\\sigma_{\\beta_0, \\beta_1} &= \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_0}\n\\end{aligned}\n\\]\nThat leaves us with this fun mess (with the lower triangle omitted because it’s repeated, and because that’s what O’Dea, Noble, and Nakagawa (2021) do in their neat overview-of-multilevel-models paper):\n\\[\n\\Sigma =\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\]\nIn Bayes Rules! they simplify this more by getting rid of the \\(\\beta\\)s in the subscripts:\n\\[\n\\Sigma =\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{0} & \\rho_{0, 1}\\, \\sigma_{0} \\sigma_{1} \\\\\n     \\dots & \\sigma^2_{1}\n  \\end{array}\n\\right)\n\\]\nThis matrix has the two \\(\\sigma_0\\) and \\(\\sigma_1\\) terms that control the variability in \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\), and it also has a messy new \\(\\rho_{0, 1}\\, \\sigma_{0} \\sigma_{1}\\) term for the correlation between \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\)\nIn Bayes Rules! they show some plots with simulated data that demonstrate what this correlation actually means. They don’t include the code for it, so I’ll make up my own version here:\n\nwithr::with_seed(123, {\n  rho_plots <- tibble(rho = c(-0.99, 0, 0.99)) |> \n    mutate(title = glue::glue(\"ρ = {rho}\"),\n           subtitle = c(\"Strong negative correlation\\nbetween slope and intercept\",\n                        \"No correlation\\nbetween slope and intercept\",\n                        \"Strong positive correlation\\nbetween slope and intercept\")) |> \n    mutate(Sigma = map(rho, ~matrix(c(1, .x, .x, 1), 2, 2))) |> \n    mutate(data = map(Sigma, ~{\n      MASS::mvrnorm(n = 100, mu = c(2, 3), Sigma = .x) |> \n        as_tibble() |> \n        rename(b0 = V1, b1 = V2)\n    })) |> \n    mutate(plot = pmap(list(data, title, subtitle), ~{\n      ggplot(..1) +\n        geom_abline(aes(intercept = b0, slope = b1, color = b0),\n                    size = 0.3) +\n        scale_color_viridis_c(option = \"rocket\", begin = 0.1, end = 0.85,\n                              limits = c(-1, 5)) +\n        labs(title = ..2, subtitle = ..3, color = \"β<sub>0</sub>\") +\n        lims(x = c(0, 3), y = c(0, 10)) +\n        coord_axes_inside() +\n        theme(axis.line = element_line(),\n              legend.title = element_markdown())\n    }))\n})\n\nwrap_plots(rho_plots$plot) + \n  plot_layout(guides = \"collect\")\n\n\n\n\nWhen \\(\\rho\\) is negative, bigger intercepts have smaller slopes. In the context of runners, this would mean that individual runners who have longer baseline race time averages would see less of an age effect. When \\(\\rho\\) is negative, bigger intercepts have steeper slopes—individual runners with longer baseline race times would see a stronger age effect.\nSOO with all that, here’s what the final formal model with varying intercepts and slopes looks like:\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Race times within runners } j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} & \\text{Linear model of within-runner variation} \\\\\n\\left(\n  \\begin{array}{c}\n  \\beta_{0_j} \\\\\n  \\beta_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n  \\left(\n  \\begin{array}{cc}\n     \\sigma^2_{0} & \\rho_{0, 1}\\, \\sigma_{0} \\sigma_{1} \\\\\n     \\dots & \\sigma^2_{1}\n  \\end{array}\n\\right)\n\\right) & \\text{Variability in average runner intercepts and slopes} \\\\\n\\\\\n\\beta_{0_c} &\\sim \\mathcal{N}(100, 10) & \\text{Prior for global average} \\\\\n\\beta_1 &\\sim \\mathcal{N}(2.5, 1) & \\text{Prior for global age effect} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for within-runner variability} \\\\\n\\sigma_0 &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for between-runner intercept variability} \\\\\n\\sigma_1 &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for between-runner slope variability} \\\\\n\\rho &\\sim \\operatorname{LKJ}(1) & \\text{Prior for between-runner variability}\n\\end{aligned}\n\\]\n\nWe can also write this using offset-based notation:\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Race times within runners } j \\\\\n\\mu_{i_j} &= (\\beta_{0c} + b_{0_j}) + (\\beta_1 + b_{1_j}) X_{i_j} & \\text{Linear model of within-runner variation} \\\\\n\\left(\n  \\begin{array}{c}\n  b_{0_j} \\\\\n  b_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    0 \\\\\n    0 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n  \\left(\n  \\begin{array}{cc}\n     \\sigma^2_{0} & \\rho_{0, 1}\\, \\sigma_{0} \\sigma_{1} \\\\\n     \\dots & \\sigma^2_{1}\n  \\end{array}\n\\right)\n\\right) & \\text{Variability in average runner intercepts and slopes} \\\\\n\\\\\n\\beta_{0_c} &\\sim \\mathcal{N}(100, 10) & \\text{Prior for global average} \\\\\n\\beta_1 &\\sim \\mathcal{N}(2.5, 1) & \\text{Prior for global age effect} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for within-runner variability} \\\\\n\\sigma_0 &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for between-runner intercept variability} \\\\\n\\sigma_1 &\\sim \\operatorname{Exponential}(1/10) & \\text{Prior for between-runner slope variability} \\\\\n\\rho &\\sim \\operatorname{LKJ}(1) & \\text{Prior for between-runner variability}\n\\end{aligned}\n\\]\n\nThere’s one new thing here in the priors—we need to set a prior for the correlation of the slopes and intercepts, or \\(\\rho_{0,1}\\). This can get really complex. Bayes Rules! has a whole optional section showing how it’s possible to decompose the \\(\\Sigma\\) covariance matrix into three component parts:\n\n\\(R\\), or a regularization hyperparameter\n\\(\\tau\\), or the total standard deviation in the intercepts and slopes\n\\(\\pi_0\\) and \\(\\pi_1\\), or the relative proportion of the variability between groups that comes from differing intercepts and slopes\n\nThat’s a ton of fine tuning! You can set these priors in rstanarm like so:\nstan_glmer(\n  ...\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  ...\n)\nbrms makes this a lot easier (though the lkj() prior lets you set all these decomposed parts too!) and only makes you set a prior for the \\(\\rho\\) term by itself, or the cor class here:\n\nget_prior(\n  net ~ age_c + (1 + age_c | runner), \n  data = running,\n  family = gaussian()\n)\n##                     prior     class      coef  group resp dpar nlpar lb ub       source\n##                    (flat)         b                                             default\n##                    (flat)         b     age_c                              (vectorized)\n##                    lkj(1)       cor                                             default\n##                    lkj(1)       cor           runner                       (vectorized)\n##  student_t(3, 89.4, 14.2) Intercept                                             default\n##     student_t(3, 0, 14.2)        sd                                   0         default\n##     student_t(3, 0, 14.2)        sd           runner                  0    (vectorized)\n##     student_t(3, 0, 14.2)        sd     age_c runner                  0    (vectorized)\n##     student_t(3, 0, 14.2)        sd Intercept runner                  0    (vectorized)\n##     student_t(3, 0, 14.2)     sigma                                   0         default\n\nThis just needs to be a value between -1 and 1, like a regular correlation coefficient. By default, brms uses an LKJ distribution, which is a strange beast that ranges between -1 and 1. There’s no built-in function like dlkj(), and I haven’t found a comparable function in other packages like extraDistr, but tidybayes supports it!\nThe LKJ distribution takes one hyperparameter \\(\\eta\\) that controls the shape:\n\nWhen \\(\\eta < 1\\), extreme correlations are more likely\nWhen \\(\\eta = 1\\), all correlations are equally likely\nWhen \\(\\eta > 1\\), central correlations are more likely, and the larger the \\(\\eta\\), the narrower the distribution is around 0\n\n\ntibble(eta = c(0.5, 1, 5, 15)) |> \n  mutate(prior = glue::glue(\"lkjcorr({eta})\")) |>\n  mutate(prior_nice = fct_inorder(glue::glue(\"LKJ(η = {eta})\"))) |> \n  parse_dist(prior) |> \n  marginalize_lkjcorr(K = 2) |>  # K = dimension of correlation matrix; ours is 2x2 here\n  ggplot(aes(y = 0, dist = .dist, args = .args)) +\n  stat_slab(fill = clrs[5]) +\n  labs(x = \"**ρ**<br>Correlation between β<sub>0</sub> and β<sub>1</sub>\") +\n  theme(axis.title.x = element_markdown(), axis.text.y = element_blank(), \n        axis.title.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid = element_blank()) +\n  facet_wrap(vars(prior_nice), nrow = 1)\n\n\n\n\n\n\nPosterior simulation and analysis\nWe’ll skip the prior simulation and go right to model fitting and exploring the posterior.\n\nbrmsrstanarm\n\n\nBump up the adapt_delta to help with convergence here.\n\npriors <- c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd),\n            prior(lkj(1), class = cor))\n\nmodel_running2_brms <- brm(\n  net ~ age_c + (1 + age_c | runner), \n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0, control = list(adapt_delta = 0.9),\n  file = \"17-manual_cache/running2-brms\"\n)\n\n\n\n\nmodel_running2_rstanarm <- stan_glmer(\n  net ~ age_c + (1 + age_c | runner), \n  data = running, family = gaussian,\n  prior_intercept = normal(100, 10),\n  prior = normal(2.5, 1), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, cores = 4, iter = 5000*2, seed = 84735, refresh = 0, adapt_delta = 0.9)\n\n\n\n\nHoly moly we have a bunch of things we can work with now—78 different parameters! 36 runner-specific intercept offsets, 36 runner-specific slope offsets, and 6 global terms.\n\nget_variables(model_running2_brms)\n##  [1] \"b_Intercept\"                  \"b_age_c\"                      \"sd_runner__Intercept\"        \n##  [4] \"sd_runner__age_c\"             \"cor_runner__Intercept__age_c\" \"sigma\"                       \n##  [7] \"r_runner[1,Intercept]\"        \"r_runner[2,Intercept]\"        \"r_runner[3,Intercept]\"       \n## [10] \"r_runner[4,Intercept]\"        \"r_runner[5,Intercept]\"        \"r_runner[6,Intercept]\"       \n## [13] \"r_runner[7,Intercept]\"        \"r_runner[8,Intercept]\"        \"r_runner[9,Intercept]\"       \n## [16] \"r_runner[10,Intercept]\"       \"r_runner[11,Intercept]\"       \"r_runner[12,Intercept]\"      \n## [19] \"r_runner[13,Intercept]\"       \"r_runner[14,Intercept]\"       \"r_runner[15,Intercept]\"      \n## [22] \"r_runner[16,Intercept]\"       \"r_runner[17,Intercept]\"       \"r_runner[18,Intercept]\"      \n## [25] \"r_runner[19,Intercept]\"       \"r_runner[20,Intercept]\"       \"r_runner[21,Intercept]\"      \n## [28] \"r_runner[22,Intercept]\"       \"r_runner[23,Intercept]\"       \"r_runner[24,Intercept]\"      \n## [31] \"r_runner[25,Intercept]\"       \"r_runner[26,Intercept]\"       \"r_runner[27,Intercept]\"      \n## [34] \"r_runner[28,Intercept]\"       \"r_runner[29,Intercept]\"       \"r_runner[30,Intercept]\"      \n## [37] \"r_runner[31,Intercept]\"       \"r_runner[32,Intercept]\"       \"r_runner[33,Intercept]\"      \n## [40] \"r_runner[34,Intercept]\"       \"r_runner[35,Intercept]\"       \"r_runner[36,Intercept]\"      \n## [43] \"r_runner[1,age_c]\"            \"r_runner[2,age_c]\"            \"r_runner[3,age_c]\"           \n## [46] \"r_runner[4,age_c]\"            \"r_runner[5,age_c]\"            \"r_runner[6,age_c]\"           \n## [49] \"r_runner[7,age_c]\"            \"r_runner[8,age_c]\"            \"r_runner[9,age_c]\"           \n## [52] \"r_runner[10,age_c]\"           \"r_runner[11,age_c]\"           \"r_runner[12,age_c]\"          \n## [55] \"r_runner[13,age_c]\"           \"r_runner[14,age_c]\"           \"r_runner[15,age_c]\"          \n## [58] \"r_runner[16,age_c]\"           \"r_runner[17,age_c]\"           \"r_runner[18,age_c]\"          \n## [61] \"r_runner[19,age_c]\"           \"r_runner[20,age_c]\"           \"r_runner[21,age_c]\"          \n## [64] \"r_runner[22,age_c]\"           \"r_runner[23,age_c]\"           \"r_runner[24,age_c]\"          \n## [67] \"r_runner[25,age_c]\"           \"r_runner[26,age_c]\"           \"r_runner[27,age_c]\"          \n## [70] \"r_runner[28,age_c]\"           \"r_runner[29,age_c]\"           \"r_runner[30,age_c]\"          \n## [73] \"r_runner[31,age_c]\"           \"r_runner[32,age_c]\"           \"r_runner[33,age_c]\"          \n## [76] \"r_runner[34,age_c]\"           \"r_runner[35,age_c]\"           \"r_runner[36,age_c]\"          \n## [79] \"lprior\"                       \"lp__\"                         \"accept_stat__\"               \n## [82] \"treedepth__\"                  \"stepsize__\"                   \"divergent__\"                 \n## [85] \"n_leapfrog__\"                 \"energy__\"\n\nLike we did with the random intercepts model, we’ll look at all of these in turn.\n\nGlobal analysis\nFirst we’ll look at the relationship between running time and age for the typical runner, or just the global \\(\\beta_0\\) and the global \\(\\beta_1\\).\n\nbrmsrstanarm\n\n\n\nmodel_running2_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 2 × 6\n##   effect term        estimate std.error conf.low conf.high\n##   <chr>  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  (Intercept)    90.9      2.28     88.1      93.8 \n## 2 fixed  age_c           1.36     0.253     1.05      1.68\n\n\n\n\nmodel_running2_rstanarm |> \n  tidy(effects = c(\"fixed\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 2 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)    90.9      2.11     88.2      93.7 \n## 2 age_c           1.37     0.263     1.04      1.72\n\n\n\n\nInterpretation time:\n\n\\(\\beta_0\\) is 90.94 is the overall global posterior mean race time for all runners. There’s an 80% chance that it’s between 88.1 and 93.76 minutes.\n\\(\\beta_1\\) shows that a typical runner slows down by a posterior mean of 1.36 minutes each year. There’s an 80% chance that the effect is between 1.05 and 1.68 minutes, which is “significant” and substantial. It’s also really close to the 1.3-year effect we found with just random intercepts.\n\nLet’s plot it for fun. Again, this is for a typical runner and incorporates no group-level information at all (hence re_formula = NA))\n\nmodel_running2_brms |> \n  linpred_draws(running, ndraws = 200, re_formula = NA) |> \n  ggplot(aes(x = age, y = net)) +\n  stat_lineribbon(aes(y = .linpred), fill = clrs[4], color = clrs[4], alpha = 0.2) +\n  labs(x = \"Age\", y = \"Race time\")\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n## Unknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nRunner-specific analysis\nWe could hypothetically extract and visualize all the runner-specific offsets to the slopes and intercepts (with ranef()), but we’ll skip that here and instead look at the runner-specific lines. Each line here is constructed using a unique set of \\(\\beta_{0_j}\\) and \\(\\beta_{1_j}\\) terms for each runner, each based around the global \\(\\beta_0\\) and \\(\\beta_1\\) estimates. To help with the intuition, here are the lines some of the runners:\n\nmodel_running2_brms |> \n  spread_draws(b_Intercept, b_age_c, r_runner[runner, term]) |> \n  pivot_wider(names_from = \"term\", values_from = r_runner) |> \n  mutate(runner_intercept = b_Intercept + Intercept,\n         runner_age = b_age_c + age_c) |> \n  group_by(runner) |> \n  summarize(across(c(runner_intercept, runner_age), ~median(.)))\n## # A tibble: 36 × 3\n##    runner runner_intercept runner_age\n##     <int>            <dbl>      <dbl>\n##  1      1             76.7      0.584\n##  2      2             85.3      1.21 \n##  3      3             91.4      1.11 \n##  4      4            102.       1.56 \n##  5      5             78.3      1.20 \n##  6      6             75.2      0.892\n##  7      7            104.       1.64 \n##  8      8            104.       1.76 \n##  9      9             96.6      1.63 \n## 10     10            114.       2.21 \n## # … with 26 more rows\n\nWe can plot these to see them easier. In the book they plot these with geom_abline() to go across the whole range of the plot. We can also limit the lines to each runners’ observed ages:\n\nrunning2_all_runner_lines <- model_running2_brms |> \n  linpred_draws(expand_grid(runner = unique(running$runner),\n                            age_c = c(50 - unscaled$age_c$scaled_center,\n                                      61 - unscaled$age_c$scaled_center))) |> \n  group_by(runner, age_c) |> \n  median_qi(.linpred) |> \n  mutate(age = age_c + unscaled$age_c$scaled_center)\n\nrunning |> \n  add_linpred_draws(model_running2_brms) |> \n  summarize(.linpred = mean(.linpred)) |> \n  ggplot(aes(x = age, y = .linpred)) +\n  geom_line(data = running2_all_runner_lines, \n            aes(group = runner), color = \"grey70\", size = 0.25) +\n  geom_line(aes(y = .linpred, group = runner),\n            size = 0.75, color = clrs[6]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = \"Race time\")\n\n\n\n\n\n\nWithin- and between-runner variability\nFinally we can look at all the \\(\\sigma\\) terms. We have a bunch now:\n\n\\(\\sigma_y\\) (sd__Observation): the variability within runners\n\\(\\sigma_0\\) (sd__(Intercept) for the runner group): the variability between runners’ baseline averages, or the variability around the \\(b_{0_j}\\) offsets\n\\(\\sigma_1\\) (sd__age_c for the runner group): the variability between runners’ age effects, or the variability around the \\(b_{1_j}\\) offsets\n\n\nbrmsrstanarm\n\n\n\nmodel_running2_brms |> \n  tidy(effects = c(\"ran_pars\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 4 × 7\n##   effect   group    term                   estimate std.error conf.low conf.high\n##   <chr>    <chr>    <chr>                     <dbl>     <dbl>    <dbl>     <dbl>\n## 1 ran_pars runner   sd__(Intercept)          13.3       1.70   11.2       15.5  \n## 2 ran_pars runner   sd__age_c                 0.796     0.355   0.324      1.25 \n## 3 ran_pars runner   cor__(Intercept).age_c    0.474     0.303   0.0991     0.829\n## 4 ran_pars Residual sd__Observation           5.04      0.325   4.63       5.47\n\n\n\n\nmodel_running2_rstanarm |> \n  tidy(effects = c(\"ran_pars\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 4 × 3\n##   term                         group    estimate\n##   <chr>                        <chr>       <dbl>\n## 1 sd_(Intercept).runner        runner     12.9  \n## 2 sd_age_c.runner              runner      0.989\n## 3 cor_(Intercept).age_c.runner runner      0.447\n## 4 sd_Observation.Residual      Residual    5.03\n\n\n\n\n\n\\(\\sigma_y\\) (or sd__Observation) is 5.04, which means that within any runner, their race times vary by 5 minutes around their individual race time average.\n\\(\\sigma_0\\) (or sd__(Intercept) for the runner group) is 13.25, which means that average runner speeds vary or bounce around by 13 minutes across runners.\n\\(\\sigma_1\\) (or sd__age_c for the runner group) is 0.8, which means that average age effects vary by 0.8 minutes across runners.\n\nWe could try to decompose this variation by squaring these \\(\\sigma\\)s and calculating ratios, but with random slopes, the math for this gets too hard if we want to incorporate slope information or details from additional groups (if we had another grouping level). performance::icc() even yells at us about it:\n\nperformance::icc(model_running2_brms, by_group = TRUE)\n## Warning: Model contains random slopes. Cannot compute accurate ICCs by group factors.\n## # ICC by Group\n## \n## Group  |   ICC\n## --------------\n## runner | 0.859\n\nWe can look at the overall ICC instead, which tells us the percent of the total variation that comes from between-runner differences\n\nperformance::icc(model_running2_brms, by_group = FALSE)\n## # Intraclass Correlation Coefficient\n## \n##     Adjusted ICC: 0.878\n##   Unadjusted ICC: 0.838\n\nFinally, we also have an estimate for \\(\\rho\\) (cor__(Intercept).age_c for the runner group), or the correlation between the runner-specific slopes and intercepts. Here it’s 0.47, which seems large! And substantially different from what the book has (-0.09). That’s weird. I think it’s because I’m using the centered version of age here and they don’t, so the intercept term is on a different scale in my models? Also, I’m not too concerned—brms provides credible intervals for this \\(\\rho\\) term (rstanarm doesn’t seem to?), and there’s an 80% chance that the correlation between the runner-specific slopes and intercepts is between 0.1 and 0.83, which is a huge window.\nContrary to the book, then, it seems that runners that start slower slow down faster over time. For fun, we can look at the relationship between \\(\\rho\\) and \\(\\beta_{0_j}\\) for each runner. It’s positive—as the intercept increases, the correlation between the intercept and slope increases.\n\nrunning2_slope_int_cor <- model_running2_brms |> \n  spread_draws(b_Intercept, b_age_c, r_runner[runner, term]) |> \n  pivot_wider(names_from = \"term\", values_from = r_runner) |> \n  mutate(runner_intercept = b_Intercept + Intercept,\n         runner_age = b_age_c + age_c) |> \n  group_by(runner) |> \n  summarize(correlation = cor(runner_intercept, runner_age),\n            across(c(runner_intercept, runner_age), ~median(.)))\n\nrunning2_slope_int_cor |> \n  ggplot(aes(x = runner_intercept, y = correlation)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = clrs[1]) +\n  labs(x = \"β<sub>0<sub>j</sub></sub>\", y = \"ρ\") +\n  theme(axis.title.x = element_markdown())"
  },
  {
    "objectID": "bayes-rules/17-chapter.html#model-evaluation-selection",
    "href": "bayes-rules/17-chapter.html#model-evaluation-selection",
    "title": "17: Hierarchical models with predictors",
    "section": "17.4: Model evaluation & selection",
    "text": "17.4: Model evaluation & selection\nPhew we’ve done a ton of modeling work here, with a completely pooled model, a model with just random intercepts, and a model with random intercepts and random slopes. They all fit the data in different ways:\n\np1 <- running |> \n  add_linpred_draws(model_complete_pooling_brms, ndraws = 100) |> \n  summarize(.linpred = mean(.linpred)) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = runner),\n            size = 1, color = clrs[2]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = \"Race time\", \n       title = \"Completely pooled model\")\n\np2 <- running |> \n  add_linpred_draws(model_running1_brms, ndraws = 100) |> \n  summarize(.linpred = mean(.linpred)) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = runner),\n            size = 0.5, color = clrs[3]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = NULL, \n       title = \"Random intercepts only\")\n\np3 <- running |> \n  add_linpred_draws(model_running2_brms, ndraws = 100) |> \n  summarize(.linpred = mean(.linpred)) |> \n  ggplot(aes(x = age, y = net)) +\n  geom_line(aes(y = .linpred, group = runner),\n            size = 0.5, color = clrs[6]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = NULL, \n       title = \"Random intercepts and slopes\")\n\np1 | p2 | p3\n\n\n\n\nTo evaluate which of these is best, we need to ask our three questions:\n\n1: How fair is each model?\nThey’re all equally fair and good.\n\n\n2: How wrong is each model?\nFor this we can look at posterior predictive checks:\n\np1 <- pp_check(model_complete_pooling_brms, ndraws = 25, type = \"dens_overlay\") +\n  labs(x = \"Running time\", title = \"Completely pooled model\") +\n  guides(color = \"none\") +\n  coord_cartesian(xlim = c(45, 135))\n\np2 <- pp_check(model_running1_brms, ndraws = 25, type = \"dens_overlay\") +\n  labs(x = \"Running time\", title = \"Random intercepts only\") +\n  guides(color = \"none\") +\n  coord_cartesian(xlim = c(45, 135))\n\np3 <- pp_check(model_running2_brms, ndraws = 25, type = \"dens_overlay\") +\n  labs(x = \"Running time\", title = \"Random intercepts and slopes\") + \n  coord_cartesian(xlim = c(45, 135))\n\np1 | p2 | p3\n\n\n\n\nThe models with the random effects definitely fit the underlying data better, but it doesn’t look like there’s a huge difference between the random intercepts-only model and the model with random intercepts and slopes.\nWe also know from earlier that the models give different estimates for the age effect. In the pooled model it’s not “significant”; in the multilevel models it is.\n\n\n3: How accurate are each model’s predictions?\nIn Bayes Rules! they look at predictive accuracy a few different ways: mean average predictions, k-fold cross validation, and ELPD/LOO values. I’m going to skip those first two because they don’t play nicely with brms models and instead just look at ELPD. There’s no huge difference between the predictive densities. Based on that, the Bayes Rules! authors conclude by choosing the model with random intercepts only, mostly for the sake of parsimony and simplicity. Adding the random slopes doesn’t really help with the estimates or the accuracy (though it does make prettier pictures), so it’s best to work with a simpler version of the model with fewer moving parts to worry about.\n\nloo_stats <- tribble(\n  ~model_name, ~model,\n  \"Completely pooled model\", model_complete_pooling_brms,\n  \"Random intercepts only\", model_running1_brms,\n  \"Random intercepts and slopes\", model_running2_brms\n) |> \n  mutate(loo = map(model, ~loo(.))) |> \n  mutate(loo_stuff = map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |> \n  select(model_name, loo_stuff) |> \n  unnest(loo_stuff) |> \n  filter(statistic == \"elpd_loo\") |> \n  arrange(desc(Estimate))\nloo_stats\n## # A tibble: 3 × 4\n##   model_name                   statistic Estimate    SE\n##   <chr>                        <chr>        <dbl> <dbl>\n## 1 Random intercepts and slopes elpd_loo     -589. 18.0 \n## 2 Random intercepts only       elpd_loo     -590. 17.5 \n## 3 Completely pooled model      elpd_loo     -753.  8.69\n\n\nloo_stats |> \n  mutate(model_name = fct_rev(fct_inorder(model_name))) |> \n  ggplot(aes(x = Estimate, y = model_name)) +\n  geom_pointrange(aes(xmin = Estimate - 2 * SE, xmax = Estimate + 2 * SE)) +\n  labs(x = \"ELPD\", y = NULL)\n\n\n\n\n\nloo_compare(loo(model_complete_pooling_brms),\n            loo(model_running1_brms), \n            loo(model_running2_brms))\n##                             elpd_diff se_diff\n## model_running2_brms            0.0       0.0 \n## model_running1_brms           -0.5       3.1 \n## model_complete_pooling_brms -163.4      17.5"
  },
  {
    "objectID": "bayes-rules/17-chapter.html#posterior-prediction",
    "href": "bayes-rules/17-chapter.html#posterior-prediction",
    "title": "17: Hierarchical models with predictors",
    "section": "17.5: Posterior prediction",
    "text": "17.5: Posterior prediction\nLike we did in chapter 16, we can use the posteriors here to generate predictions, both for existing runners and for completely new runners. We’ll use posterior_predict() (technically its tidybayes wrapper predicted_draws()) with re_formula = NULL to incorporate the random effects, allow_new_levels to allow it to make predictions for Miles, and sample_new_levels = \"gaussian\" to draw the random offsets in slopes and intercepts from a normal distribution.\nNote how the predictions for Miles here are really wide. We don’t know much about him, so the predictions really just reflect a bunch of randomness around the global mean race time.\n\nmodel_running2_brms |> \n  predicted_draws(tibble(runner = c(\"1\", \"Miles\", \"10\"),\n                         age_c = 61 - unscaled$age_c$scaled_center),\n                  re_formula = NULL, allow_new_levels = TRUE,\n                  sample_new_levels = \"gaussian\") |> \n  ungroup() |> \n  mutate(runner = fct_inorder(runner)) |> \n  ggplot(aes(x = .prediction, y = runner, fill = runner)) +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[6])) +\n  stat_halfeye() +\n  labs(x = \"Predicted race time\", y = NULL, fill = \"Runner\")\n\n\n\n\nSince we modeled it as a random effect, we can also do some neat things with the age effect in the posterior predictions. Runner 1’s time doesn’t really change as they age; Runner 10 slows down substantially over time. Miles has a slight increase in running time over the years, since the effect is really just a bunch of randomness around the global \\(\\beta_1\\) effect.\n\nmodel_running2_brms |> \n  predicted_draws(expand_grid(runner = c(\"1\", \"Miles\", \"10\"),\n                              age_c = seq(50, 62, by = 2) - unscaled$age_c$scaled_center),\n                  re_formula = NULL, allow_new_levels = TRUE,\n                  sample_new_levels = \"gaussian\") |> \n  mutate(age = age_c + unscaled$age_c$scaled_center) |> \n  ungroup() |> \n  mutate(runner = fct_inorder(runner)) |> \n  ggplot(aes(x = .prediction, y = factor(age), fill = runner)) +\n  scale_fill_manual(values = c(clrs[1], clrs[3], clrs[6]), guide = \"none\") +\n  stat_slab(height = 2, slab_color = \"white\", slab_size = 0.25) +\n  labs(x = \"Predicted race time\", y = \"Age\", fill = \"Runner\") +\n  facet_wrap(vars(runner)) +\n  theme(panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "bayes-rules/17-chapter.html#spotify-and-dancibility",
    "href": "bayes-rules/17-chapter.html#spotify-and-dancibility",
    "title": "17: Hierarchical models with predictors",
    "section": "17.6: Spotify and dancibility",
    "text": "17.6: Spotify and dancibility\nHOLY COW this chapter has been long and difficult and I’m tired. In the book they do one more example with the Spotify data with two models:\n\ndanceability ~ valence + genre + (1 | artist)\ndanceability ~ valence + genre + (valence | artist)\n\nI’m not going to recreate their stuff with brms here (for now; maybe someday I’ll come back and do that). See that section for complete details."
  },
  {
    "objectID": "bayes-rules/18-chapter.html",
    "href": "bayes-rules/18-chapter.html",
    "title": "18: Non-normal hierarchical regression & classification",
    "section": "",
    "text": "(Original chapter)\nAll this multilevel modeling stuff also works with other distributional families. This chapter covers logistic regression and Poisson regression, but the same principles apply to any distribution supported by Stan. This chapter is a lot less detailed than chapters 15, 16, and 17, which spend a ton of time on the mechanics of multilevel models. Instead, this just shows how to add multiple levels to logit and Poisson models."
  },
  {
    "objectID": "bayes-rules/18-chapter.html#hierarchical-logistic-regression",
    "href": "bayes-rules/18-chapter.html#hierarchical-logistic-regression",
    "title": "18: Non-normal hierarchical regression & classification",
    "section": "18.1: Hierarchical logistic regression",
    "text": "18.1: Hierarchical logistic regression\n\nThe general setup\nWith the logistic regression example in this chapter, we want to model the probability of a successful Himalayan climbing expedition. The data we have includes details about the expedition team, the year and season of the climb, and details about the climbers themselves, like their age, their role in the expedition, and whether or not they used oxygen during the climb.\nMost climbers were unsuccessful and did not make it to the summit:\n\nclimbers |> \n  count(success) |> \n  mutate(prop = n / sum(n))\n## # A tibble: 2 × 3\n##   success     n  prop\n##   <lgl>   <int> <dbl>\n## 1 FALSE    1269 0.611\n## 2 TRUE      807 0.389\n\nThese climbers never climb on their own. They always go in groups ranging from 5 to 44 people, and teams do not always unanimously finish together:\n\nclimbers_expeditions <- climbers |> \n  group_by(expedition_id) |> \n  summarize(n = n(), prop_success = mean(success))\n\np1 <- climbers_expeditions |> \n  ggplot(aes(x = n, y = prop_success)) +\n  geom_count(alpha = 0.8, color = clrs[5]) +\n  scale_size_area(max_size = 8, breaks = c(1, 5, 10)) +\n  labs(x = \"Expedition team size\", \n       y = \"Proportion of expedition team that finished\",\n       size = \"Number of\\nexpeditions\")\n\np2 <- climbers_expeditions |> \n  ggplot(aes(x = prop_success)) +\n  geom_histogram(binwidth = 0.05, center = 0, color = \"white\", fill = clrs[5]) +\n  labs(x = \"Proportion of expedition team that finished\", y = \"Count\")\n\np1 + p2\n\n\n\n\nWe thus have a hierarchical/multilevel structure that looks like this, with climbers \\(y_i\\) nested in expedition teams \\(j\\):\n\n\n\n\n\nMultilevel structure of climbers within expeditions\n\n\n\n\n\n\nModel building\nIn this example, we want to model whether an individual climber successfully reaches the summit. This is a binary outcome: 1 if yes, 0 if no. It’s also an individual outcome, nested in groups, so it has subscripts:\n\\[\n\\text{Success}_{i_j} = \\begin{cases}\n1 & \\text{Yes} \\\\\n0 & \\text{No} \\\\\n\\end{cases}\n\\]\nIn the book they use two covariates to explain climbing success:\n\n\\(X_{i_{j}1}\\) or \\(\\text{Age}_{i_j}\\): Age of climber \\(i\\) in expedition \\(j\\)\n\\(X_{i_{j}2}\\) or \\(\\text{Oxygen}_{i_j}\\): Whether climber \\(i\\) in expedition \\(j\\) used oxygen\n\n…so we’ll do that here too. At first glance, it doesn’t look like age makes much of a difference in the probability of success, but oxygen use matters a lot. Almost all of the successful climbers used oxygen; only a handful of oxygen-users were unsuccessful:\n\nclimbers |> \n  ggplot(aes(x = age, y = success, color = oxygen_used)) +\n  geom_dots(aes(side = ifelse(success == TRUE, \"bottom\", \"top\")), \n            pch = 19, scale = 0.6) +\n  scale_y_discrete(expand = expansion(mult = 0.1)) +\n  scale_color_manual(values = c(clrs[3], clrs[1])) +\n  labs(x = \"Age\", y = \"Success\", color = \"Oxygen used\")\n\n\n\n\nInstead of building up this model step-by-step like in previous chapters, we’ll just define it all at once here:\n\n\\[\n\\begin{aligned}\n\\text{Success}_{i_j} &\\sim \\operatorname{Bernoulli}(\\pi_{i_j}) & \\text{Probability of success for climber}_i \\text{ in team}_j \\\\\n\\operatorname{logit}(\\pi_{i_j}) &= \\beta_{0_j} + \\beta_1\\, \\text{Age}_{i_j} + \\beta_2\\, \\text{Oxygen}_{i_j} & \\text{Model for probability} \\\\\n\\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0) & \\text{Team-specific intercepts, or baseline probabilities} \\\\\n\\\\\n\\beta_{0_c} &\\sim \\mathcal{N}(0, 2.5) & \\text{Prior for global average success rate} \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) & \\text{Prior for global age effect, holding oxygen constant} \\\\\n\\beta_2 &\\sim \\mathcal{N}(0, 2.5) & \\text{Prior for global oxygen effect, holding age constant} \\\\\n\\sigma_0 &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-team variability}\n\\end{aligned}\n\\]\n\n \nOr with offset-based notation:\n\n\\[\n\\begin{aligned}\n\\text{Success}_{i_j} &\\sim \\operatorname{Bernoulli}(\\pi_{i_j}) & \\text{Probability of success for climber}_i \\text{ in team}_j \\\\\n\\operatorname{logit}(\\pi_{i_j}) &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{Age}_{i_j} + \\beta_2\\, \\text{Oxygen}_{i_j} & \\text{Model for probability} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Team-specific offsets from global success rate} \\\\\n\\\\\n\\beta_{0_c} &\\sim \\mathcal{N}(0, 2.5) & \\text{Prior for global average success rate} \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) & \\text{Prior for global age effect, holding oxygen constant} \\\\\n\\beta_2 &\\sim \\mathcal{N}(0, 2.5) & \\text{Prior for global oxygen effect, holding age constant} \\\\\n\\sigma_0 &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-team variability}\n\\end{aligned}\n\\]\n\n\n\nPosterior simulation and analysis\nWe can run this model by including (1 | expedition_id) for the random team intercepts:\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(0, 2.5), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(1), class = sd))\n\nmodel_success_brms <- brm(\n  bf(success ~ age_c + oxygen_used + (1 | expedition_id)), \n  data = climbers,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"18-manual_cache/success-brms\"\n)\n\n\n\nWe’ll keep the autoscaled priors here because rstan complains about init issues otherwise?\n\nmodel_success_rstanarm <- stan_glmer(\n  success ~ age_c + oxygen_used + (1 | expedition_id), \n  data = climbers, family = binomial,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, cores = 4, iter = 5000*2, seed = 84735, refresh = 0)\n\n\n\n\n\nGlobal analysis\nFirst we’ll look at what this model says about a typical climber, which involves the \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) terms:\n\nbrmsrstanarm\n\n\n\nmodel_success_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 3 × 6\n##   effect term            estimate std.error conf.low conf.high\n##   <chr>  <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  (Intercept)      -3.09     0.349    -3.54     -2.65  \n## 2 fixed  age_c            -0.0473   0.00913  -0.0589   -0.0358\n## 3 fixed  oxygen_usedTRUE   5.65     0.457     5.08      6.25\n\nOdds ratio-scale coefficients:\n\n# There's a tiny bug in broom.mixed when exponentiating, so we'll use\n# parameters::model_parameters() instead\nmodel_success_brms %>%\n  model_parameters(centrality = \"mean\", dispersion = TRUE,\n                   test = FALSE, verbose = FALSE, ci_method = \"hdi\", ci = 0.8,\n                   exponentiate = TRUE)\n## Parameter       |   Mean |       SD |           80% CI |  Rhat |     ESS\n## ------------------------------------------------------------------------\n## (Intercept)     |   0.05 |     0.35 | [  0.03,   0.07] | 1.003 | 1359.00\n## age_c           |   0.95 | 9.13e-03 | [  0.94,   0.96] | 1.000 | 7270.00\n## oxygen_usedTRUE | 284.45 |     0.46 | [157.94, 500.89] | 1.002 | 2508.00\n\n\nmodel_success_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  ungroup() |> \n  mutate(.value = exp(.value),\n         .variable = fct_inorder(.variable)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[6]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n\n\n\n\n\n\n\nmodel_success_rstanarm |> \n  tidy(effects = c(\"fixed\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 3 × 5\n##   term            estimate std.error conf.low conf.high\n##   <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)      -3.16     0.355    -3.63     -2.72  \n## 2 age_c            -0.0475   0.00922  -0.0593   -0.0358\n## 3 oxygen_usedTRUE   5.79     0.474     5.20      6.43\n\n\n# There's a tiny bug in broom.mixed when exponentiating, so we'll use\n# parameters::model_parameters() instead\nmodel_success_rstanarm %>%\n  model_parameters(centrality = \"mean\", dispersion = TRUE, prior = FALSE,\n                   test = FALSE, verbose = FALSE, ci_method = \"hdi\", ci = 0.8,\n                   exponentiate = TRUE)\n## Parameter       |   Mean |       SD |           80% CI |  Rhat |      ESS\n## -------------------------------------------------------------------------\n## (Intercept)     |   0.04 |     0.36 | [  0.03,   0.07] | 1.001 |  3980.00\n## age_c           |   0.95 | 9.21e-03 | [  0.94,   0.96] | 1.000 | 26332.00\n## oxygen_usedTRUE | 332.40 |     0.48 | [180.00, 608.64] | 1.001 |  8012.00\n\n\nmodel_success_rstanarm |> \n  gather_draws(`(Intercept)`, age_c, oxygen_usedTRUE) |>\n  ungroup() |> \n  mutate(.value = exp(.value),\n         .variable = fct_inorder(.variable)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = c(clrs[5], clrs[4], clrs[6]), guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n\n\n\n\n\n\n\nHere’s how to interpret all these coefficients, using the brms results:\n\nIntercept (\\(\\beta_0\\))\nThe posterior mean for \\(\\beta_0\\) is -3.09, but that’s weird and on the logit scale. We can convert this to the probability scale with \\(\\frac{e^{-3.09}}{1 + e^{-3.09}}\\) or with plogis(), which is 0.043. This means that the posterior mean probability of success for a climber of average age (36.96 years) and when not using oxygen is 4.3%, and there’s an 80% chance it’s between 2.8% and 6.6%.\n\n\nAge (\\(\\beta_1\\))\nThe posterior mean for \\(\\beta_1\\) is -0.047, but that’s also weird and logit-ed. We can exponentiate it (\\(e^{-0.047}\\)) and get a mean posterior odds ratio of 0.954, with an 80% credible interval of 0.943–0.965). A one-year increase in a typical climber’s age makes climbing success 4.6% less likely (with an 80% credible interval of 3.5%–5.7%).\nWe can also convert these results to a much more manageable probability scale, but that’s a little trickier now that we’re working with multiple covariates. We can’t just do this, since we also have a b2 to deal with:\n\nplogis(b0 + b1) - plogis(b0)\n\nInstead, we can use the magical marginaleffects package to get probability-scale estimates. First we’ll plot the predicted probabilities of success as age increases, holding oxygen use constant (as a no). Note that we include re_formula = NA to omit any random effects—this only uses the global parameters:\n\n# Fast automatic version:\n# plot_cap(model_success_brms, condition = \"age_c\", re_formula = NA)\n\ndatagrid(model = model_success_brms,\n         age_c = seq(17, 76, by = 1) - unscaled_climbers$age_c$scaled_center) |> \n  add_epred_draws(model_success_brms, re_formula = NA) |> \n  mutate(age = age_c + unscaled_climbers$age_c$scaled_center) |> \n  ggplot(aes(x = age, y = .epred)) +\n  stat_lineribbon(color = clrs[3], fill = clrs[3], alpha = 0.35) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Age\", y = \"Posterior probability of success\")\n\n\n\n\nSuccessful expeditions are most likely among younger climbers, and as a typical climber ages, the probability of success decreases. We can get exact estimates of the slope of this posterior with marginaleffects().\n\nmfx_success <- model_success_brms |>\n  marginaleffects(newdata = datagrid(age_c = seq(17, 76, by = 1) -\n                                       unscaled_climbers$age_c$scaled_center),\n                  variables = \"age_c\",\n                  type = \"response\", re_formula = NA) |>\n  posteriordraws() |>\n  mutate(age = age_c + unscaled_climbers$age_c$scaled_center)\n\nmfx_success |> \n  ggplot(aes(x = age, y = draw * 100)) +\n  stat_lineribbon(alpha = 0.25, fill = clrs[6], color = clrs[6]) +\n  labs(x = \"Age\", \n       y = \"Percentage point change in\\nprobability of climbing success\")\n\n\n\n\nWe can pick out the slope at a few of these different ages:\n\nmfx_success |> \n  filter(age %in% c(18, 40, 60)) |> \n  group_by(age) |> \n  summarize(mean_slope = mean(draw),\n            conf.low = mean(conf.low),\n            conf.high = mean(conf.high)) |> \n  mutate(across(c(mean_slope, conf.low, conf.high), ~ . * 100))\n## # A tibble: 3 × 4\n##     age mean_slope conf.low conf.high\n##   <dbl>      <dbl>    <dbl>     <dbl>\n## 1    18    -0.449    -0.884   -0.166 \n## 2    40    -0.179    -0.324   -0.0784\n## 3    60    -0.0723   -0.128   -0.0336\n\nFor a typical young climber (18 years old), a one-year increase in age is associated with a posterior mean -0.449 percentage point decrease in the probability of success. That’s a sizable drop! For a typical middle-aged climber (40 years old), getting older is associated with a smaller -0.179 percentage point drop in probability, while older climbers (60 years old) see an even smaller -0.072 percentage point decline. Age thus seems to matter the most for younger climbers—it doesn’t have an effect on the typical older climber.\n\n\nOxygen (\\(\\beta_2\\))\nThe posterior mean for \\(\\beta_2\\) is 5.651 in the log odds world. Exponentiating it (\\(e^{5.651}\\)) gives us a massive mean posterior odds ratio of 284.446, with an 80% credible interval of 160.629–515.843). Holding age constant, oxygen use in a typical climber increases the odds of success by 161–516 times! Wild!\nLet’s translate this to probabilities. Holy crap there’s a massive difference the probability of success:\n\ndatagrid(model = model_success_brms,\n         oxygen_used = c(TRUE, FALSE)) |> \n  add_epred_draws(model_success_brms, re_formula = NA) |> \n  ggplot(aes(x = oxygen_used, y = .epred, fill = oxygen_used, color = oxygen_used)) +\n  stat_gradientinterval(width = 0.25) +\n  scale_y_continuous(labels = label_percent()) +\n  scale_fill_manual(values = c(clrs[3], clrs[1]), guide = \"none\") +\n  scale_color_manual(values = colorspace::darken(c(clrs[3], clrs[1]), 0.5), guide = \"none\") +\n  labs(x = \"Oxygen used\", y = \"Posterior probability of success\")\n\n\n\n\nTo calculate the exact difference we could just use the results from add_epred_draws() and do some dplyr group_by() and summarize() and subtraction work, or we can use tidybayes’s compare_levels() to do the same thing. This is a marginal effect at user-specified values.\n\ndatagrid(model = model_success_brms,\n         oxygen_used = c(TRUE, FALSE)) |> \n  add_epred_draws(model_success_brms, re_formula = NA) |> \n  compare_levels(variable = .epred, by = oxygen_used) |> \n  ggplot(aes(x = .epred * 100)) +\n  stat_halfeye(fill = clrs[4]) +\n  labs(x = \"Difference in probability of success due to oxygen use\\n(Percentage points)\",\n       y = \"Density\")\n\n\n\n\nAlternatively, we can use marginaleffects::comparisons() to calculate actual average marginal effects (or rather, group contrasts, since strictly speaking, marginal effects are partial derivatives). Instead of feeding the model a single average value for age, we’ll plug in each original row of the data into the model and get one contrast per row.\n\nmfx_cmp_success <- model_success_brms |> \n  comparisons(variables = \"oxygen_used\", re_formula = NA)\n\n\nnrow(mfx_cmp_success)\n## [1] 2076\nnrow(climbers)\n## [1] 2076\n\n\nmfx_cmp_success |> \n  posteriordraws() |> \n  group_by(drawid) |> \n  summarize(draw = mean(draw)) |> \n  ggplot(aes(x = draw * 100)) +\n  stat_halfeye(fill = clrs[2]) +\n  labs(x = \"Difference in probability of success due to oxygen use\\n(Percentage points)\",\n       y = \"Density\")\n\n\n\n\nFor even more bonus fun, we can actually do something new with marginaleffects and integrate out the random effects (see this too for an example). So far, we’ve been using re_formula = NA to ignore the expedition team effects entirely. If we use re_formula = NULL to include them, we either have to (1) specify one specific expedition team ID, or (2) invent a new hypothetical team that’s based on the the global average. Alternatively, we can create a bunch of new hypothetical teams (like 100) and calculate either the marginal effects or contrasts for each coefficient in those hypothetical teams, then take the average (see this vignette for brmsmargins for more details). Fortunately marginalffects can handle all this for us:\n\nmfx_success_integrated_out <- comparisons(\n  model_success_brms,\n  # 100 fake expedition IDs\n  newdata = datagrid(expedition_id = -1:-100),\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\")\n\n\nmfx_success_integrated_out |> summary()\n##          Term     Contrast    Effect    2.5 %  97.5 %\n## 1       age_c  (x + 1) - x -0.004151 -0.09081 0.08254\n## 2 oxygen_used TRUE - FALSE  0.520871  0.39713 0.62711\n## \n## Model type:  brmsfit \n## Prediction type:  response\n\n\nmfx_success_integrated_out |> \n  posteriordraws() |> \n  group_by(drawid, term) |> \n  summarize(draw = mean(draw)) |> \n  mutate(term = recode(term, age_c = \"Age\", oxygen_used = \"Oxygen used\")) |> \n  ggplot(aes(x = draw * 100, fill = term)) +\n  stat_halfeye() +\n  facet_wrap(vars(term), scales = \"free_x\") +\n  scale_fill_manual(values = c(clrs[1], clrs[5]), guide = \"none\") +\n  labs(title = \"Average marginal effect of coefficients\",\n       subtitle = \"Random effects integrated out\",\n       x = \"Percentage point change in probability of success\", y = \"Density\")\n\n\n\n\n\n\nAge and oxygen at the same time\nFor fun, here’s what the effect of both age and oxygen look like simultaneously:\n\ndatagrid(model = model_success_brms,\n         oxygen_used = c(TRUE, FALSE),\n         age_c = seq(17, 76, by = 1) - unscaled_climbers$age_c$scaled_center) |> \n  add_epred_draws(model_success_brms, re_formula = NA, ndraws = 100) |> \n  mutate(age = age_c + unscaled_climbers$age_c$scaled_center) |> \n  ggplot(aes(x = age, y = .epred, color = oxygen_used)) +\n  geom_line(aes(group = paste(oxygen_used, .draw)), alpha = 0.35) +\n  scale_y_continuous(labels = label_percent()) +\n  scale_color_manual(values = c(clrs[3], clrs[1])) +\n  labs(x = \"Age\", y = \"Posterior probability of success\", \n       color = \"Oxygen used\")\n\n\n\n\n\n\n\nTeam-specific analysis\nNext we can look at the team-specific details, or the offsets from the baseline probability of success:\n\\[\nB_0 + b_{0_j}\n\\]\nThere’s a surprising amount of variation in success across teams here. Some have a predicted 97% baseline chance of success; others have practically no baseline chance of success. Here are the top 5 and bottom 5 as an example:\n\nteam_baselines <- model_success_brms |> \n  epred_draws(tibble(expedition_id = unique(climbers$expedition_id),\n                     age_c = 0, oxygen_used = FALSE)) |> \n  ungroup() |> \n  mutate(expedition_id = fct_reorder(factor(expedition_id), .epred, .fun = mean))\n\nteam_baselines |> \n  group_by(expedition_id) |> \n  summarize(avg = mean(.epred)) |> \n  arrange(desc(avg)) |> \n  slice(1:5, 196:200)\n## # A tibble: 10 × 2\n##    expedition_id      avg\n##    <fct>            <dbl>\n##  1 TUKU16301     0.965   \n##  2 SPHN93101     0.950   \n##  3 MANA84101     0.942   \n##  4 AMAD11321     0.931   \n##  5 AMAD98311     0.926   \n##  6 MANA82401     0.000651\n##  7 MAKA08112     0.000606\n##  8 EVER07194     0.000402\n##  9 MANA08324     0.000365\n## 10 EVER19112     0.000112\n\nAnd for bonus fun, here are all 200 teams simultaneously:\n\nglobal_baseline <- model_success_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  filter(term == \"(Intercept)\") |> \n  mutate(across(c(estimate, conf.low, conf.high), plogis))\n\nteam_baselines |> \n  ggplot(aes(x = .epred, y = expedition_id)) +\n  annotate(geom = \"rect\", ymin = -Inf, ymax = Inf,\n           xmin = global_baseline$conf.low, xmax = global_baseline$conf.high,\n           fill = clrs[2], alpha = 0.4) +\n  geom_vline(xintercept = global_baseline$estimate, color = clrs[2]) +\n  stat_pointinterval(color = clrs[3], size = 0.1,\n                     point_interval = \"mean_qi\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Baseline probability of success\", y = \"Expedition team ID\",\n       title = \"Baseline probability of expedition team success\",\n       subtitle = \"Global baseline and 80% credible interval shown in yellow\") +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\nWe can also look at how these team-specific offsets influence the age-success relationship. Here we’ll plot three arbitrarily chosen teams (I just scrolled through the list and picked at random). All the model does here is shift the baseline intercept around—the slope is the same in all three panels (but looks difference because of logit slopes are nonlinear and weird).\n\nclimbers_small <- climbers |> \n  filter(expedition_id %in% c(\"AMAD81101\", \"AMAD03107\", \"EVER07194\"))\n\nclimbers_small |> \n  add_epred_draws(model_success_brms, ndraws = 250) |> \n  ggplot(aes(x = age, y = as.numeric(success))) +\n  geom_line(aes(y = .epred, group = paste(expedition_id, .draw), \n                color = expedition_id),\n            alpha = 0.2, size = 0.3) +\n  geom_point(data = climbers_small, aes(color = expedition_id)) +\n  scale_color_manual(values = c(clrs[1], clrs[2], clrs[5]), guide = \"none\") +\n  facet_wrap(vars(expedition_id)) +\n  labs(x = \"Age\", y = \"Posterior probability of success\")\n\n\n\n\n\n\nBetween-team variability\nWith linear regression in chapter 17, we had two forms of variability:\n\n\\(\\sigma_y\\) for the variability within units nested in groups (i.e. the scale term in \\(Y_{i_j} \\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_{i_j})\\))\n\\(\\sigma_0\\) for the variability between groups (i.e. the variation around the random offsets in \\(b_{0_j} \\sim \\mathcal{N}(0, \\sigma_0)\\))\n\nIn logistic regression with a Bernoulli model for \\(Y\\), we don’t have a corresponding \\(\\sigma_y\\) term for variability within expedition teams. We do have a \\(\\sigma_0\\) term for variability between teams, so we can look at that:\n\nbrmsrstanarm\n\n\n\nmodel_success_brms |> \n  tidy(effects = c(\"ran_pars\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 1 × 7\n##   effect   group         term            estimate std.error conf.low conf.high\n##   <chr>    <chr>         <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 ran_pars expedition_id sd__(Intercept)     3.57     0.337     3.16      4.02\n\n\n\n\nmodel_success_rstanarm |> \n  tidy(effects = c(\"ran_pars\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 1 × 3\n##   term                         group         estimate\n##   <chr>                        <chr>            <dbl>\n## 1 sd_(Intercept).expedition_id expedition_id     3.63\n\n\n\n\n\\(\\sigma_0\\) (or sd__(Intercept) for the expedition_id group) is 3.57, which is the amount of variance in the log odds of the global intercept, so the average baseline log odds of success varies between teams with a standard deviation of 3.57 logits/log odds. We can see this if we look at all the team-specific intercepts on the log odds scale. The global average is -3.09, marked in yellow, and there’s a ton of variation around that average—a standard deviation of 3.57.\nAccording to Isabella Ghement, the “sd(Intercept) term is a quantification of sorts of how ‘similar’ the intercepts for different [teams] … are”. If it’s a big value, the teams aren’t super similar; if it’s small, the teams are pretty similar. These teams are not similar at all.\n\nteam_baselines_logit <- model_success_brms |> \n  linpred_draws(tibble(expedition_id = unique(climbers$expedition_id),\n                     age_c = 0, oxygen_used = FALSE)) |> \n  ungroup() |> \n  mutate(expedition_id = fct_reorder(factor(expedition_id), .linpred, .fun = mean))\n\nglobal_baseline_logit <- model_success_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  filter(term == \"(Intercept)\")\n\nteam_baselines_logit |> \n  ggplot(aes(x = .linpred, y = expedition_id)) +\n  annotate(geom = \"rect\", ymin = -Inf, ymax = Inf,\n           xmin = global_baseline_logit$conf.low, xmax = global_baseline_logit$conf.high,\n           fill = clrs[2], alpha = 0.4) +\n  geom_vline(xintercept = global_baseline_logit$estimate, color = clrs[2]) +\n  stat_pointinterval(color = clrs[6], size = 0.1,\n                     point_interval = \"mean_qi\") +\n  labs(x = \"Baseline log odds\", y = \"Expedition team ID\") +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\nTo make this more interpretable, according to Isabella Ghement (and others in that super helpful Twitter thread), we can unlogit this with plogis(Intercept ± 2*sd(Intercept)). This gives us a massive range in probability land!\n\nmodel_success_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\")) |> \n  filter(str_detect(term, \"Intercept\")) |> \n  select(term, estimate) |> \n  mutate(term = janitor::make_clean_names(term)) |> \n  pivot_wider(names_from = term, values_from = estimate) |> \n  mutate(range_low = plogis(intercept - (2 * sd_intercept)),\n         range_high = plogis(intercept + (2 * sd_intercept))) |> \n  mutate(across(starts_with(\"range\"), ~label_percent(accuracy = 0.01)(.)))\n## # A tibble: 1 × 4\n##   intercept sd_intercept range_low range_high\n##       <dbl>        <dbl> <chr>     <chr>     \n## 1     -3.09         3.57 0.00%     98.29%\n\nTo see this better, we can look back look back at the plot with all 200 team baseline probabilities. There’s a ton of variation away from that thin yellow baseline average probability. We’ve got sizable variation here.\nBut also, in the words of TJ Mahr, “don’t overthink it.” :)\nWe can get an ICC here like we did with Gaussian regression, but I’m not entirely sure what it means. In Gaussian land, this is the proportion of variability that is attributable to between-team differences. Here, we don’t have within-team differences, so I’m not sure what goes in the denominator of the ratio. But it feels like the ICC we used earlier, so it probably means something like 80ish% of the variability in success rates comes from between-team differences? I guess?\n\nperformance::icc(model_success_brms, by_group = TRUE)\n## # ICC by Group\n## \n## Group         |   ICC\n## ---------------------\n## expedition_id | 0.795\n\n\n\n\nPosterior classification\nTo see how this model predict/classifies new climbers, we can create a set of simulated climbers with different combinations of ages and oxygen use. As expected, the climber with the highest probability of success is the young one planning on using oxygen; the climber with the lowest probability is the old one sans oxygen.\n\nbinary_prediction <- model_success_brms |> \n  predicted_draws(expand_grid(age_c = c(20, 60) - unscaled_climbers$age_c$scaled_center,\n                              oxygen_used = c(FALSE, TRUE), expedition_id = \"New\"),\n                  allow_new_levels = TRUE)\n\nbinary_prediction |> \n  mean_qi(.prediction) |> \n  mutate(age = age_c + unscaled_climbers$age_c$scaled_center,\n         age = glue::glue(\"{age} years old\")) |> \n  ggplot(aes(x = factor(.row), y = .prediction, fill = oxygen_used)) +\n  geom_col() +\n  scale_y_continuous(labels = label_percent()) +\n  scale_fill_manual(values = c(clrs[3], clrs[1])) +\n  facet_wrap(vars(age), scales = \"free_x\") +\n  labs(x = \"Simulated climber\", y = \"Posterior predicted probability of success\",\n       fill = \"Oxygen used\")\n\n\n\n\n\n\nModel evaluation\n\nHow fair is the model?\nGreat.\n\nThe data we used are part of public record and we do not foresee our analysis having any negative impact on individuals or society. (Again, boring answers to the question of fairness are the best kind.)\n\n\n\nHow wrong is the model?\nWe can use pp_check() to compare the posterior predictions to the actual data, both with bars:\n\nbrmsrstanarm\n\n\n\npp_check(model_success_brms, ndraws = 100, type = \"bars\")\n\n\n\n\n\n\n\npp_check(model_success_rstanarm, n = 100, plotfun = \"bars\")\n\n\n\n\n\n\n\n…and with a histogram that shows the proportion of successful climbers in each simulated dataset compared with the observed probability of success. Most simulated posterior datasets saw successful climbs ≈42ish% of the the time, with some as low as 36% and some as high as 42%. That seems good and reasonable.\n\nbrmsrstanarm\n\n\n\npp_check(model_success_brms, type = \"stat\") +\n  labs(x = \"Probability of success\")\n\n\n\n\n\n\n\npp_check(model_success_rstanarm, plotfun = \"stat\") +\n  labs(x = \"Probability of success\")\n\n\n\n\n\n\n\n\n\nHow accurate are the model’s posterior classifications?\nTo check the accuracy of our predictions, we can find the overall classification accuracy rate, the true negative rate (specificity), and the true positive rate (sensitivity rate)\n\n\n\n\n\\(\\hat{Y} = 0\\)\n\\(\\hat{Y} = 1\\)\n\n\n\n\n\\(Y = 0\\)\na\nb\n\n\n\\(Y = 1\\)\nc\nd\n\n\n\n\nOverall classification accuracy rate = \\(\\frac{a + d}{a + b + c + d}\\)\nTrue negative rate, or specificity = \\(\\frac{a}{a + b}\\)\nTrue positive rate, or sensitivity = \\(\\frac{c}{c + d}\\)\n\nLet’s make predictions with the existing data and classify them using a 50% cutoff—if the probability is above 50%, we’ll call it successful.\n\nsuccess_preds <- model_success_brms |> \n  predicted_draws(climbers)\n\nsuccess_classifications <- success_preds |> \n  group_by(.row) |> \n  summarize(success_prob = mean(.prediction),\n            success_actual = success[1]) |> \n  mutate(success_pred = success_prob >= 0.5)\n\nsuccess_classifications |> \n  count(success_actual, success_pred) |> \n  pivot_wider(names_from = \"success_pred\", values_from = \"n\")\n## # A tibble: 2 × 3\n##   success_actual `FALSE` `TRUE`\n##   <lgl>            <int>  <int>\n## 1 FALSE             1173     96\n## 2 TRUE                78    729\n\nsuccess_classifications |> \n  janitor::tabyl(success_actual, success_pred) |> \n  janitor::adorn_totals(c(\"row\", \"col\"))\n##  success_actual FALSE TRUE Total\n##           FALSE  1173   96  1269\n##            TRUE    78  729   807\n##           Total  1251  825  2076\n\nBased on these numbers, we have these accuracy rates:\n\nOverall accuracy (\\(\\frac{a + d}{a + b + c + d}\\)): 91.6%\nSpecificity, or true negative rate (\\(\\frac{a}{a + b}\\)): 92.4%\nSensitivity, or true positive rate (\\(\\frac{c}{c + d}\\)): 90.3%\n\nWow, this is a fantastic model. Use oxygen!\nThe model successfully predicts the outcomes for 91.6% of climbers. Because the consequences of failure are so high (injury and death), we should prioritize specificity here. We accurately predicted 92.4% of failed expeditions, which might not be great.\nWe can boost that specificity if we increase the probability cutoff and make it harder to predict success. In the book they settle on 0.65 (so a predicted probability of 65% is necessary to be considered a success). This lowers the sensitivity to 80ish%, lowers the overall accuracy to 90ish%, but boosts the specificity to 95ish%.\nAnd that’s it! Complete analysis of a multilevel logistic regression model!"
  },
  {
    "objectID": "bayes-rules/18-chapter.html#hierarchical-poisson-and-negative-binomial",
    "href": "bayes-rules/18-chapter.html#hierarchical-poisson-and-negative-binomial",
    "title": "18: Non-normal hierarchical regression & classification",
    "section": "18.2: Hierarchical Poisson and negative binomial",
    "text": "18.2: Hierarchical Poisson and negative binomial\n\nThe general setup\nIn this example, we want to model the number of reviews an AirBnB listing gets (since review count is probably a good proxy for guests). The data we have comes from Chicago and includes listings in 43 different neighborhoods:\n\nnrow(airbnb)\n## [1] 1561\n\nairbnb |> \n  distinct(neighborhood) |> \n  nrow()\n## [1] 43\n\nWe have listings \\(y_i\\) nested in neighborhoods \\(j\\), so we have a standard multilevel structure:\n\n\n\n\n\nMultilevel structure of AirBnB listings within neighborhoods\n\n\n\n\n\n\nModel building\nWe want to model the count of reviews on a listing, which is a count, and because it’s nested in neighborhoods it has subscripts:\n\\[\n\\text{Number of reviews}_{i_j}\n\\]\nIn the book they use two variables to explain the count of reviews, but one is categorical (room type, with three levels: private unit, private room, and shared room) and thus gets two parameters:\n\n\\(X_{i_{j}1}\\) or \\(\\text{Rating}_{i_j}\\): Visitor rating of listing \\(i\\) in neighborhood \\(j\\)\n\\(X_{i_{j}2}\\) or \\(\\text{Private room}_{i_j}\\): Binary indicator for if listing \\(i\\) in neighborhood \\(j\\) is a private room\n\\(X_{i_{j}3}\\) or \\(\\text{Shared room}_{i_j}\\): Binary indicator for if listing \\(i\\) in neighborhood \\(j\\) is a shared room\n\nSome quick exploratory data analysis:\n\np1 <- airbnb |> \n  ggplot(aes(x = reviews)) +\n  geom_histogram(binwidth = 10, color = \"white\", boundary = 0, fill = clrs[3]) +\n  labs(x = \"Number of reviews\", y = \"Count\")\n\np2 <- airbnb |> \n  ggplot(aes(x = rating, y = reviews)) +\n  geom_jitter(color = clrs[1], alpha = 0.4, size = 1) +\n  labs(x = \"Rating\", y = \"Number of reviews\")\n\np3 <- airbnb |> \n  ggplot(aes(x = room_type, y = reviews, \n             color = room_type, fill = room_type)) +\n  geom_half_point(side = \"l\", size = 1, alpha = 0.4) +\n  geom_half_violin(side = \"r\") +\n  scale_color_manual(values = c(clrs[2], clrs[6], clrs[5]), guide = \"none\") +\n  scale_fill_manual(values = c(clrs[2], clrs[6], clrs[5]), guide = \"none\") +\n  scale_x_discrete(labels = label_wrap(10)) +\n  labs(x = \"Room type\", y = \"Number of reviews\")\n\np1 | p2 | p3\n\n\n\n\nThe count of reviews looks Poisson-y, since it’s a right-skewed count. Poisson distributions have the magical property that the mean and variance of \\(Y\\) are the same. We should check if that’s the case here:\n\nairbnb |> \n  summarize(mean = mean(reviews),\n            variance = sd(reviews))\n##       mean variance\n## 1 27.19283 34.94139\n\nThey’re not! There’s more variance in the count of reviews than we should see in a Poisson distribution.\nAs another check, we can compare a super basic intercept-only model of the count of reviews with both a Poisson and a negative binomial model. The Poisson results from pp_check() aren’t great and show overdispersion; the negative binomial model fits well (since there’s a hyperparameter for dispersion).\n\nmodel_is_poisson <- brm(\n  bf(reviews ~ 1),\n  data = airbnb,\n  family = poisson(),\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"18-manual_cache/is-poisson\"\n)\n\nmodel_is_negbinom <- brm(\n  bf(reviews ~ 1),\n  data = airbnb,\n  family = negbinomial(),\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"18-manual_cache/is-negbinom\"\n)\n\n\np1 <- pp_check(model_is_poisson) +\n  labs(title = \"Poisson model\") +\n  guides(color = \"none\")\n\np2 <- pp_check(model_is_negbinom) +\n  labs(title = \"Negative binomial model\")\n\np1 | p2\n\n\n\n\nSo we’ll use a negative binomial model for this data. Like the logistic regression model, we’ll just define it all at once here instead of building it up slowly. In the book they base their priors on rstanarm’s magical autoscaled hyperparameters. I’ll modify them just a tiny bit:\n\n\\(\\beta_{0_c}\\) or baseline number of reviews: they used \\(\\mathcal{N}(3, 2.5)\\), which implies a baseline average of \\(e^3\\), or 20ish reviews. That seems fine.\n\\(\\beta_1\\) or the effect of ratings: they used \\(\\mathcal{N}(0, 7.37)\\), which is pretty vague. This is the slope of the line on the log scale—a one unit increase in ratings is associated with a β increase in the logged count of reviews, or a \\(e^\\beta\\) percent increase in the count of reviews. We’ll just use \\(\\mathcal{N}(0, 7)\\) since it feels less arbitrary than the automatic 7.37.\n\\(\\beta_2\\) (private room) and \\(\\beta_3\\) (shared room) or the effect of room type: they used \\(\\mathcal{N}(0, 5.04)\\) and \\(\\mathcal{N}(0, 14.19)\\). This implies that the two room types influence the logged count of reviews in different ways. We’ll just round these to \\(\\mathcal{N}(0, 5)\\) and \\(\\mathcal{N}(0, 15)\\)\n\\(r\\) and \\(\\sigma_0\\): these are standard deviations and have to be positive. \\(\\operatorname{Exponential}(1)\\) seems fine.\n\nWith that, there’s the official formal model:\n\n\\[\n\\begin{aligned}\n\\text{Reviews}_{i_j} \\sim&\\ \\operatorname{NegBin}(\\mu_{i_j}, r) & \\text{Reviews for listing}_i \\text{ in neighborhood}_j \\\\\n\\log (\\mu_{i_j}) =&\\ \\beta_{0_j} + \\beta_1\\, \\text{Rating}_{i_j} + & \\text{Model for } \\mu \\\\\n&\\ \\beta_2\\, \\text{Private room}_{i_j} + \\beta_3\\, \\text{Shared room}_{i_j} \\\\\n\\beta_{0_j} \\sim&\\ \\mathcal{N}(\\beta_0, \\sigma_0) & \\text{Neighborhood-specific review counts} \\\\\n\\\\\n\\beta_{0_c} \\sim&\\ \\mathcal{N}(3, 2.5) & \\text{Prior for global average count} \\\\\n\\beta_1 \\sim&\\ \\mathcal{N}(0, 7) & \\text{Prior for global rating effect} \\\\\n\\beta_2 \\sim&\\ \\mathcal{N}(0, 5) & \\text{Prior for global private room effect} \\\\\n\\beta_3 \\sim&\\ \\mathcal{N}(0, 15) & \\text{Prior for global shared room effect} \\\\\nr \\sim&\\ \\operatorname{Exponential}(1) & \\text{Prior for within-listing dispersion} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) & \\text{Prior for between-neighborhood variability}\n\\end{aligned}\n\\]\n\n \nOr with offset-based notation:\n\n\\[\n\\begin{aligned}\n\\text{Reviews}_{i_j} \\sim&\\ \\operatorname{NegBin}(\\mu_{i_j}, r) & \\text{Reviews for listing}_i \\text{ in neighborhood}_j \\\\\n\\log (\\mu_{i_j}) =&\\ (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{Rating}_{i_j} + & \\text{Model for } \\mu \\\\\n&\\ \\beta_2\\, \\text{Private room}_{i_j} + \\beta_3\\, \\text{Shared room}_{i_j} \\\\\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) & \\text{Neighborhood-specific offsets from global average count} \\\\\n\\\\\n\\beta_{0_c} \\sim&\\ \\mathcal{N}(3, 2.5) & \\text{Prior for global average count} \\\\\n\\beta_1 \\sim&\\ \\mathcal{N}(0, 7) & \\text{Prior for global rating effect} \\\\\n\\beta_2 \\sim&\\ \\mathcal{N}(0, 5) & \\text{Prior for global private room effect} \\\\\n\\beta_3 \\sim&\\ \\mathcal{N}(0, 15) & \\text{Prior for global shared room effect} \\\\\nr \\sim&\\ \\operatorname{Exponential}(1) & \\text{Prior for within-listing dispersion} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) & \\text{Prior for between-neighborhood variability}\n\\end{aligned}\n\\]\n\n\n\nPosterior simulation and analysis\nWe can run this model by including (1 | expedition_id) for the random team intercepts:\n\nbrmsrstanarm\n\n\n\npriors <- c(prior(normal(3, 2.5), class = Intercept),\n            prior(normal(0, 2.5), class = b, coef = \"rating_c\"),\n            prior(normal(0, 2.5), class = b, coef = \"room_typePrivateroom\"),\n            prior(normal(0, 2.5), class = b, coef = \"room_typeSharedroom\"),\n            prior(exponential(1), class = shape),\n            prior(exponential(1), class = sd))\n\nmodel_reviews_brms <- brm(\n  bf(reviews ~ rating_c + room_type + (1 | neighborhood)), \n  data = airbnb,\n  family = negbinomial(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"18-manual_cache/reviews-brms\"\n)\n\n\n\nWe’ll keep the autoscaled priors here though?\n\nmodel_reviews_rstanarm <- stan_glmer(\n  reviews ~ rating_c + room_type + (1 | neighborhood), \n  data = airbnb, family = neg_binomial_2,\n  prior_intercept = normal(3, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, cores = 4, iter = 5000*2, seed = 84735, refresh = 0)\n\n\n\n\n\nGlobal analysis\nFirst we’ll look at what this model says about a typical listing, which involves the \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) terms:\n\nbrmsrstanarm\n\n\n\nmodel_reviews_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 4 × 6\n##   effect term                 estimate std.error  conf.low conf.high\n##   <chr>  <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n## 1 fixed  (Intercept)            3.26      0.0538  3.19         3.32 \n## 2 fixed  rating_c               0.265     0.0847  0.155        0.372\n## 3 fixed  room_typePrivateroom   0.0683    0.0531  0.000556     0.136\n## 4 fixed  room_typeSharedroom   -0.469     0.152  -0.658       -0.274\n\n\nmodel_reviews_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  ungroup() |> \n  mutate(.variable = fct_inorder(.variable)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = clrs[1:4], guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Logged value\", y = \"Density\")\n\n\n\n\nUnlogged coefficients:\n\nmodel_reviews_brms %>%\n  model_parameters(centrality = \"mean\", dispersion = TRUE, component = \"conditional\",\n                   test = FALSE, verbose = FALSE, ci_method = \"hdi\", ci = 0.8,\n                   exponentiate = TRUE)\n## Parameter            |  Mean |   SD |         80% CI |  Rhat |      ESS\n## -----------------------------------------------------------------------\n## (Intercept)          | 25.92 | 0.05 | [24.21, 27.76] | 1.000 |  5461.00\n## rating_c             |  1.30 | 0.08 | [ 1.18,  1.46] | 1.000 | 14545.00\n## room_typePrivateroom |  1.07 | 0.05 | [ 1.00,  1.15] | 1.000 | 13535.00\n## room_typeSharedroom  |  0.63 | 0.15 | [ 0.52,  0.76] | 1.000 | 12843.00\n\n\nmodel_reviews_brms |> \n  gather_draws(`^b_.*`, regex = TRUE) |>\n  ungroup() |> \n  mutate(.variable = fct_inorder(.variable),\n         .value = exp(.value)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = clrs[1:4], guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Unlogged value\", y = \"Density\")\n\n\n\n\n\n\nLogged coefficients\n\nmodel_reviews_rstanarm |> \n  tidy(effects = c(\"fixed\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 4 × 5\n##   term                  estimate std.error conf.low conf.high\n##   <chr>                    <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)             3.26      0.0502  3.19        3.32 \n## 2 rating_c                0.265     0.0836  0.156       0.372\n## 3 room_typePrivate room   0.0688    0.0525  0.00147     0.135\n## 4 room_typeShared room   -0.468     0.149  -0.658      -0.274\n\n\nmodel_reviews_rstanarm |> \n  gather_draws(`(Intercept)`, rating_c, \n               `room_typePrivate room`, `room_typeShared room`) |>\n  ungroup() |> \n  mutate(.variable = fct_inorder(.variable)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = clrs[1:4], guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Logged value\", y = \"Density\")\n\n\n\n\nUnlogged coefficients\n\nmodel_reviews_rstanarm %>%\n  model_parameters(centrality = \"mean\", dispersion = TRUE, prior = FALSE, \n                   test = FALSE, verbose = FALSE, ci_method = \"hdi\", ci = 0.8,\n                   exponentiate = TRUE)\n## Parameter             |  Mean |   SD |         80% CI |  Rhat |      ESS\n## ------------------------------------------------------------------------\n## (Intercept)           | 25.91 | 0.05 | [24.22, 27.67] | 1.000 |  8368.00\n## rating_c              |  1.30 | 0.08 | [ 1.17,  1.46] | 1.000 | 22356.00\n## room_typePrivate room |  1.07 | 0.05 | [ 1.00,  1.15] | 1.000 | 22730.00\n## room_typeShared room  |  0.63 | 0.15 | [ 0.51,  0.75] | 1.000 | 21767.00\n## reciprocal_dispersion |  2.87 | 0.04 | [ 2.74,  3.00] | 1.000 | 20341.00\n\n\nmodel_reviews_rstanarm |> \n  gather_draws(`(Intercept)`, rating_c, \n               `room_typePrivate room`, `room_typeShared room`) |>\n  ungroup() |> \n  mutate(.value = exp(.value),\n         .variable = fct_inorder(.variable)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_manual(values = clrs[1:4], guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\") +\n  labs(x = \"Unlogged value\", y = \"Density\")\n\n\n\n\n\n\n\nInterpretation time! (with the brms results):\n\nIntercept (\\(\\beta_0\\))\nThe posterior mean for \\(\\beta_0\\) is 3.26, but that’s logged and fairly meaningless. Exponentiating it like \\(e^{3.26}\\) gives us a posterior mean count of 25.92 reviews for a private property listing with an average rating in an typical neighborhood, and there’s an 80% chance the count is between 24.19 and 27.74.\n\n\nRating (\\(\\beta_1\\))\nThe posterior mean for \\(\\beta_1\\) is 0.265, which means that the logged count of reviews should increase by that amount for every one-point increase in rating. Exponentiating that value (\\(e^{0.265}\\)) gives us a posterior unlogged value of 1.303 with an 80% credible interval of 1.17–1.45. That means that a 1-point increase in ratings is associated with a 30% increase in the count of reviews with an 80% credible interval of 17%–45%.\nWe can measure this with actual counts too instead of working with percent changes. First, we can look at the linear predictor (posterior_linpred(transform = TRUE) or posterior_epred()) to see how the predicted count of reviews changes across possible ratings:\n\n# Automatic way:\n# plot_cap(model_reviews_brms, condition = \"rating_c\", re_formula = NA)\n\ndatagrid(model = model_reviews_brms,\n         rating_c = seq(2.5, 5, by = 0.5) - unscaled_airbnb$rating_c$scaled_center) |> \n  add_epred_draws(model_reviews_brms, re_formula = NA) |> \n  mutate(rating = rating_c + unscaled_airbnb$rating_c$scaled_center) |> \n  ggplot(aes(x = rating, y = .epred)) +\n  stat_lineribbon(color = clrs[1], fill = clrs[1], alpha = 0.35) +\n  labs(x = \"Rating\", y = \"Posterior count of reviews\")\n\n\n\n\nThe slope of this line changes depending on the rating—it gets steeper at higher ratings. We can get exact estimates of the slope of this posterior with marginaleffects().\n\nmfx_reviews <- model_reviews_brms |>\n  marginaleffects(newdata = datagrid(rating_c = seq(2.5, 5, by = 0.5) -\n                                       unscaled_airbnb$rating_c$scaled_center),\n                  variables = \"rating_c\",\n                  type = \"response\", re_formula = NA) |>\n  posteriordraws() |>\n  mutate(rating = rating_c + unscaled_airbnb$rating_c$scaled_center)\n\nmfx_reviews |> \n  ggplot(aes(x = rating, y = draw)) +\n  stat_lineribbon(alpha = 0.25, fill = clrs[4], color = clrs[4]) +\n  labs(x = \"Rating\", \n       y = \"Marginal effect on count of reviews\")\n\n\n\n\nThe y-axis here is the slope of the line, not the predicted count of reviews. For a listing currently rated a 3, a one-unit change to a 4 is associated with an increase of rround(mfx_reviews_list\\(x3\\)mean_slope, 2)` reviews; for a listing rated a 4, a one-unit change to a 5 is associated with a larger increase of 5.51 reviews.\n\nmfx_reviews |> \n  filter(rating %in% c(3, 4)) |> \n  group_by(rating) |> \n  summarize(mean_slope = mean(draw),\n            conf.low = mean(conf.low),\n            conf.high = mean(conf.high))\n## # A tibble: 2 × 4\n##   rating mean_slope conf.low conf.high\n##    <dbl>      <dbl>    <dbl>     <dbl>\n## 1      3       4.15     2.11      5.44\n## 2      4       5.51     2.33      8.22\n\n\n\nRoom type (\\(\\beta_2\\) and \\(\\beta_3\\))\nThe posterior means for room type show the average difference in \\(\\mu\\) when the room type is a private room vs. an entire home and a shared room vs. an entire home:\n\nmodel_reviews_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  mutate(across(c(estimate, conf.low, conf.high), list(exp = exp))) |> \n  filter(str_detect(term, \"room_type\")) |> \n  select(-effect, -component, -std.error) |> \n  gt() |> \n  tab_spanner(\n    label = \"Logged scale\",\n    columns = c(estimate, conf.low, conf.high)\n  ) |> \n  tab_spanner(\n    label = \"Exponentiated scale\",\n    columns = c(estimate_exp, conf.low_exp, conf.high_exp)\n  ) |> \n  fmt_number(\n    columns = -term,\n    n_sigfig = 3\n  ) |> \n  tab_source_note(\n    source_note = \"Estimates show posterior means and 80% credible intervals\"\n  )\n\n\n\n\n\n  \n  \n    \n      term\n      \n        Logged scale\n      \n      \n        Exponentiated scale\n      \n    \n    \n      estimate\n      conf.low\n      conf.high\n      estimate_exp\n      conf.low_exp\n      conf.high_exp\n    \n  \n  \n    room_typePrivateroom\n0.0683\n0.000556\n0.136\n1.07\n1.00\n1.15\n    room_typeSharedroom\n−0.469\n−0.658\n−0.274\n0.626\n0.518\n0.760\n  \n  \n    \n      Estimates show posterior means and 80% credible intervals\n    \n  \n  \n\n\n\n\nWe’ll skip right to the exponentiated scale since that makes more sense to me. When compared to an entire private home, a private room doesn’t see too much of a difference in the count of reviews—there’s a 7% increase in the count of reviews with an 80% credible interval of 0.1%–14.6%. That range doesn’t include zero, but only because we’re looking at an 80% credible interval. Expand that to 89% or 89% or whatever and there’s a chance that the difference is 0.\nShared rooms, on the other hand, have substantially fewer reviews than private home. For these, there’s a -37% decrease in the count of reviews with an 80% credible interval of -24%–-48%. That’s definitely not zero and it feels like a substantial real difference. For whatever reason, people don’t seem to leave as many reviews for shared rooms.\nHere’s what these differences look like as counts instead of percent changes:\n\ndatagrid(model = model_reviews_brms,\n         room_type = unique(airbnb$room_type)) |> \n  add_epred_draws(model_reviews_brms, re_formula = NA) |> \n  ggplot(aes(x = room_type, y = .epred, fill = room_type, color = room_type)) +\n  stat_gradientinterval(width = 0.25) +\n  scale_fill_manual(values = c(clrs[2], clrs[6], clrs[5]), guide = \"none\") +\n  scale_color_manual(values = colorspace::darken(c(clrs[2], clrs[6], clrs[5]), 0.5), guide = \"none\") +\n  labs(x = \"Room type\", y = \"Posterior count of reviews\")\n\n\n\n\nAs before, we can find the exact difference in these posteriors with tidybayes::compare_levels(). The difference between a private room and an entire house is roughly 2 reviews, while the difference between a shared room and an entire house is a more sizable ≈10 reviews.\n\nroom_type_diffs <- datagrid(\n  model = model_reviews_brms,\n  room_type = unique(airbnb$room_type)\n) |> \n  add_epred_draws(model_reviews_brms, re_formula = NA) |> \n  compare_levels(variable = .epred, by = room_type)\n\nroom_type_diffs |> \n  group_by(room_type) |> \n  mean_qi(.epred)\n## # A tibble: 3 × 7\n##   room_type                      .epred  .lower .upper .width .point .interval\n##   <chr>                           <dbl>   <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 Private room - Entire home/apt   1.84  -0.959   4.71   0.95 mean   qi       \n## 2 Shared room - Entire home/apt   -9.55 -14.3    -3.70   0.95 mean   qi       \n## 3 Shared room - Private room     -11.4  -16.3    -5.55   0.95 mean   qi\n\n\nroom_type_diffs |> \n  filter(room_type != \"Shared room - Private room\") |> \n  ggplot(aes(x = .epred, fill = room_type)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[6], clrs[5])) +\n  labs(x = \"Difference in predicted review counts\",\n       y = \"Density\", fill = \"Contrast in room type\")\n\n\n\n\nWe can also use marginaleffects::comparisons() to calculate actual average marginal effects (or group contrasts).\n\nmfx_cmp_reviews <- model_reviews_brms |> \n  comparisons(variables = \"room_type\", re_formula = NA)\n\n\nmfx_cmp_reviews |> \n  posteriordraws() |> \n  group_by(drawid, contrast) |> \n  summarize(draw = mean(draw)) |> \n  ggplot(aes(x = draw, fill = contrast)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[6], clrs[5])) +\n  labs(x = \"Difference in predicted review counts\",\n       y = \"Density\", fill = \"Contrast in room type\")\n\n\n\n\nAnd for extra fun we can integrate out the random effects instead of ignoring them like we’ve been doing so far. This is super neat! Instead of choosing an arbitrary existing neighborhood for our predictions, this essentially accounts for general neighborhood-level effects but for global parameters.\n\nmfx_reviews_integrated_out <- comparisons(\n  model_reviews_brms,\n  # 100 fake neighborhoods\n  newdata = datagrid(neighborhood = -1:-100),\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\")\n\n\nmfx_reviews_integrated_out |> summary()\n##        Term                       Contrast Effect   2.5 % 97.5 %\n## 1  rating_c                    (x + 1) - x  6.997   2.381 11.909\n## 2 room_type Private room - Entire home/apt  1.848  -1.334  5.097\n## 3 room_type  Shared room - Entire home/apt -9.907 -14.847 -3.660\n## \n## Model type:  brmsfit \n## Prediction type:  response\n\nA one-unit change in rating is associated with a “significant” and substantial ≈7 additional reviews; a typical private room sees 2ish more reviews than an entire home, but that could also be 0 or even negative; a typical shared room sees 10ish fewer reviews than an entire home, and that difference is “significant” and substantial.\n\nmfx_reviews_integrated_out |> \n  posteriordraws() |> \n  group_by(drawid, term, contrast) |> \n  summarize(draw = mean(draw)) |> \n  ggplot(aes(x = draw, fill = contrast)) +\n  stat_halfeye() +\n  facet_wrap(vars(term), scales = \"free_x\") +\n  scale_fill_manual(values = c(clrs[1], clrs[6], clrs[5])) +\n  labs(title = \"Average marginal effect of coefficients\",\n       subtitle = \"Random effects integrated out\",\n       x = \"Change in count of reviews\", y = \"Density\", \n       fill = \"Marginal effect or contrast\")\n\n\n\n\n\n\nRating and room type at the same time\nFor fun, here’s the effect of rating across all three room types. All three room types trend upward in parallel (they just have different intercepts, not room-type-specific slopes), and there’s not really a visible difference private rooms and entire homes.\n\ndatagrid(model = model_reviews_brms,\n         rating_c = seq(2.5, 5, by = 0.5) - unscaled_airbnb$rating_c$scaled_center,\n         room_type = unique(airbnb$room_type)) |> \n  add_epred_draws(model_reviews_brms, re_formula = NA, ndraws = 100) |> \n  mutate(rating = rating_c + unscaled_airbnb$rating_c$scaled_center) |> \n  ggplot(aes(x = rating, y = .epred, color = room_type)) +\n  geom_line(aes(group = paste(room_type, .draw)), alpha = 0.35) +\n  scale_color_manual(values = c(clrs[2], clrs[6], clrs[5])) +\n  labs(x = \"Rating\", y = \"Posterior count of reviews\", \n       color = \"Room type\")\n\n\n\n\n\n\n\nNeighborhood-specific analysis\nNext we can look at the neighborhood-specific details, or the offsets from the baseline probability of success:\n\\[\nB_0 + b_{0_j}\n\\]\nUnlike the expedition teams from the logistic regression example, there’s not a huge amount of variation between neighborhoods here. If we look at the top 5 and bottom 5, we can see that the widest differences is only a matter of 12 reviews (highest neighborhood − lowest neighborhood):\n\nneighborhood_baselines <- model_reviews_brms |> \n  epred_draws(tibble(neighborhood = unique(airbnb$neighborhood),\n                     rating_c = 0, room_type = \"Entire home/apt\")) |> \n  ungroup() |> \n  mutate(neighborhood = fct_reorder(factor(neighborhood), .epred, .fun = mean))\n\nneighborhood_baselines |> \n  group_by(neighborhood) |> \n  summarize(avg = mean(.epred)) |> \n  arrange(desc(avg)) |> \n  slice(1:5, 39:43)\n## # A tibble: 10 × 2\n##    neighborhood         avg\n##    <fct>              <dbl>\n##  1 Gage Park           32.5\n##  2 East Garfield Park  32.4\n##  3 West Lawn           32.0\n##  4 Burnside            31.0\n##  5 Edgewater           30.3\n##  6 North Park          23.6\n##  7 Irving Park         23.0\n##  8 South Chicago       22.9\n##  9 Lincoln Square      21.9\n## 10 Albany Park         20.6\n\nWe can see this narrower range when looking at all 43 teams simultaneously:\n\nglobal_baseline <- model_reviews_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  filter(term == \"(Intercept)\") |> \n  mutate(across(c(estimate, conf.low, conf.high), exp))\n\nneighborhood_baselines |> \n  ggplot(aes(x = .epred, y = neighborhood)) +\n  annotate(geom = \"rect\", ymin = -Inf, ymax = Inf,\n           xmin = global_baseline$conf.low, xmax = global_baseline$conf.high,\n           fill = clrs[2], alpha = 0.4) +\n  geom_vline(xintercept = global_baseline$estimate, color = clrs[2]) +\n  stat_pointinterval(color = clrs[3], size = 0.1,\n                     point_interval = \"mean_qi\") +\n  labs(x = \"Baseline count of reviews\", y = \"Neighborhood\",\n       title = \"Baseline count of reviews for listings in neighborhood\",\n       subtitle = \"Global baseline and 80% credible interval shown in yellow\")\n\n\n\n\nWe can also look at how these neighborhood-specific offsets influence the relationship of ratings and review counts. Here are three arbitrarily chosen neighborhoods—the slope is the same in all three panels, but the baseline intercept gets shifted up and down,\n\nairbnb_small <- airbnb |> \n  filter(neighborhood %in% c(\"Albany Park\", \"East Garfield Park\", \"The Loop\"))\n\nairbnb_small |> \n  add_epred_draws(model_reviews_brms, ndraws = 250) |> \n  ggplot(aes(x = rating, y = reviews)) +\n  geom_line(aes(y = .epred, group = paste(neighborhood, .draw), \n                color = neighborhood),\n            alpha = 0.2, size = 0.3) +\n  geom_point(data = airbnb_small, aes(color = neighborhood)) +\n  scale_color_manual(values = c(clrs[1], clrs[2], clrs[5]), guide = \"none\") +\n  labs(x = \"Rating\", y = \"Reviews\") +\n  facet_wrap(vars(neighborhood))\n\n\n\n\n\n\nBetween-neighborhood variability\nWith linear regression we have two forms of variability:\n\n\\(\\sigma_y\\) for the variability within units nested in groups (i.e. the scale term in \\(Y_{i_j} \\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_{i_j})\\))\n\\(\\sigma_0\\) for the variability between groups (i.e. the variation around the random offsets in \\(b_{0_j} \\sim \\mathcal{N}(0, \\sigma_0)\\))\n\nWith logistic regression we didn’t have a \\(\\sigma_y\\) term for variability within expedition teams, since the Bernoulli model for \\(Y_{i_j}\\) only has one hyperparameter \\(\\pi_{i_j}\\)\nWith negative binomial regression, though we have two hyperparameters, roughly equivalent to the location (\\(\\mu\\), or mean) and scale (\\(r\\), or spread) of the distribution. I don’t think not sure if this \\(r\\) term is technically comparable to \\(\\sigma_y\\) in Gaussian regression, since it doesn’t show up in ran_pars and is instead a distributional parameter\n\nmodel_reviews_brms |> \n  tidy(parameters = c(\"shape\", \"^sd_\"))\n## # A tibble: 2 × 6\n##   component term                         estimate std.error conf.low conf.high\n##   <chr>     <chr>                           <dbl>     <dbl>    <dbl>     <dbl>\n## 1 cond      shape                           1.06     0.0355   0.987      1.13 \n## 2 cond      sd_neighborhood__(Intercept)    0.174    0.0724   0.0381     0.328\n\n\\(\\sigma_0\\) (or sd__(Intercept) for the neighborhood group) is 0.17, which is the amount of variance in the log of the global intercept, so the average baseline log count varies between neighborhoods with a standard deviation of 0.17 log units. We can see this in a plot of each neighborhood’s log-scale baseline average. The global average is 3.26, marked in yellow, and there a little bit of variation around that average—a standard deviation of 0.17.\n\nglobal_baseline_log <- model_reviews_brms |> \n  tidy(effects = c(\"fixed\"), conf.level = 0.8) |> \n  filter(term == \"(Intercept)\")\n\nneighborhood_baselines_log <- model_reviews_brms |> \n  linpred_draws(tibble(neighborhood = unique(airbnb$neighborhood),\n                       rating_c = 0, room_type = \"Entire home/apt\")) |> \n  ungroup() |> \n  mutate(neighborhood = fct_reorder(factor(neighborhood), .linpred, .fun = mean))\n\nneighborhood_baselines_log |> \n  ggplot(aes(x = .linpred, y = neighborhood)) +\n  annotate(geom = \"rect\", ymin = -Inf, ymax = Inf,\n           xmin = global_baseline_log$conf.low, xmax = global_baseline_log$conf.high,\n           fill = clrs[2], alpha = 0.4) +\n  geom_vline(xintercept = global_baseline_log$estimate, color = clrs[2]) +\n  stat_pointinterval(color = clrs[6], size = 0.1,\n                     point_interval = \"mean_qi\") +\n  labs(x = \"Baseline average log count of laws\", y = \"Neighborhood\")\n\n\n\n\nFor whatever reason, we can’t get an ICC for the percent of variation explained by between-neighborhood differences.\n\nperformance::icc(model_reviews_brms)\n## [1] NA\n\nWe can use performance::variance_decomposition() to get a comparable number, since it uses the posterior predictive distribution of the model to figure out the between-group variance. It looks like neighborhood difference only explain 8ish% of the variation? But the credible interval goes negative and I don’t know what it would mean to have a negative ratio like that?\n\nperformance::variance_decomposition(model_reviews_brms)\n## # Random Effect Variances and ICC\n## \n## Conditioned on: all random effects\n## \n## ## Variance Ratio (comparable to ICC)\n## Ratio: 0.08  CI 95%: [-0.18 0.31]\n## \n## ## Variances of Posterior Predicted Distribution\n## Conditioned on fixed effects: 714.85  CI 95%: [556.40 907.03]\n## Conditioned on rand. effects: 776.68  CI 95%: [644.48 946.32]\n## \n## ## Difference in Variances\n## Difference: 62.25  CI 95%: [-132.07 267.03]\n\n\n\n\nPosterior predictions\nAccording to the image above that shows the baseline average differences across neighborhoods, Albany Park has fewer reviews than average, East Garfield Park has more reviews than average, and The Loop has basically the average number of reviews.\n\npredicted_reviews <- model_reviews_brms |> \n  predicted_draws(\n    expand_grid(rating_c = 5 - unscaled_airbnb$rating_c$scaled_center,\n                room_type = \"Entire home/apt\",\n                neighborhood = c(\"Albany Park\", \"East Garfield Park\", \"The Loop\"))\n  )\n\npredicted_reviews |> \n  ggplot(aes(x = .prediction, fill = neighborhood)) +\n  geom_histogram(binwidth = 2, color = \"white\", linewidth = 0.3) +\n  stat_pointinterval() +\n  scale_fill_manual(values = c(clrs[1], clrs[2], clrs[5]), guide = \"none\") +\n  coord_cartesian(xlim = c(0, 150)) +\n  labs(x = \"Predicted count of reviews\") +\n  facet_wrap(vars(neighborhood), ncol = 1) +\n  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nEast Garfield has a higher predicted count of reviews, but there’s ultimately not a huge difference across these neighborhoods.\n\n\nModel evaluation\n\nHow fair is the model?\nFine, but not super generalizable:\n\nWhat was available was information about the AirBnB market in Chicago. Thus, we’d be hesitant to use our analysis for anything more than general conclusions about the broader market.\n\n\n\nHow wrong is the model?\npp_check() looks great:\n\nbrmsrstanarm\n\n\n\npp_check(model_reviews_brms, ndraws = 25)\n\n\n\n\n\n\n\npp_check(model_reviews_rstanarm, n = 25)\n\n\n\n\n\n\n\n\n\nHow accurate are the model’s posterior classifications?\nWe don’t have competing models to compare, so LOO isn’t super helpful, but it does show that there aren’t any concerning Pareto k values, so that’s good I guess.\n\nloo(model_reviews_brms)\n## \n## Computed from 8000 by 1561 log-likelihood matrix\n## \n##          Estimate    SE\n## elpd_loo  -6737.7  50.5\n## p_loo        29.4   2.7\n## looic     13475.4 101.1\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     1559  99.9%   1584      \n##  (0.5, 0.7]   (ok)          2   0.1%   1103      \n##    (0.7, 1]   (bad)         0   0.0%   <NA>      \n##    (1, Inf)   (very bad)    0   0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\n\nWe also can calculate the mean absolute prediction error (MAE) and scaled MAE. This shows that the observed number of reviews for a listing is around ≈18ish reviews away from its posterior mean prediction, or ≈1 standard deviation:\n\nreviews_preds_brms <- model_reviews_brms |> \n  add_predicted_draws(newdata = airbnb)\n\nreviews_preds_brms |> \n  ungroup() |> \n  mutate(error = reviews - .prediction) |> \n  summarize(mae = median(abs(error)),\n            mae_scaled = median(abs(error / mad(.prediction))))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1    18      0.934\n\nAnd that’s it! Complete analysis of a multilevel Poisson/negative binomial regression model!"
  },
  {
    "objectID": "bayes-rules/19-chapter.html",
    "href": "bayes-rules/19-chapter.html",
    "title": "19: Adding more layers",
    "section": "",
    "text": "(Original chapter)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bayesf22 Notebook",
    "section": "",
    "text": "My notebook for working through the readings and exercises in my Bayesian Statistics readings class in Fall 2022"
  },
  {
    "objectID": "rethinking/03-chapter.html",
    "href": "rethinking/03-chapter.html",
    "title": "Chapter 3 notes",
    "section": "",
    "text": "library(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(patchwork)\nlibrary(posterior)\nlibrary(broom.mixed)\n\nset.seed(1234)\nAssuming 9 globe tosses, 6 are water:\nOr in code:\nGiven this data, what’s the proportion of water on the globe?"
  },
  {
    "objectID": "rethinking/03-chapter.html#grid-approximation",
    "href": "rethinking/03-chapter.html#grid-approximation",
    "title": "Chapter 3 notes",
    "section": "Grid approximation",
    "text": "Grid approximation\nFor each possible value of \\(p\\), compute the product \\(\\operatorname{Pr}(W, L \\mid p) \\times \\operatorname{Pr}(p)\\). The relative sizes of each of those products are the posterior probabilities.\n\nBase R with Rethinking\n\nUniform flat prior\n\n# List of possible explanations of p to consider\np_grid <- seq(from = 0, to = 1, length.out = 10000)\nplot(p_grid, main = \"Possible proportions (p)\")\n#\n\n# Probability of each value of p\n# Super vague uniform prior: just 1 at each possible p\nprob_p_uniform <- rep(1, 10000)\nplot(prob_p_uniform, main = \"Uniform flat prior\")\n#\n\n# Probability of each proportion, given 6/9 water draws\nprob_data <- dbinom(6, size = 9, prob = p_grid)\n\n# Unnormalized posterior\nposterior_raw <- prob_data * prob_p_uniform\nplot(posterior_raw, main = \"Unnormalized posterior\")\n#\n\n# Normalized posterior that sums to 1\nposterior_normalized <- posterior_raw / sum(posterior_raw)\nplot(posterior_normalized, main = \"Normalized posterior\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeta prior\n\n# Beta distribution with 3 / (3 + 1)\nprob_p_beta <- dbeta(p_grid, shape1 = 3, shape2 = 1)\nplot(prob_p_beta, main = \"Beta(3, 1) prior\")\n#\n# Posterior that sums to 1\nposterior_normalized_beta <- (prob_data * prob_p_beta) / sum(posterior_raw)\nplot(posterior_normalized_beta, main = \"Normalized postiorior with beta prior\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse style from Solomon Kurz\n\nglobe_tossing <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1001),\n                        prior_uniform = 1) %>%  # prob_p_uniform from earlier\n  mutate(prior_beta = dbeta(p_grid, shape1 = 3, shape2 = 1)) %>%  # prob_p_beta from earlier\n  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%   # prob_data from earlier\n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform),\n         posterior_beta = (likelihood * prior_beta) / sum(likelihood * prior_beta))\nglobe_tossing\n## # A tibble: 1,001 × 6\n##    p_grid prior_uniform prior_beta likelihood posterior_uniform posterior_beta\n##     <dbl>         <dbl>      <dbl>      <dbl>             <dbl>          <dbl>\n##  1  0                 1  0           0                 0              0       \n##  2  0.001             1  0.000003    8.37e-17          8.37e-19       1.97e-24\n##  3  0.002             1  0.000012    5.34e-15          5.34e-17       5.04e-22\n##  4  0.003             1  0.0000270   6.07e-14          6.07e-16       1.29e-20\n##  5  0.004             1  0.000048    3.40e-13          3.40e-15       1.28e-19\n##  6  0.005             1  0.000075    1.29e-12          1.29e-14       7.62e-19\n##  7  0.006             1  0.000108    3.85e-12          3.85e-14       3.27e-18\n##  8  0.007             1  0.000147    9.68e-12          9.68e-14       1.12e-17\n##  9  0.008             1  0.000192    2.15e-11          2.15e-13       3.24e-17\n## 10  0.009             1  0.000243    4.34e-11          4.34e-13       8.30e-17\n## # … with 991 more rows\n\n\nglobe_tossing %>% \n  pivot_longer(starts_with(\"posterior\")) %>% \n  ggplot(aes(x = p_grid, y = value, fill = name)) +\n  geom_area(position = position_identity(), alpha = 0.5)"
  },
  {
    "objectID": "rethinking/03-chapter.html#working-with-the-posterior",
    "href": "rethinking/03-chapter.html#working-with-the-posterior",
    "title": "Chapter 3 notes",
    "section": "Working with the posterior",
    "text": "Working with the posterior\nWe now have a posterior! We typically can’t use the posterior alone. We have to average any inference across the entire posterior. This requires calculus, which is (1) hard, and (2) often impossible. So instead, we can use samples from the distribution and make inferences based on those.\n\n3.2: Sampling to summarize\nHere are 10,000 samples from the posterior (based on the uniform flat prior). These are the sampling distributions.\n\nsamples <- sample(p_grid, prob = posterior_normalized, size = 10000, replace = TRUE)\nplot(samples, main = \"10,000 posterior samples\")\n#\n\nplot(density(samples), main = \"Distribution of 10,000 posterior samples\")\n\n\n\n\n\n\n\n\n\n\n\n\nsamples_tidy <- globe_tossing %>% \n  slice_sample(n = 10000, weight_by = posterior_uniform, replace = T)\n\n\n3.2.1: Intervals of defined boundaries\n\nBase RTidyverse\n\n\nWhat’s the probability that the proportion of water is less than 50%?\n\nsum(samples < 0.5) / 10000\n## [1] 0.1745\n\nHow much of the posterior is between 50% and 75%?\n\nsum(samples > 0.5 & samples < 0.75) / 10000\n## [1] 0.6037\n\n\n\nWhat’s the probability that the proportion of water is less than 50%?\n\nglobe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_line() +\n  geom_area(data = filter(globe_tossing, p_grid < 0.5))\n\n\n\n\nsamples_tidy %>% \n  count(p_grid < 0.5) %>% \n  mutate(probability = n / sum(n))\n## # A tibble: 2 × 3\n##   `p_grid < 0.5`     n probability\n##   <lgl>          <int>       <dbl>\n## 1 FALSE           8321       0.832\n## 2 TRUE            1679       0.168\n\nHow much of the posterior is between 50% and 75%?\n\nglobe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_line() +\n  geom_area(data = filter(globe_tossing, p_grid > 0.5 & p_grid < 0.75))\n\n\n\n\nsamples_tidy %>% \n  count(p_grid > 0.5 & p_grid < 0.75) %>% \n  mutate(probability = n / sum(n))\n## # A tibble: 2 × 3\n##   `p_grid > 0.5 & p_grid < 0.75`     n probability\n##   <lgl>                          <int>       <dbl>\n## 1 FALSE                           4000         0.4\n## 2 TRUE                            6000         0.6\n\n\n\n\n\n\n3.2.2: Intervals of defined mass\n\nBase RTidyverse\n\n\nLower 80% posterior probability lies below this number:\n\nquantile(samples, 0.8)\n##      80% \n## 0.759576\n\nMiddle 80% posterior probability lies between these numbers:\n\nquantile(samples, c(0.1, 0.9))\n##       10%       90% \n## 0.4448445 0.8106911\n\n50% percentile interval vs. 50% HPDI\n\nquantile(samples, c(0.25, 0.75))\n##       25%       75% \n## 0.5416292 0.7387989\nrethinking::HPDI(samples, prob = 0.5)\n##      |0.5      0.5| \n## 0.5659566 0.7592759\n\n\n\nLower 80% posterior probability lies below this number:\n\nsamples_tidy %>% \n  summarize(`80th percentile` = quantile(p_grid, 0.8))\n## # A tibble: 1 × 1\n##   `80th percentile`\n##               <dbl>\n## 1             0.762\n\nMiddle 80% posterior probability lies between these numbers:\n\nsamples_tidy %>% \n  summarize(q = c(0.1, 0.9), percentile = quantile(p_grid, q)) %>% \n  pivot_wider(names_from = q, values_from = percentile)\n## # A tibble: 1 × 2\n##   `0.1` `0.9`\n##   <dbl> <dbl>\n## 1  0.45 0.811\n\n50% percentile interval vs. 50% HPDI\n\nsamples_tidy %>% \n  summarize(q = c(0.25, 0.75), \n            percentile = quantile(p_grid, q),\n            hpdi = HDInterval::hdi(p_grid, 0.5))\n## # A tibble: 2 × 3\n##       q percentile  hpdi\n##   <dbl>      <dbl> <dbl>\n## 1  0.25      0.544 0.563\n## 2  0.75      0.742 0.756\n\n\n\n\n\n\n3.2.3: Point estimates\n\nBase RTidyverse\n\n\n\nmean(samples)\n## [1] 0.6352952\nmedian(samples)\n## [1] 0.6448645\n\n\n\n\nsamples_tidy %>% \n  summarize(mean = mean(p_grid),\n            median = median(p_grid))\n## # A tibble: 1 × 2\n##    mean median\n##   <dbl>  <dbl>\n## 1 0.638  0.646\n\n\n\n\n\n\n\n3.3: Sampling to simulate prediction\n\nBase R\nWe can use the uncertainty inherent in the sampling distributions from above to generate a posterior predictive distribution, based on a 9-toss situation:\n\n# Posterior predictive distribution\nposterior_predictive_dist <- rbinom(10000, size = 9, prob = samples)\nhist(posterior_predictive_dist, breaks = 0:9)\n\n\n\n\n\n\nTidyverse style\n\n# Generate 100,000 samples from the posterior\nsamples_tidy <- globe_tossing %>% \n  slice_sample(n = 100000, weight_by = posterior_uniform, replace = T)\n#\nsamples_tidy %>% \n  mutate(sample_number = 1:n()) %>% \n  ggplot(aes(x = sample_number, y = p_grid)) +\n  geom_point(alpha = 0.05) +\n  labs(title = \"100,000 posterior samples\", x = \"Sample number\")\n#\nsamples_tidy %>% \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"grey50\", color = NA) +\n  labs(title = \"Distribution of 100,000 posterior samples\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6 with ggplot\n\n# Posterior probability\nglobe_smaller <- globe_tossing %>% \n  filter(p_grid %in% c(seq(0.1, 0.9, 0.1), 0.3))\n\npanel_top <- globe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) + \n  geom_area(fill = \"grey50\", color = NA) +\n  geom_segment(data = globe_smaller, aes(xend = p_grid, yend = 0, size = posterior_uniform)) +\n  geom_point(data = globe_smaller) +\n  scale_size_continuous(range = c(0, 1), guide = \"none\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"Proportion/probability of water\",\n       title = \"Posterior probability\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n\n# Sampling distributions\nglobe_sample_dists <- tibble(probability = seq(0.1, 0.9, 0.1)) %>% \n  mutate(draws = map(probability, ~{\n    set.seed(1234)\n    rbinom(10000, size = 9, prob = .x)\n  })) %>% \n  unnest(draws) %>% \n  mutate(label = paste0(\"p = \", probability))\n\npanel_middle <- ggplot(globe_sample_dists, aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.1) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = NULL, y = NULL, title = \"Sampling distributions\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\")) +\n  facet_wrap(vars(label), ncol = 9)\n\n# Posterior predictive distribution\nglobe_samples <- globe_tossing %>% \n  slice_sample(n = 10000, weight_by = posterior_uniform, replace = TRUE) %>% \n  mutate(prediction = map_dbl(p_grid, rbinom, n = 1, size = 9))\n\npanel_bottom <- globe_samples %>% \n  ggplot(aes(x = prediction)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.5) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = \"Number of water samples\", y = NULL, title = \"Posterior predictive distribution\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n\nlayout <- \"\nAAAAAAAAAAA\n#BBBBBBBBB#\n###CCCCC###\n\"\n\npanel_top / panel_middle / panel_bottom +\n  plot_layout(design = layout)"
  },
  {
    "objectID": "rethinking/03-chapter.html#brms-and-tidybayes-version-of-all-this",
    "href": "rethinking/03-chapter.html#brms-and-tidybayes-version-of-all-this",
    "title": "Chapter 3 notes",
    "section": "brms and tidybayes version of all this",
    "text": "brms and tidybayes version of all this\nOoh neat, you can pass single values as data instead of a data frame! Everything else here looks like regular brms stuff.\n\nmodel_globe <- brm(\n  bf(water | trials(9) ~ 0 + Intercept),\n  data = list(water = 6),\n  family = binomial(link = \"identity\"),\n  # Flat uniform prior\n  prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = 1234,\n  # TODO: Eventually switch to cmdstanr once this issue is fixed\n  # https://github.com/quarto-dev/quarto-cli/issues/2258\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\nCredible intervals / HPDI / etc.\n\n# Using broom.mixed\ntidy(model_globe, effects = \"fixed\",\n     conf.level = 0.5, conf.method = \"HPDinterval\")\n## # A tibble: 1 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   <chr>  <chr>     <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  cond      (Intercept)    0.639     0.137    0.568     0.761\n\n# Using the posterior package\ndraws <- as_draws_array(model_globe)\nsummarize_draws(draws, default_summary_measures()) %>% \n  filter(variable == \"b_Intercept\")\n## # A tibble: 1 × 7\n##   variable     mean median    sd   mad    q5   q95\n##   <chr>       <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 b_Intercept 0.639  0.648 0.137 0.144 0.400 0.848\n\n# Using tidybayes\n# get_variables(model_globe)\nmodel_globe %>% \n  spread_draws(b_Intercept) %>% \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.648  0.568  0.761   0.5  median hdci     \n## 2       0.648  0.430  0.864   0.89 median hdci     \n## 3       0.648  0.374  0.892   0.95 median hdci\n\nmodel_globe %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye()\n\n\n\n\nPredictions\n\nmodel_globe %>% \n  predicted_draws(newdata = tibble(nothing = 1)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.5) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = \"Number of water samples\", y = NULL, title = \"Posterior predictive distribution\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))"
  },
  {
    "objectID": "rethinking/03-practice.html",
    "href": "rethinking/03-practice.html",
    "title": "Chapter 3 exercises",
    "section": "",
    "text": "library(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(broom.mixed)\nlibrary(glue)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)\nThis births data shows the sex of the first and second children born to 100 different families (1 = boy, 2 = girl). The first family had a boy then a girl, the second had a girl and then a boy, the thrid had no boys, etc.\nIn these exercises we don’t care about birth order, so we’ll pool all the births into one long 200-birth vector:\nWhat proportion of births were boys?"
  },
  {
    "objectID": "rethinking/03-practice.html#h1",
    "href": "rethinking/03-practice.html#h1",
    "title": "Chapter 3 exercises",
    "section": "3H1",
    "text": "3H1\n\nUsing grid approximation, compute the posterior distribution for the probability of being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?\n\n\nbirth_grid <- tibble(p_grid = seq(0, 1, length.out = 1001),\n                     prior_uniform = 1) %>% \n  mutate(likelihood = dbinom(n_boys, size = total_births, prob = p_grid)) %>% \n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform))\nbirth_grid\n## # A tibble: 1,001 × 4\n##    p_grid prior_uniform likelihood posterior_uniform\n##     <dbl>         <dbl>      <dbl>             <dbl>\n##  1  0                 1  0                 0        \n##  2  0.001             1  2.48e-275         4.98e-276\n##  3  0.002             1  5.89e-242         1.18e-242\n##  4  0.003             1  1.89e-222         3.81e-223\n##  5  0.004             1  1.28e-208         2.57e-209\n##  6  0.005             1  6.68e-198         1.34e-198\n##  7  0.006             1  3.76e-189         7.56e-190\n##  8  0.007             1  9.28e-182         1.86e-182\n##  9  0.008             1  2.32e-175         4.66e-176\n## 10  0.009             1  1.01e-169         2.03e-170\n## # … with 991 more rows\n\n\nbirth_grid %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_area(fill = clrs[6])\n\n\n\n\nParameter that maximizes the probability:\n\nbirth_grid %>% \n  filter(posterior_uniform == max(posterior_uniform))\n## # A tibble: 1 × 4\n##   p_grid prior_uniform likelihood posterior_uniform\n##    <dbl>         <dbl>      <dbl>             <dbl>\n## 1  0.555             1     0.0567            0.0114\n\n\nWith brms\n\nmodel_births <- brm(\n  bf(boy | trials(total_births) ~ 0 + Intercept),\n  data = list(boy = n_boys, total_births = total_births),\n  family = binomial(link = \"identity\"),\n  prior = prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_births %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye(fill = clrs[6])\n\n\n\n\n\nmodel_births %>% \n  gather_draws(b_Intercept) %>% \n  summarize(median = median(.value))\n## # A tibble: 1 × 2\n##   .variable   median\n##   <chr>        <dbl>\n## 1 b_Intercept  0.555"
  },
  {
    "objectID": "rethinking/03-practice.html#h2",
    "href": "rethinking/03-practice.html#h2",
    "title": "Chapter 3 exercises",
    "section": "3H2",
    "text": "3H2\n\nUsing the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals.\n\n\nbirth_samples <- sample(birth_grid$p_grid, prob = birth_grid$posterior_uniform, \n                        size = 10000, replace = TRUE)\n\ntibble(x = birth_samples) %>% \n  ggplot(aes(x = x)) +\n  stat_slab(\n    aes(fill_ramp = stat(level)),\n        .width = c(0.02, 0.5, 0.89, 0.97, 1),\n    fill = clrs[3]\n  ) +\n  scale_fill_ramp_discrete(range = c(0.2, 1), guide = \"none\")\n\n\n\n\nHDInterval::hdi(birth_samples, credMass = 0.5)\n## lower upper \n## 0.530 0.577 \n## attr(,\"credMass\")\n## [1] 0.5\nHDInterval::hdi(birth_samples, credMass = 0.89)\n## lower upper \n## 0.494 0.605 \n## attr(,\"credMass\")\n## [1] 0.89\nHDInterval::hdi(birth_samples, credMass = 0.97)\n## lower upper \n## 0.473 0.626 \n## attr(,\"credMass\")\n## [1] 0.97\n\n\nWith brms\n\nmodel_births %>% \n  spread_draws(b_Intercept) %>% \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.97))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.555  0.532  0.578   0.5  median hdci     \n## 2       0.555  0.501  0.612   0.89 median hdci     \n## 3       0.555  0.480  0.629   0.97 median hdci\n\n\nmodel_births %>% \n  tidy_draws() %>% \n  ggplot(aes(x = b_Intercept)) +\n  stat_slab(\n    aes(fill_ramp = stat(level)),\n        .width = c(0.02, 0.5, 0.89, 0.97, 1),\n    fill = clrs[3]\n  ) +\n  scale_fill_ramp_discrete(range = c(0.2, 1), guide = \"none\")"
  },
  {
    "objectID": "rethinking/03-practice.html#h3",
    "href": "rethinking/03-practice.html#h3",
    "title": "Chapter 3 exercises",
    "section": "3H3",
    "text": "3H3\n\nUse rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a acount of boyts out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). … Does it look like the model fits the data well? That is, does the distribution of predcitions include the actual observation as a central, likely outcome?\n\nLooks good!\n\n# Posterior predictive distribution\nposterior_pred_births <- rbinom(10000, size = 200, prob = birth_samples)\n\nposterior_pred_births %>% \n  enframe() %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[1]) +\n  geom_vline(xintercept = n_boys, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys ({n_boys})\"))\n\n\n\n\n\nWith brms\n\nmodel_births %>% \n  predicted_draws(newdata = tibble(total_births = 200)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[1]) +\n  geom_vline(xintercept = n_boys, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys ({n_boys})\"))"
  },
  {
    "objectID": "rethinking/03-practice.html#h4",
    "href": "rethinking/03-practice.html#h4",
    "title": "Chapter 3 exercises",
    "section": "3H4",
    "text": "3H4\n\nNow compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?\n\nWe need to just look at first births:\n\nn_boys_first <- sum(birth1)\ntotal_births_first <- length(birth1)\n\nbirth_grid_first <- tibble(p_grid = seq(0, 1, length.out = 1001),\n                           prior_uniform = 1) %>% \n  mutate(likelihood = dbinom(n_boys_first, size = total_births_first, prob = p_grid)) %>% \n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform))\n\nfirst_samples <- sample(birth_grid_first$p_grid, prob = birth_grid_first$posterior_uniform, \n                        size = 10000, replace = TRUE)\n\nposterior_pred_first <- rbinom(10000, size = 100, prob = first_samples)\n\nposterior_pred_first %>% \n  enframe() %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[6]) +\n  geom_vline(xintercept = n_boys_first, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of firstborn boys ({n_boys_first})\"))\n\n\n\n\nLooks good still!\n\nmodel_births_first <- brm(\n  bf(boy | trials(total_births) ~ 0 + Intercept),\n  data = list(boy = n_boys_first, total_births = total_births_first),\n  family = binomial(link = \"identity\"),\n  prior = prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, cores = 4\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n\n\nWith brms\nLooks the same with brms too:\n\nmodel_births_first %>% \n  predicted_draws(newdata = tibble(total_births = 100)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[6]) +\n  geom_vline(xintercept = n_boys_first, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of firstborn boys ({n_boys_first})\"))"
  },
  {
    "objectID": "rethinking/03-practice.html#h5",
    "href": "rethinking/03-practice.html#h5",
    "title": "Chapter 3 exercises",
    "section": "3H5",
    "text": "3H5\n\nThe model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated conts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?\n\n\nn_girls_first <- length(birth1) - sum(birth1)\nn_boys_after_girls <- all_births %>%\n  filter(birth1 == 0) %>%  # All families with a firstborn girl\n  summarize(boy_after_girl = sum(birth2)) %>% \n  pull(boy_after_girl)\n\nposterior_pred_first_girl <- rbinom(10000, size = n_girls_first, prob = first_samples)\n\nposterior_pred_first_girl %>% \n  enframe() %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[5]) +\n  geom_vline(xintercept = n_boys_after_girls, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys born after girls ({n_boys_after_girls})\"))\n\n\n\n\n\nWith brms\n\nmodel_births_first_girls <- brm(\n  bf(girl | trials(total_births) ~ 0 + Intercept),\n  data = list(girl = n_girls_first, total_births = total_births_first),\n  family = binomial(link = \"identity\"),\n  prior = prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, cores = 4\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_births_first_girls %>% \n  predicted_draws(newdata = tibble(total_births = n_girls_first)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[5]) +\n  geom_vline(xintercept = n_boys_after_girls, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys born after girls ({n_boys_after_girls})\"))"
  },
  {
    "objectID": "rethinking/03-video.html#linear-regression",
    "href": "rethinking/03-video.html#linear-regression",
    "title": "Video #3 code",
    "section": "Linear regression",
    "text": "Linear regression\nThe general process for drawing the linear regression owl:\n\nQuestion/goal/estimand\nScientific model\nStatistical model(s)\nValidate model\nAnalyze data\n\n\n1. Question/goal/estimand\nWe want to describe the association between adult weight and height\n\nggplot(d, aes(x = height, y = weight)) +\n  geom_point(color = clrs[3])\n\n\n\n\n\n\n2. Scientific model\nHow does height influence weight?\nHeight has a causal relationship with weight:\n\\[\nH \\rightarrow W\n\\]\nWeight is a function of height. Plug in some value of height, get some weight:\n\\[\nW = f(H)\n\\]\nWe need to write down that model somehow. The normal \\(y = mx + b\\) way of writing a linear model looks like this, where \\(\\alpha\\) is the intercept and \\(\\beta\\) is the slope:\n\\[\ny_i = \\alpha + \\beta x_i\n\\]\nWe can also write it like this, where \\(\\mu\\) is the expectation (\\(E(y \\mid x) = \\mu\\)), and \\(\\sigma\\) is the standard deviation:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\n\\end{aligned}\n\\]\nSo, we can think of a generative model for height causing weight like this:\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta H_i\n\\end{aligned}\n\\]\nThis can map directly to code:\n\nset.seed(1234)\n\nalpha <- 0\nbeta <- 0.5\nsigma <- 5\nn_individuals <- 100\n\nfake_people <- tibble(height = runif(n_individuals, 130, 170),\n                      mu = alpha + beta*height,\n                      weight = rnorm(n_individuals, mu, sigma))\n\nlm(weight ~ height, data = fake_people) %>% \n  tidy()\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    1.22     6.37       0.191 8.49e- 1\n## 2 height         0.494    0.0431    11.5   7.64e-20\n\nggplot(fake_people, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n3: Statistical model\n\nSimulating and checking the priors\nWe don’t actually know \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) - they all have priors and limits and bounds. For instance, \\(\\sigma\\) is a scale parameter - it shifts distributions up and down - it has to be positive (hence the uniform distribution here). We can write a generic model like this:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 1)\n\\end{aligned}\n\\]\nBut if we sample from that prior distribution, we get lines that are all over the place!\n\nset.seed(1234)\n\nn_samples <- 10\n\ntibble(alpha = rnorm(n_samples, 0, 1),\n       beta = rnorm(n_samples, 0, 1)) %>% \n  ggplot() +\n  geom_abline(aes(slope = beta, intercept = alpha)) +\n  xlim(c(-2, 2)) + ylim(c(-2, 2))\n\n\n\n\nInstead, we can set some more specific priors and rescale variables so that the intercept makes more sense.\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (H_i - \\bar{H}) \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\nHere’s what those priors look like:\n\nPlain old ggplotbrms::prior() and ggdist::parse_dist()\n\n\n\nplot_prior_alpha <- ggplot() +\n  stat_function(fun = ~dnorm(., 60, 10), geom = \"area\", fill = clrs[1]) +\n  xlim(c(30, 90)) +\n  labs(title = \"Normal(60, 10)\", subtitle = \"Prior for intercept (α)\", caption = \"Average adult weight\")\n\nplot_prior_beta <- ggplot() +\n  stat_function(fun = ~dnorm(., 0, 10), geom = \"area\", fill = clrs[2]) +\n  xlim(c(-30, 30)) +\n  labs(title = \"Normal(0, 10)\", subtitle = \"Prior for slope (β)\", caption = \"kg per cm\")\n\nplot_prior_sigma <- ggplot() +\n  stat_function(fun = ~dunif(., 0, 10), geom = \"area\", fill = clrs[3]) +\n  xlim(c(0, 10)) +\n  labs(title = \"Uniform(0, 10)\", subtitle = \"Prior for sd (σ)\")\n\nplot_prior_alpha | plot_prior_beta | plot_prior_sigma\n\n\n\n\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\npriors %>% \n  parse_dist() %>% \n  ggplot(aes(y = 0, dist = .dist, args = .args, fill = prior)) +\n  stat_slab(normalize = \"panels\") +\n  scale_fill_manual(values = clrs[1:3]) +\n  facet_wrap(vars(prior), scales = \"free_x\")\n\n\n\n\n\n\n\nLet’s see how well those priors work!\n\nManual plottingbrms and sample_prior = \"only\"\n\n\n\nset.seed(1234)\n\nn <- 100\nHbar <- 150\nHseq <- seq(130, 170, length.out = 30)\n\ntibble(alpha = rnorm(n, 60, 10),\n       beta = rnorm(n, 0, 10)) %>% \n  mutate(weight = map2(alpha, beta, ~.x + .y*(Hseq - Hbar)),\n         height = list(Hseq),\n         id = 1:n) %>% \n  unnest(c(weight, height)) %>% \n  ggplot(aes(x = height, y = weight)) + \n  geom_line(aes(group = id), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_prior_only_normal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\"\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 100)) %>% \n  add_epred_draws(height_weight_prior_only_normal, ndraws = 100) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\ndraws_prior %>% \n  ggplot(aes(x = height_unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\nLots of those lines are completely implausible, so we’ll use a lognormal β prior instead:\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (H_i - \\bar{H}) \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\nThe lognormal distribution forces βs to be > 0 and it’s clusttered down at low values like 1ish:\n\nggplot() +\n  stat_function(fun = ~dlnorm(., 0, 1), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 5)) +\n  labs(x = \"Simulated β values\", y = \"Density\")\n\n\n\n\n\nManual plottingbrms and sample_prior = \"only\"\n\n\n\nset.seed(1234)\n\nn <- 100\nHbar <- 150\nHseq <- seq(130, 170, length.out = 30)\n\ntibble(alpha = rnorm(n, 60, 10),\n       beta = rlnorm(n, 0, 1)) %>% \n  mutate(weight = map2(alpha, beta, ~.x + .y*(Hseq - Hbar)),\n         height = list(Hseq),\n         id = 1:n) %>% \n  unnest(c(weight, height)) %>% \n  ggplot(aes(x = height, y = weight)) + \n  geom_line(aes(group = id), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_prior_only_lognormal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 100)) %>% \n  add_epred_draws(height_weight_prior_only_lognormal, ndraws = 100) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\ndraws_prior %>% \n  ggplot(aes(x = height_unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\n\n\nFitting the model\nFitting the model is super complex because it’s the joint posterior of all those distributions:\n\\[\n\\begin{aligned}\n\\operatorname{Pr}(\\alpha, \\beta, \\sigma \\mid W, H) \\propto~ & \\mathcal{N}(W \\mid \\mu, \\sigma) \\\\\n&\\times \\mathcal{N}(\\alpha \\mid 60, 10) \\\\\n&\\times \\operatorname{LogNormal}(\\beta \\mid 0, 1) \\\\\n&\\times \\operatorname{Uniform}(\\sigma \\mid 0, 10)\n\\end{aligned}\n\\]\nIf we do this with grid approximation, 100 values of each parameter = 1 million calculations. In the book he shows it with grid approximation and with quadratic approximation.\nI’ll do it with brms and Stan instead.\n\nbrmsStan\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_lognormal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n\n\nprint(height_weight_lognormal)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 1 + height_z \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    45.05      0.23    44.61    45.50 1.00     3979     3023\n## height_z      4.83      0.23     4.38     5.29 1.00     3802     2716\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.27      0.17     3.96     4.62 1.00     4119     2987\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nThis is a little trickier because add_epred_draws() and predicted_draws() and all those nice functions don’t work with raw Stan samples, by design.\nInstead, we need to generate predictions that incorporate sigma using a generated quantities block in Stan (see the official Stan documentation and this official example and this neat blog post by Monica Alexander and this Medium post).\nHere we return two different but similar things: weight_rep[i] for the posterior predictive (which corresponds to predicted_draws() in tidybayes) and mu[i] for the expectation of the posterior predictive (which corresponds to epred_draws() in tidybayes):\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] height;  // Explanatory variable\n  vector[n] weight;  // Outcome variable\n  real height_bar;   // Average height\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  real beta;\n  real alpha;\n}\n\ntransformed parameters {\n  // Regression-y model of mu with scaled height\n  vector[n] mu;\n  mu = alpha + beta * (height - height_bar);\n}\n\nmodel {\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  alpha ~ normal(60, 10);\n  beta ~ lognormal(0, 1);\n  sigma ~ uniform(0, 10);\n}\n\ngenerated quantities {\n  // Generate a posterior predictive distribution\n  vector[n] weight_rep;\n\n  for (i in 1:n) {\n    // Calculate a new predicted mu for each iteration here\n    real mu_hat_n = alpha + beta * (height[i] - height_bar);\n    weight_rep[i] = normal_rng(mu_hat_n, sigma);\n    \n    // Alternatively, we can use mu[i] from the transformed parameters\n    // weight_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n\n\n# compose_data listifies things for Stan\n# stan_data <- d %>% compose_data()\n# stan_data$height_bar <- mean(stan_data$height)\n\n# Or we can manually build the list\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  height_bar = mean(d$height))\n\nmodel_lognormal_stan <- rstan::sampling(\n  object = lognormal_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_lognormal_stan, \n      pars = c(\"sigma\", \"beta\", \"alpha\", \"mu[1]\", \"mu[2]\"))\n## Inference for Stan model: 3c0d0064926253eea8547fd513e8019e.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  4.27       0 0.16  3.97  4.16  4.27  4.38  4.60 13529    1\n## beta   0.62       0 0.03  0.57  0.60  0.62  0.64  0.68 15170    1\n## alpha 45.06       0 0.23 44.61 44.90 45.05 45.21 45.51 15781    1\n## mu[1] 43.26       0 0.24 42.78 43.09 43.26 43.42 43.74 15594    1\n## mu[2] 35.72       0 0.50 34.73 35.39 35.72 36.06 36.69 15326    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep  8 13:44:25 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\n\n\n\n\n\n\n4 & 5: Validate model and analyze data\n\nbrmsStan\n\n\nExpectation of the posterior (plotting uncertainty of the mean):\n\ndraws_posterior_epred <- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 100)) %>% \n  add_epred_draws(height_weight_lognormal, ndraws = 50) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  geom_line(data = draws_posterior_epred,\n            aes(x = height_unscaled, y = .epred, group = .draw), alpha = 0.2, color = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nPosterior predictions (plotting uncertainty of the predictions):\n\ndraws_posterior_pred <- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 500)) %>% \n  add_predicted_draws(height_weight_lognormal, ndraws = 100) %>%\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  stat_lineribbon(data = draws_posterior_pred,\n                  aes(x = height_unscaled, y = .prediction), .width = 0.95, \n                  alpha = 0.2, color = clrs[5], fill = clrs[5]) +\n  coord_cartesian(ylim = c(30, 65))\n## Warning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n## Warning: Unknown or uninitialised column: `linewidth`.\n## Warning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\nAnd here’s the posterior predictive check, just for fun:\n\npp_check(height_weight_lognormal, type = \"dens_overlay\", ndraws = 25)\n\n\n\n\n\n\nSummarized predictions:\n\npredicted_values_lognormal <- model_lognormal_stan %>% \n  spread_draws(mu[i], weight_rep[i]) %>%\n  mean_qi() %>%\n  mutate(weight = d$weight,\n         height = d$height)\n\nExpectation of the posterior (plotting uncertainty of the mean):\n\nggplot(predicted_values_lognormal, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_line(aes(y = mu), color = clrs[6]) +\n  geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.2, fill = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nPosterior predictions (plotting uncertainty of the predictions):\n\nggplot(predicted_values_lognormal, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_line(aes(y = mu), color = clrs[5]) +\n  geom_ribbon(aes(ymin = weight_rep.lower, ymax = weight_rep.upper), alpha = 0.2, fill = clrs[5]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nAnd here’s the posterior predictive check, just for fun:\n\nyrep1 <- rstan::extract(model_lognormal_stan)[[\"weight_rep\"]]\nsamp25 <- sample(nrow(yrep1), 25)\nbayesplot::ppc_dens_overlay(d$weight, yrep1[samp25, ])"
  },
  {
    "objectID": "rethinking/04-video.html",
    "href": "rethinking/04-video.html",
    "title": "Video #4 code",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(ggdag)\nlibrary(splines)\nlibrary(lubridate)\nlibrary(patchwork)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)\n\n# Data\ndata(Howell1, package = \"rethinking\")\n\nd <- Howell1 %>% \n  filter(age > 18) %>% \n  # Stan doesn't like working with columns with attributes, but I want to keep\n  # the attributes for unscaling later, so there are two scaled height columns\n  mutate(height_scaled = scale(height),\n         height_z = as.numeric(height_scaled)) %>% \n  mutate(sex = factor(male),\n         sex_nice = factor(male, labels = c(\"Female\", \"Male\")))\n\nheight_scale <- attributes(d$height_scaled) %>% \n  set_names(janitor::make_clean_names(names(.)))\n\n\nhead(Howell1)\n##    height   weight age male\n## 1 151.765 47.82561  63    1\n## 2 139.700 36.48581  63    0\n## 3 136.525 31.86484  65    0\n## 4 156.845 53.04191  41    1\n## 5 145.415 41.27687  51    0\n## 6 163.830 62.99259  35    1\n\nggplot(d, aes(x = height, y = weight, color = sex_nice)) +\n  geom_point() +\n  scale_color_manual(values = clrs[1:2]) +\n  labs(x = \"Height (cm)\", y = \"Weight (kg)\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nSex only\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{S[i]} \\\\\n\\\\\n\\alpha_j &\\sim \\mathcal{N}(60, 10) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\n\nbrmsStan\n\n\nCreate a model with no intercept; use a factor version of sex to get the indexes like he does with \\(\\alpha_{S[i]}\\).\n\npriors <- c(prior(normal(60, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nsex_weight <- brm(\n  bf(weight ~ 0 + sex),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nsex_weight\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 0 + sex \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sex0    41.87      0.41    41.08    42.65 1.00     3580     3109\n## sex1    48.63      0.43    47.81    49.49 1.00     4090     2920\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     5.52      0.22     5.12     5.98 1.00     4135     2820\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPosterior mean weights:\n\nsw_post_means <- sex_weight %>% \n  gather_draws(b_sex0, b_sex1)\n\nsw_post_means %>% \n  mean_hdci()\n## # A tibble: 2 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 b_sex0      41.9   41.1   42.7   0.95 mean   hdci     \n## 2 b_sex1      48.6   47.7   49.4   0.95 mean   hdci\n\nsw_post_means %>% \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye() +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior mean weight (kg)\\n(Coefficient for sex)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior mean contrast in weights:\n\nsw_post_means_wide <- sex_weight %>% \n  spread_draws(b_sex0, b_sex1) %>% \n  mutate(diff = b_sex1 - b_sex0)\n\nsw_post_means_wide %>% \n  select(diff) %>% \n  mean_hdci()\n## # A tibble: 1 × 6\n##    diff .lower .upper .width .point .interval\n##   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1  6.77   5.63   7.95   0.95 mean   hdci\n\nsw_post_means_wide %>% \n  ggplot(aes(x = diff)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nPosterior predicted weights:\n\nsw_post_pred <- tibble(sex = c(\"0\", \"1\")) %>% \n  add_predicted_draws(sex_weight, ndraws = 1000)\n\nsw_post_pred %>% \n  mean_hdci()\n## # A tibble: 2 × 8\n##   sex    .row .prediction .lower .upper .width .point .interval\n##   <chr> <int>       <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 0         1        41.6   30.7   50.9   0.95 mean   hdci     \n## 2 1         2        49.0   38.2   59.3   0.95 mean   hdci\n\nsw_post_pred %>% \n  ungroup() %>% \n  mutate(sex_nice = factor(sex, labels = c(\"Female\", \"Male\"))) %>% \n  ggplot(aes(x = .prediction, fill = sex_nice)) +\n  stat_halfeye(alpha = 0.75) +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior predicted weight (kg)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior predicted contrast in weights:\n\nsw_post_pred_diff <- tibble(sex = c(\"0\", \"1\")) %>% \n  add_predicted_draws(sex_weight, ndraws = 1000) %>% \n  compare_levels(variable = .prediction, by = sex)\n\nsw_post_pred_diff %>% \n  mean_hdci()\n## # A tibble: 1 × 7\n##   sex   .prediction .lower .upper .width .point .interval\n##   <chr>       <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 1 - 0        7.21  -7.29   21.9   0.95 mean   hdci\n\nsw_post_pred_diff %>% \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\nsw_stan.stan\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] weight;\n  int sex[n];\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  vector[2] a;\n}\n\ntransformed parameters {\n  vector[n] mu;\n  mu = a[sex];\n}\n\nmodel {\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  sigma ~ uniform(0, 10);\n  a ~ normal(60, 10);\n}\n\ngenerated quantities {\n  real diff;\n  matrix[n, 2] weight_rep;\n  vector[n] diff_rep;\n  \n  // Calculate the contrasts / difference between group means\n  diff = a[2] - a[1];\n  \n  // Generate a posterior predictive distribution for each sex\n  // To do this we have to create a matrix, with a column per sex\n  for (j in 1:2) {\n    for (i in 1:n) {\n      weight_rep[i, j] = normal_rng(a[j], sigma);\n    }\n  }\n  \n  // Generate a posterior predictive distribution of group contrasts\n  for (i in 1:n) {\n    diff_rep[i] = normal_rng(a[2], sigma) - normal_rng(a[1], sigma);\n  }\n}\n\n\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  sex = d$male + 1)\n\nmodel_sw_stan <- rstan::sampling(\n  object = sw_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_sw_stan,\n      pars = c(\"sigma\", \"a[1]\", \"a[2]\", \"diff\"))\n## Inference for Stan model: ce4f92ccf1e48f603c1b01a6bf1ee94d.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  5.52       0 0.21  5.12  5.37  5.51  5.66  5.96 14336    1\n## a[1]  41.86       0 0.41 41.05 41.58 41.86 42.14 42.67 14671    1\n## a[2]  48.63       0 0.43 47.79 48.34 48.64 48.92 49.49 15314    1\n## diff   6.77       0 0.60  5.60  6.37  6.77  7.17  7.94 14697    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep 15 01:54:27 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\nPosterior mean weights:\n\nsw_stan_post_means <- model_sw_stan %>% \n  gather_draws(a[i])\n## Warning: `gather_()` was deprecated in tidyr 1.2.0.\n## Please use `gather()` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\nsw_stan_post_means %>% \n  mean_hdci()\n## # A tibble: 2 × 8\n##       i .variable .value .lower .upper .width .point .interval\n##   <int> <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1     1 a           41.9   41.1   42.7   0.95 mean   hdci     \n## 2     2 a           48.6   47.7   49.4   0.95 mean   hdci\n\nsw_stan_post_means %>% \n  ungroup() %>% \n  mutate(nice_i = factor(i, labels = c(\"a_female\", \"a_male\"))) %>% \n  ggplot(aes(x = .value, fill = nice_i)) +\n  stat_halfeye() +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior mean weight (kg)\\n(Coefficient for sex)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior mean contrast in weights:\n\nsw_stan_post_diff_means <- model_sw_stan %>% \n  gather_draws(diff)\n\nsw_stan_post_diff_means %>% \n  mean_hdci()\n## # A tibble: 1 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 diff        6.77   5.58   7.92   0.95 mean   hdci\n\nsw_stan_post_diff_means %>% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nPosterior predicted weights:\n\npredicted_weights_stan <- model_sw_stan %>% \n  spread_draws(weight_rep[i, sex])\n\npredicted_weights_stan %>% \n  group_by(sex) %>% \n  mean_hdci()\n## # A tibble: 2 × 10\n##     sex     i i.lower i.upper weight_rep weight_…¹ weigh…² .width .point .inte…³\n##   <int> <dbl>   <int>   <int>      <dbl>     <dbl>   <dbl>  <dbl> <chr>  <chr>  \n## 1     1  174.       1     329       41.9      30.9    52.7   0.95 mean   hdci   \n## 2     2  174.       1     329       48.6      37.8    59.5   0.95 mean   hdci   \n## # … with abbreviated variable names ¹​weight_rep.lower, ²​weight_rep.upper,\n## #   ³​.interval\n\npredicted_weights_stan %>% \n  ungroup() %>% \n  mutate(sex_nice = factor(sex, labels = c(\"Female\", \"Male\"))) %>% \n  ggplot(aes(x = weight_rep, fill = sex_nice)) +\n  stat_halfeye(alpha = 0.75) +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior predicted weight (kg)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior predicted contrast in weights:\n\nsw_post_pred_diff_stan <- model_sw_stan %>% \n  gather_draws(diff_rep[i])\n\nsw_post_pred_diff_stan %>%\n  group_by(.variable) %>%\n  mean_hdci() %>% \n  select(-starts_with(\"i\"))\n## # A tibble: 1 × 7\n##   .variable .value .value.lower .value.upper .width .point .interval\n##   <chr>      <dbl>        <dbl>        <dbl>  <dbl> <chr>  <chr>    \n## 1 diff_rep    6.77        -8.63         22.1   0.95 mean   hdci\n\nsw_post_pred_diff_stan %>% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\n\n\n\nSex + height\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{S[i]} + \\beta_{S[i]}(H_i - \\bar{H}) \\\\\n\\\\\n\\alpha_j &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta_j &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\n\nbrmsStan\n\n\nThis is the wonkiest syntax ever, but it works! We can hack the nl capabilities of brms to create indexed parameters.\nAlternatively you can avoid this nl syntax! Use bf(weight ~ 0 + sex + sex:height_z), instead. Note the : for the interaction term instead of the more standard *. If you use *, you’ll get a more standard interaction term (i.e. the change in the slope when one group is active); if you use :, you’ll get slopes for each group. It’s a little subtlety in R’s formula syntax. The * is a shortcut for complete crossing of the terms, so x * z really turns into x + z + x:z behind the scenes. The : only does the interaction of the two terms, so that x:z is just \\(x \\times z\\).\n\npriors <- c(prior(normal(60, 10), class = b, nlpar = a),\n            prior(lognormal(0, 1), class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex <- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_height_sex\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 0 + a + b * height_z \n##          a ~ 0 + sex\n##          b ~ 0 + sex\n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## a_sex0    45.10      0.46    44.19    45.99 1.00     3021     3038\n## a_sex1    45.19      0.47    44.30    46.11 1.00     2828     2799\n## b_sex0     4.90      0.50     3.88     5.89 1.00     3018     2442\n## b_sex1     4.65      0.45     3.79     5.53 1.00     2964     2861\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.28      0.17     3.97     4.62 1.00     3600     2789\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nsex_height_weight_post_pred <- expand_grid(\n  height_z = seq(min(d$height_z), max(d$height_z), length.out = 50),\n  sex = 0:1\n) %>% \n  add_predicted_draws(model_height_sex) %>%\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"0\", \"1\"))) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nOverall distribution of predictive posterior contrasts:\n\nggplot(sex_height_weight_post_pred, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nDistribution of predictive posterior contrasts across range of heights:\n\nggplot(sex_height_weight_post_pred, aes(x = height_unscaled, y = .prediction)) +\n  stat_lineribbon(aes(fill_ramp = stat(.width)), .width = ppoints(50),\n                  fill = clrs[3], color = colorspace::darken(clrs[3], 0.5), \n                  show.legend = FALSE) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_ramp_continuous(from = \"transparent\", range = c(1, 0)) +\n  labs(x = \"Height (cm)\", y = \"Posterior weight contrast (kg)\\nWomen − Men\")\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Stan code from Rethinking models\n\n\n\n\n\nThe ulam() function is super helpful for translating McElreath’s quap() syntax into Stan!\n\nm_SHW <- rethinking::quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu <- a[S] + b[S] * (H - Hbar),\n    a[S] ~ dnorm(60, 10),\n    b[S] ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 10)\n  ), \n  data = list(\n    W = d$weight,\n    H = d$height,\n    Hbar = mean(d$height),\n    S = d$male + 1\n  )\n)\n\ncat(rethinking::ulam(m_SHW, sample = FALSE)$model)\n## data{\n##     vector[346] W;\n##     real Hbar;\n##     vector[346] H;\n##     int S[346];\n## }\n## parameters{\n##     vector[2] a;\n##     vector<lower=0>[2] b;\n##     real<lower=0,upper=10> sigma;\n## }\n## model{\n##     vector[346] mu;\n##     sigma ~ uniform( 0 , 10 );\n##     b ~ lognormal( 0 , 1 );\n##     a ~ normal( 60 , 10 );\n##     for ( i in 1:346 ) {\n##         mu[i] = a[S[i]] + b[S[i]] * (H[i] - Hbar);\n##     }\n##     W ~ normal( mu , sigma );\n## }\n\n\n\n\nsex_height.stan\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] weight;\n  vector[n] height;\n  int sex[n];\n}\n\ntransformed data {\n  // Center and standardize height\n  vector[n] height_z;\n  height_z = (height - mean(height)) / sd(height);\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  vector[2] a;\n  vector<lower=0>[2] b;\n}\n\nmodel {\n  vector[n] mu;\n  \n  // Model for mu with intercepts (a) and coefficients (b) for each sex\n  for (i in 1:n) {\n    mu[i] = a[sex[i]] + b[sex[i]] * height_z[i];\n  }\n\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  sigma ~ uniform(0, 10);\n  a ~ normal(60, 10);\n  b ~ lognormal(0, 1);\n}\n\ngenerated quantities {\n  matrix[n, 2] weight_rep;\n  vector[n] diff_rep;\n  \n  // Generate a posterior predictive distribution for each sex\n  // To do this we have to create a matrix, with a column per sex\n  for (j in 1:2) {\n    for (i in 1:n) {\n      real mu_hat_n = a[sex[i]] + b[sex[i]] * height_z[i];\n      weight_rep[i, j] = normal_rng(mu_hat_n, sigma);\n    }\n  }\n  \n  // Generate a posterior predictive distribution of group contrasts\n  for (i in 1:n) {\n    diff_rep[i] = weight_rep[i, 1] - weight_rep[i, 2];\n  }\n}\n\n\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  sex = d$male + 1)\n\nmodel_sex_height_stan <- rstan::sampling(\n  object = sex_height_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_sex_height_stan,\n      pars = c(\"sigma\", \"a[1]\", \"a[2]\", \"b[1]\", \"b[2]\"))\n## Inference for Stan model: f8b7828023f48992dd5011f1f1fa456e.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  4.28       0 0.17  3.97  4.17  4.28  4.39  4.62 15332    1\n## a[1]  45.10       0 0.45 44.22 44.80 45.10 45.41 45.99 10776    1\n## a[2]  45.20       0 0.46 44.29 44.89 45.20 45.51 46.09 10499    1\n## b[1]   4.91       0 0.49  3.95  4.57  4.91  5.24  5.86 10718    1\n## b[2]   4.65       0 0.43  3.80  4.36  4.65  4.95  5.50 10394    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep 15 01:55:38 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\n\noriginal_hw <- tibble(height = d$height,\n                      weight = d$weight) %>% \n  mutate(i = 1:n())\n\npredicted_diffs_sex_height_stan <- model_sex_height_stan %>% \n  spread_draws(diff_rep[i]) %>% \n  left_join(original_hw, by = \"i\")\n\nOverall distribution of predictive posterior contrasts:\n\nggplot(predicted_diffs_sex_height_stan, aes(x = diff_rep)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nDistribution of predictive posterior contrasts across range of heights:\n(The y-values are way off from the video here :shrug:)\n\nggplot(predicted_diffs_sex_height_stan, aes(x = height, y = diff_rep)) +\n  stat_lineribbon(aes(fill_ramp = stat(.width)), .width = ppoints(50),\n                  fill = clrs[3], color = colorspace::darken(clrs[3], 0.5), \n                  show.legend = FALSE) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_ramp_continuous(from = \"transparent\", range = c(1, 0)) +\n  labs(x = \"Height (cm)\", y = \"Posterior weight contrast (kg)\\nWomen − Men\")\n\n\n\n\n\n\n\n\n\nFull luxury Bayes!\nGiven this DAG:\n\nheight_sex_dag <- dagify(\n  x ~ z,\n  y ~ x + z,\n  exposure = \"x\",\n  outcome = \"y\",\n  labels = c(x = \"Height\", y = \"Weight\", z = \"Sex\"),\n  coords = list(x = c(x = 1, y = 3, z = 2),\n                y = c(x = 1, y = 1, z = 2))) %>% \n  tidy_dagitty() %>% \n  node_status()\n\nggplot(height_sex_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = label), size = 3.5) +\n  scale_color_manual(values = clrs[c(1, 4)], guide = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n…what’s the causal effect of sex on weight? Or:\n\\[\nE(\\text{Weight} \\mid \\operatorname{do}(\\text{Sex}))\n\\]\nHere’s the official model:\n\\[\n\\begin{aligned}\nH_i &\\sim \\mathcal{N}(\\nu_i, \\tau) \\\\\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\nu_i &= h_{S[i]} \\\\\n\\mu_i &= \\alpha_{S[i]} + \\beta_{S[i]}(H_i - \\bar{H}) \\\\\n\\\\\nh_j &\\sim \\mathcal{N}(160, 10) \\\\\n\\alpha_j &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta_j &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma, \\tau &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\nThe results should look something like this, from the slides:\n\n\n\n\n\n\n\n\n\n\nbrmsStan\n\n\n\npriors <- c(prior(normal(60, 10), resp = weight, class = b, nlpar = a),\n            prior(lognormal(0, 1), resp = weight, class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), resp = weight, class = sigma, lb = 0, ub = 10),\n            # prior(normal(160, 10), resp = height, class = b),\n            prior(normal(0, 1), resp = heightz, class = b),\n            prior(uniform(0, 10), resp = heightz, class = sigma, lb = 0, ub = 10))\n\nmodel_luxury <- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE) + \n    bf(height_z ~ 0 + sex) + \n    set_rescor(TRUE),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_luxury\n##  Family: MV(gaussian, gaussian) \n##   Links: mu = identity; sigma = identity\n##          mu = identity; sigma = identity \n## Formula: weight ~ 0 + a + b * height_z \n##          a ~ 0 + sex\n##          b ~ 0 + sex\n##          height_z ~ 0 + sex \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## weight_a_sex0    42.58      0.64    41.52    44.07 1.01      598      285\n## weight_a_sex1    47.97      0.64    46.49    49.02 1.01      552      313\n## weight_b_sex0     1.09      0.77     0.17     3.02 1.01      565      255\n## weight_b_sex1     0.89      0.70     0.12     2.73 1.01      491      275\n## heightz_sex0     -0.66      0.05    -0.77    -0.55 1.00     2384     1681\n## heightz_sex1      0.74      0.06     0.62     0.85 1.00     2806     2528\n## \n## Family Specific Parameters: \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_weight      5.13      0.29     4.49     5.67 1.01      666      291\n## sigma_heightz     0.72      0.03     0.67     0.77 1.00     3262     1965\n## \n## Residual Correlations: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## rescor(weight,heightz)     0.54      0.09     0.32     0.65 1.01      560\n##                        Tail_ESS\n## rescor(weight,heightz)      265\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPosterior mean contrast in weights:\n\nluxury_post_mean_diff <- expand_grid(\n  height_z = seq(min(d$height_z), max(d$height_z), length.out = 50),\n  sex = 0:1\n) %>% \n  add_epred_draws(model_luxury) %>%\n  compare_levels(variable = .epred, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_mean_diff %>% \n  filter(.category == \"weight\") %>% \n  ggplot(aes(x = .epred)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nPosterior predicted contrast in weights:\n\nluxury_post_pred_diff <- expand_grid(\n  height_z = seq(min(d$height_z), max(d$height_z), length.out = 50),\n  sex = 0:1\n) %>% \n  add_predicted_draws(model_luxury) %>%\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_pred_diff %>% \n  filter(.category == \"weight\") %>% \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Stan code from Rethinking models\n\n\n\n\n\n\nm_SHW_full <- rethinking::quap(\n  alist(\n    # Weight\n    W ~ dnorm(mu, sigma),\n    mu <- a[S] + b[S] * (H - Hbar),\n    a[S] ~ dnorm(60, 10),\n    b[S] ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 10),\n    \n    # Height\n    H ~ dnorm(nu, tau),\n    nu <- h[S],\n    h[S] ~ dnorm(160, 10),\n    tau ~ dunif(0, 10)\n  ), data = list(\n    W = d$weight,\n    H = d$height,\n    Hbar = mean(d$height),\n    S = d$male + 1\n  )\n)\n\ncat(rethinking::ulam(m_SHW_full, sample = FALSE)$model)\n\n\n\n\nluxury_stan.stan\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] weight;\n  real Hbar;\n  vector[n] height;\n  int sex[n];\n}\n\nparameters {\n  // Things to estimate\n  vector[2] a;\n  vector<lower=0>[2] b;\n  real<lower=0,upper=10> sigma;\n  vector[2] h;\n  real<lower=0,upper=10> tau;\n}\n\nmodel {\n  vector[n] mu;\n  vector[n] nu;\n  \n  // Height model\n  tau ~ uniform(0, 10);\n  h ~ normal(160, 10);\n  \n  for (i in 1:n) {\n    nu[i] = h[sex[i]];\n  }\n  \n  // Weight model\n  height ~ normal(nu , tau);\n  sigma ~ uniform(0, 10);\n  b ~ lognormal(0, 1);\n  a ~ normal(60, 10);\n  \n  for (i in 1:n) {\n    mu[i] = a[sex[i]] + b[sex[i]] * (height[i] - Hbar);\n  }\n  \n  weight ~ normal(mu, sigma);\n}\n\ngenerated quantities {\n  matrix[n, 2] weight_rep;\n  matrix[n, 2] height_rep;\n  vector[n] w_do_s;\n  vector[2] mu_sex;\n  real mu_diff;\n  \n  for (i in 1:2) {\n    mu_sex[i] = a[sex[i]] + b[sex[i]] * (h[sex[i]] - Hbar);\n  }\n  \n  mu_diff = mu_sex[1] - mu_sex[2];\n  \n  // Generate a posterior predictive distribution for each sex\n  // To do this we have to create a matrix, with a column per sex\n  for (j in 1:2) {\n    for (i in 1:n) {\n      height_rep[i, j] = normal_rng(h[sex[j]], tau);\n      weight_rep[i, j] = normal_rng(a[sex[j]] + b[sex[j]] * (height_rep[i, j] - Hbar), sigma);\n    }\n  }\n  \n  // Generate a posterior predictive distribution of group contrasts\n  for (i in 1:n) {\n    w_do_s[i] = weight_rep[i, 1] - weight_rep[i, 2];\n  }\n}\n\n\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  Hbar = mean(d$height),\n                  sex = d$male + 1)\n\nmodel_luxury_stan <- rstan::sampling(\n  object = luxury_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_luxury_stan,\n      pars = c(\"a[1]\", \"a[2]\", \"b[1]\", \"b[2]\", \"sigma\", \"h[1]\", \"h[2]\", \"tau\", \n               \"mu_sex[1]\", \"mu_sex[2]\", \"mu_diff\"))\n## Inference for Stan model: 2ac72225665508fb4100c7f800818c3a.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##             mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n## a[1]       45.18       0 0.46  44.28  44.87  45.17  45.48  46.08 12392    1\n## a[2]       45.14       0 0.46  44.25  44.83  45.14  45.45  46.03 12553    1\n## b[1]        0.65       0 0.06   0.52   0.60   0.65   0.69   0.77 12075    1\n## b[2]        0.61       0 0.06   0.50   0.57   0.61   0.65   0.72 13048    1\n## sigma       4.28       0 0.16   3.97   4.17   4.28   4.39   4.62 21039    1\n## h[1]      149.50       0 0.41 148.70 149.21 149.50 149.78 150.30 22463    1\n## h[2]      160.37       0 0.44 159.52 160.08 160.37 160.67 161.22 22320    1\n## tau         5.57       0 0.21   5.18   5.43   5.56   5.71   6.01 19301    1\n## mu_sex[1]  48.63       0 0.42  47.79  48.35  48.63  48.92  49.46 21139    1\n## mu_sex[2]  41.85       0 0.42  41.04  41.57  41.85  42.13  42.67 21824    1\n## mu_diff     6.78       0 0.59   5.62   6.37   6.78   7.18   7.94 20965    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep 15 01:56:43 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\nPosterior mean contrast in weights:\n\nluxury_post_mean_diff_stan <- model_luxury_stan %>% \n  spread_draws(mu_diff)\n\nluxury_post_mean_diff_stan %>% \n  mean_hdci()\n## # A tibble: 1 × 6\n##   mu_diff .lower .upper .width .point .interval\n##     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1    6.78   5.64   7.95   0.95 mean   hdci\n\nggplot(luxury_post_mean_diff_stan, aes(x = mu_diff)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\nluxury_post_pred_diff_stan <- model_luxury_stan %>% \n  spread_draws(w_do_s[i])\n\nluxury_post_pred_diff_stan %>% \n  mean_hdci()\n## # A tibble: 346 × 7\n##        i w_do_s .lower .upper .width .point .interval\n##    <int>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n##  1     1   6.79  -8.96   21.5   0.95 mean   hdci     \n##  2     2   6.86  -8.69   22.2   0.95 mean   hdci     \n##  3     3   6.79  -8.46   22.6   0.95 mean   hdci     \n##  4     4   6.86  -9.38   21.3   0.95 mean   hdci     \n##  5     5   6.71  -8.26   22.7   0.95 mean   hdci     \n##  6     6   6.85  -8.15   22.8   0.95 mean   hdci     \n##  7     7   6.78  -8.74   22.4   0.95 mean   hdci     \n##  8     8   6.85  -8.36   22.6   0.95 mean   hdci     \n##  9     9   6.82  -8.79   21.9   0.95 mean   hdci     \n## 10    10   6.80  -8.53   21.8   0.95 mean   hdci     \n## # … with 336 more rows\n\nggplot(luxury_post_pred_diff_stan, aes(x = w_do_s)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\n\n\n\nCurvy linear models\nThe full data isn’t linear, but linear models can be fit to curvy data, but in geocentric, purposely wrong ways\n\nggplot(Howell1, aes(x = height, y = weight)) +\n  geom_point()\n\n\n\n\n\nPolynomials\nWe can use a squared term like:\n\\[\n\\mu_i = \\alpha + \\beta_1 H_i + \\beta_2 H_i^2\n\\]\nAnd that fits okay, but it does weird things on the edges of the data, like weight increasing when height gets really small\n\nggplot(Howell1, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), fullrange = TRUE) +\n  xlim(c(40, 200))\n\n\n\n\nWe can throw in more terms too:\n\\[\n\\mu_i = \\alpha + \\beta_1 H_i + \\beta_2 H_i^2 + \\beta_3 H_i^3 + \\beta_4 H_i^4\n\\]\nAnd the line fits better, but it does really weird things on the edges, like dropping precipitously after the max height:\n\nggplot(Howell1, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 4), fullrange = TRUE) +\n  xlim(c(40, 200))\n\n\n\n\nWeirdly, logging works really well because of biological reasons (that he’ll explain in chapter 19)\n\nggplot(Howell1, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", formula = y ~ x,\n              method.args = list(family = gaussian(link = \"log\"))) +\n  scale_x_log10()\n\n\n\n\n\n\nSplines\n\ndata(cherry_blossoms, package = \"rethinking\")\n\ncherry_blossoms <- cherry_blossoms %>% \n  filter(complete.cases(doy)) %>% \n  mutate(idx = 1:n()) %>% \n  arrange(year)\n\nnum_knots <- 3\nknot_list <- seq(from = min(cherry_blossoms$year), \n                 to = max(cherry_blossoms$year), \n                 length.out = num_knots)\n\ncherry_splines <- cherry_blossoms %>% \n  nest(data = everything()) %>% \n  mutate(basis_matrix = purrr::map(data, ~{\n    t(bs(.$year, knots = knot_list, degree = 2, intercept = FALSE))\n  })) %>% \n  mutate(weights = purrr::map(basis_matrix, ~rep(c(1,-1), length = nrow(.)))) \n\ncherry_splines_mu <- cherry_splines %>% \n  mutate(mu = purrr::map2(weights, basis_matrix, ~as.vector(.x %*% .y))) %>% \n  unnest(c(data, mu))\n\ncherry_basis <- cherry_splines %>% \n  unnest(weights) %>% \n  mutate(row = 1:n()) %>% \n  filter(row != 5) %>% \n  mutate(basis = purrr::pmap(list(basis_matrix, weights, row), ~{\n    ..1[..3,] * ..2\n  })) %>% \n  mutate(row = glue::glue(\"Basis {row} (w = {weights})\")) %>% \n  unnest(c(data, basis)) \n  \nggplot(data = cherry_splines_mu, aes(x = year)) +\n  geom_line(aes(y = mu), size = 2) +\n  geom_line(data = cherry_basis, aes(y = basis, color = row), size = 1) +\n  labs(y = \"Weighted basis function\", color = NULL) +\n  theme(axis.text.x = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank())\n\n\n\n\nPlot from book cover, for fun:\n\nmodel_doy <- brm(\n  bf(doy ~ 1 + s(year, bs = \"bs\", k = 30)),\n  family = gaussian(),\n  data = cherry_blossoms,\n  prior = c(prior(normal(100, 10), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(student_t(3, 0, 5.9), class = sds),\n                prior(exponential(1), class = sigma)),\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n\n\nplot_doy <- cherry_blossoms %>% \n  add_epred_draws(model_doy) %>% \n  summarize(mean_hdci(.epred, .width = 0.89)) %>% \n  ungroup() %>% \n  mutate(day_of_year = as.Date(doy, origin = \"2021-12-31\"))\n\npanel_bottom <- plot_doy %>% \n  ggplot(aes(x = year)) +\n  geom_point(aes(y = doy), pch = 8, size = 3.5, color = \"#FF4136\", alpha = 0.5) +\n  geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = \"#111111\", alpha = 0.5) +\n  scale_x_continuous(breaks = c(900, 1400, 2000),\n                     labels = function(x) paste(x, \"CE\")) +\n  scale_y_continuous(labels = function(y) format(as.Date(y, origin = \"2021-12-31\"), \"%B %e\"),\n                     breaks = yday(ymd(c(\"2022-04-01\", \"2022-05-01\")))) +\n  labs(x = NULL, y = \"Day of first blossom\")\npanel_bottom\n\n\n\n\nThe cover uses imputed data for the missing values. I’m assuming that’ll get covered later in the book, but in the meantime, here’s a partial version of the top panel of the front cover plot:\n\npanel_top <- cherry_blossoms %>% \n  drop_na(temp) %>% \n  ggplot(aes(x = year, y = temp)) +\n  geom_point(color = \"#001f3f\", size = 3, alpha = 0.3) +\n  scale_x_continuous(breaks = c(900, 1400, 2000),\n                     labels = function(x) paste(x, \"CE\")) +\n  scale_y_continuous(labels = function(y) paste0(y, \"°C\"),\n                     breaks = c(5, 8)) +\n  labs(x = NULL, y = \"March temperature\")\npanel_top\n\n\n\n\nAll together:\n\npanel_top / \n  (panel_bottom + \n     theme(axis.text.x = element_blank(),\n           axis.ticks.x = element_blank()))"
  },
  {
    "objectID": "rethinking/05-video.html",
    "href": "rethinking/05-video.html",
    "title": "Video #5 code",
    "section": "",
    "text": "\\[\n\\newcommand{\\ind}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\notind}{\\not\\!\\perp\\!\\!\\!\\perp}\n\\]\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdag)\nlibrary(ggrepel)\nlibrary(patchwork)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)\n\n\nThe fork (confounders)\n\\[\nX \\leftarrow Z \\rightarrow Y\n\\]\n\\(Z\\) connects \\(X\\) and \\(Y\\) so that \\(Y \\notind X\\)\n\nSimulated example\nWe can make some data to prove that they’re connected:\n\nn <- 1000\n\nfork_sim <- tibble(Z = rbinom(n, 1, prob = 0.5)) %>% \n  # When Z is 0, there's a 10% chance of X or Y being 1\n  # When Z is 1, there's a 90% chance of X or Y being 1\n  mutate(X = rbinom(n, 1, prob = ((1 - Z) * 0.1) + (Z * 0.9)),\n         Y = rbinom(n, 1, prob = ((1 - Z) * 0.1) + (Z * 0.9)))\n\nfork_sim %>% \n  select(-Z) %>% \n  table()\n##    Y\n## X     0   1\n##   0 390 101\n##   1  82 427\n\nfork_sim %>% \n  summarize(cor = cor(X, Y))\n## # A tibble: 1 × 1\n##     cor\n##   <dbl>\n## 1 0.634\n\nBut if we stratify by (or adjust for) \\(Z\\), we can see that \\(Y \\ind X \\mid Z\\):\n\nfork_sim %>% \n  select(X, Y, Z) %>% \n  table()\n## , , Z = 0\n## \n##    Y\n## X     0   1\n##   0 388  56\n##   1  36   2\n## \n## , , Z = 1\n## \n##    Y\n## X     0   1\n##   0   2  45\n##   1  46 425\n\nfork_sim %>% \n  group_by(Z) %>% \n  summarize(cor = cor(X, Y))\n## # A tibble: 2 × 2\n##       Z     cor\n##   <int>   <dbl>\n## 1     0 -0.0609\n## 2     1 -0.0546\n\nHere’s a continuous version too. When looking at all values of \\(Z\\), there’s a positive slope and relationship; when looking within each group, the relationship is 0 and flat.\n\nn <- 300\n\nfork_sim_cont <- tibble(Z = rbinom(n, 1, 0.5)) %>% \n  mutate(X = rnorm(n, 2 * Z - 1),\n         Y = rnorm(n, 2 * Z - 1))\n\nggplot(fork_sim_cont, aes(x = X, y = Y, color = factor(Z))) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(aes(color = NULL), method = \"lm\")\n\n\n\n\n\n\nWaffle House example\n\ndata(WaffleDivorce, package = \"rethinking\")\n\nWaffleDivorce <- WaffleDivorce %>% \n  mutate(across(c(Marriage, Divorce, MedianAgeMarriage), ~scale(.), .names = \"{col}_scaled\")) %>% \n  mutate(across(c(Marriage, Divorce, MedianAgeMarriage), ~as.numeric(scale(.)), .names = \"{col}_z\"))\n\nWhat is the causal effect of marriage on divorce?\n\nheight_sex_dag <- dagify(\n  x ~ z,\n  y ~ x + z,\n  exposure = \"x\",\n  outcome = \"y\",\n  labels = c(x = \"Marriage\", y = \"Divorce\", z = \"Age\"),\n  coords = list(x = c(x = 1, y = 3, z = 2),\n                y = c(x = 1, y = 1, z = 2))) %>% \n  tidy_dagitty() %>% \n  node_status()\n\nggplot(height_sex_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = label), size = 3.5, color = \"black\") +\n  scale_color_manual(values = clrs[c(1, 4)], guide = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\nWe can look at the relationship of all three of these arrows\n\nAge → MarriageAge → DivorceMarriage → Divorce\n\n\n\nggplot(WaffleDivorce, aes(x = MedianAgeMarriage, y = Marriage)) +\n  geom_point(aes(fill = factor(South)), size = 4, pch = 21, color = \"white\") +\n  geom_smooth(method = \"lm\") +\n  geom_text_repel(aes(label = Loc), max.overlaps = 2) +\n  scale_fill_manual(values = clrs[c(1, 3)], guide = \"none\") +\n  labs(x = \"Median age of marriage\", y = \"Marriage rate\")\n\n\n\n\n\n\n\nggplot(WaffleDivorce, aes(x = MedianAgeMarriage, y = Divorce)) +\n  geom_point(aes(fill = factor(South)), size = 4, pch = 21, color = \"white\") +\n  geom_smooth(method = \"lm\") +\n  geom_text_repel(aes(label = Loc), max.overlaps = 2) +\n  scale_fill_manual(values = clrs[c(1, 3)], guide = \"none\") +\n  labs(x = \"Median age of marriage\", y = \"Divorce rate\")\n\n\n\n\n\n\n\nggplot(WaffleDivorce, aes(x = Marriage, y = Divorce)) +\n  geom_point(aes(fill = factor(South)), size = 4, pch = 21, color = \"white\") +\n  geom_smooth(method = \"lm\") +\n  geom_text_repel(aes(label = Loc), max.overlaps = 2) +\n  scale_fill_manual(values = clrs[c(1, 3)], guide = \"none\") +\n  labs(x = \"Marriage rate\", y = \"Divorce rate\")\n\n\n\n\n\n\n\nHow do we stratify by a continuous variable though? Regression!\n\\[\n\\begin{aligned}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M M_i + \\beta_A A_i\n\\end{aligned}\n\\]\n\nPrior predictive simulation\n\\[\n\\begin{aligned}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M M_i + \\beta_A A_i \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_M &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_A &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\npriors <- c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b, coef = \"Marriage_z\"),\n            prior(normal(0, 0.5), class = b, coef = \"MedianAgeMarriage_z\"),\n            prior(exponential(1), class = sigma))\n\nmarriage_divorce_prior_only <- brm(\n  bf(Divorce_z ~ Marriage_z + MedianAgeMarriage_z),\n  data = WaffleDivorce,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Start sampling\n\n\ndraws_prior <- tibble(MedianAgeMarriage_z = seq(-2, 2, length.out = 100),\n                      Marriage_z = 0) %>% \n  add_epred_draws(marriage_divorce_prior_only, ndraws = 100)\n\ndraws_prior %>% \n  ggplot(aes(x = MedianAgeMarriage_z, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  labs(x = \"Median age of marriage (standardized)\",\n       y = \"Divorce rate (standardized)\",\n       caption = \"Standardized marriage rate held constant at 0\")\n\n\n\n\n\n\nActual model\nBased on these models,\n\nOnce we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state. (p. 134)\n\n\nbrmsStan\n\n\n\npriors <- c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b, coef = \"Marriage_z\"),\n            prior(normal(0, 0.5), class = b, coef = \"MedianAgeMarriage_z\"),\n            prior(exponential(1), class = sigma))\n\nmarriage_divorce_actual <- brm(\n  bf(Divorce_z ~ Marriage_z + MedianAgeMarriage_z),\n  data = WaffleDivorce,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Start sampling\n\n\nprint(marriage_divorce_actual)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: Divorce_z ~ Marriage_z + MedianAgeMarriage_z \n##    Data: WaffleDivorce (Number of observations: 50) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              -0.00      0.10    -0.20     0.20 1.00     3824     2317\n## Marriage_z             -0.07      0.15    -0.36     0.24 1.00     3087     2922\n## MedianAgeMarriage_z    -0.62      0.16    -0.92    -0.31 1.00     2894     2655\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.83      0.09     0.68     1.01 1.00     3630     2543\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n# get_variables(marriage_divorce_actual)\n\nmarriage_divorce_actual %>% \n  gather_draws(b_Intercept, b_Marriage_z, b_MedianAgeMarriage_z, sigma) %>% \n  ggplot(aes(x = .value, y = fct_rev(.variable))) +\n  stat_halfeye() +\n  coord_cartesian(xlim = c(-1, 1))\n\n\n\n\n\n\nmarriage_divorce_stan.stan\n\ndata {\n  int<lower=1> n;  // Observations\n  vector[n] Divorce_z;  // Outcome: divorce rate\n  vector[n] Marriage_z;  // \"Treatment\": marriage rate\n  vector[n] MedianAgeMarriage_z;  // Confounder: age\n}\n\nparameters {\n  real a;\n  real bM;\n  real bA;\n  real<lower=0> sigma;\n}\n\ntransformed parameters {\n  vector[n] mu;\n  mu = a + bM*Marriage_z + bA*MedianAgeMarriage_z;\n}\n\nmodel {\n  // Likelihood\n  Divorce_z ~ normal(mu, sigma);\n  \n  // Priors\n  a ~ normal(0, 0.2);\n  bM ~ normal(0, 0.5);\n  bA ~ normal(0, 0.5);\n  sigma ~ exponential(1);\n}\n\ngenerated quantities {\n  vector[n] Divorce_z_rep;\n  \n  for (i in 1:n) {\n    Divorce_z_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n\n\nstan_data <- WaffleDivorce %>% \n  select(Divorce_z, Marriage_z, MedianAgeMarriage_z) %>% \n  compose_data()\n\nmodel_marriage_divorce_stan <- rstan::sampling(\n  object = marriage_divorce_stan,\n  data = stan_data,\n  iter = 2000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_marriage_divorce_stan,\n      pars = c(\"a\", \"bM\", \"bA\", \"sigma\"))\n## Inference for Stan model: 8cc6e06905b678b9147ee76469c82d06.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## a      0.00       0 0.10 -0.19 -0.07  0.00  0.07  0.20  3754    1\n## bM    -0.06       0 0.15 -0.36 -0.17 -0.06  0.04  0.24  2518    1\n## bA    -0.61       0 0.15 -0.91 -0.71 -0.61 -0.51 -0.31  2568    1\n## sigma  0.83       0 0.09  0.68  0.76  0.82  0.88  1.02  3049    1\n## \n## Samples were drawn using NUTS(diag_e) at Wed Sep 21 11:06:44 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\n\n# get_variables(model_marriage_divorce_stan)\n\nmodel_marriage_divorce_stan %>% \n  gather_draws(a, bM, bA, sigma) %>% \n  mutate(.variable = factor(.variable, levels = c(\"a\", \"bM\", \"bA\", \"sigma\"))) %>% \n  ggplot(aes(x = .value, y = fct_rev(.variable))) +\n  stat_halfeye() +\n  coord_cartesian(xlim = c(-1, 1))\n\n\n\n\n\n\n\n\n\nSimulating causal effects\nWe can make counterfactual plots if we model the whole system, just like the “full luxury Bayes” model from video 4.\nWe want to know the causal effect of the marriage rate on the divorce rate, or:\n\\[\nE(\\text{Divorce rate} \\mid \\operatorname{do}(\\text{Marriage rate}))\n\\]\nHere’s model for the whole system:\n\\[\n\\begin{aligned}\nM_i &\\sim \\mathcal{N}(\\nu_i, \\tau) \\\\\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\nu_i &= \\alpha_M + \\beta_{AM} A_i \\\\\n\\mu_i &= \\alpha + \\beta_M M_i + \\beta_A A_i \\\\\n\\\\\n\\alpha_M &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_{AM} &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_M &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_A &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\tau &\\sim \\operatorname{Exponential}(1) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\n\nbrmsStan\n\n\n\npriors <- c(prior(normal(0, 0.2), class = Intercept, resp = Divorcez),\n            prior(normal(0, 0.5), class = b, coef = \"Marriage_z\", resp = Divorcez),\n            prior(normal(0, 0.5), class = b, coef = \"MedianAgeMarriage_z\", resp = Divorcez),\n            prior(exponential(1), class = sigma, resp = Divorcez),\n            \n            prior(normal(0, 0.2), class = Intercept, resp = Marriagez),\n            prior(normal(0, 0.5), class = b, coef = \"MedianAgeMarriage_z\", resp = Marriagez),\n            prior(exponential(1), class = sigma, resp = Marriagez))\n\nmodel_dag_full <- brm(\n  bf(Divorce_z ~ Marriage_z + MedianAgeMarriage_z) +\n    bf(Marriage_z ~ MedianAgeMarriage_z) + \n    set_rescor(FALSE),\n  data = WaffleDivorce,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Start sampling\n\n\nprint(model_dag_full)\n##  Family: MV(gaussian, gaussian) \n##   Links: mu = identity; sigma = identity\n##          mu = identity; sigma = identity \n## Formula: Divorce_z ~ Marriage_z + MedianAgeMarriage_z \n##          Marriage_z ~ MedianAgeMarriage_z \n##    Data: WaffleDivorce (Number of observations: 50) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                               Estimate Est.Error l-95% CI u-95% CI Rhat\n## Divorcez_Intercept                0.00      0.10    -0.20     0.19 1.00\n## Marriagez_Intercept               0.00      0.09    -0.18     0.19 1.00\n## Divorcez_Marriage_z              -0.06      0.16    -0.36     0.25 1.00\n## Divorcez_MedianAgeMarriage_z     -0.61      0.16    -0.91    -0.29 1.00\n## Marriagez_MedianAgeMarriage_z    -0.69      0.10    -0.89    -0.48 1.00\n##                               Bulk_ESS Tail_ESS\n## Divorcez_Intercept                5471     2861\n## Marriagez_Intercept               5718     2909\n## Divorcez_Marriage_z               3565     3160\n## Divorcez_MedianAgeMarriage_z      3484     2710\n## Marriagez_MedianAgeMarriage_z     4943     2673\n## \n## Family Specific Parameters: \n##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_Divorcez      0.83      0.09     0.68     1.02 1.00     4322     2680\n## sigma_Marriagez     0.71      0.07     0.58     0.87 1.00     4992     3220\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nsim_age_divorce <- tibble(MedianAgeMarriage_z = seq(from = -2, to = 2, length.out = 40),\n                          Marriage_z = 0) %>% \n  add_predicted_draws(model_dag_full, resp = \"Divorcez\")\n\nggplot(sim_age_divorce, aes(x = MedianAgeMarriage_z, y = .prediction)) +\n  stat_lineribbon(.width = 0.89, color = clrs[5], fill = clrs[5], alpha = 0.5) +\n  labs(title = \"Total counterfactual effect of age on divorce rate\",\n       subtitle = \"A → D in the DAG\")\n\n\n\n\n\nsim_age_marriage <- tibble(MedianAgeMarriage_z = seq(from = -2, to = 2, length.out = 40)) %>% \n  add_predicted_draws(model_dag_full, resp = \"Marriagez\")\n\nggplot(sim_age_marriage, aes(x = MedianAgeMarriage_z, y = .prediction)) +\n  stat_lineribbon(.width = 0.89, color = clrs[6], fill = clrs[6], alpha = 0.5) +\n  labs(title = \"Counterfactual effect of age on marriage rate\",\n       subtitle = \"A → M in the DAG\")\n\n\n\n\n\nsim_age_marriage_divorce <- tibble(Marriage_z = seq(from = -2, to = 2, length.out = 40),\n                                   MedianAgeMarriage_z = 0) %>% \n  add_predicted_draws(model_dag_full, resp = \"Marriagez\")\n\nggplot(sim_age_marriage_divorce, aes(x = Marriage_z, y = .prediction)) +\n  stat_lineribbon(.width = 0.89, color = clrs[3], fill = clrs[3], alpha = 0.5) +\n  labs(title = \"Total counterfactual effect of marriage rate on divorce rate\",\n       subtitle = \"M → D, after adjusting for A in the DAG, or E(D | do(M))\")\n\n\n\n\n\n\nmarriage_dag_full_stan.stan\n\ndata {\n  int<lower=1> n;  // Observations\n  vector[n] Divorce_z;  // Outcome: divorce rate\n  vector[n] Marriage_z;  // \"Treatment\": marriage rate\n  vector[n] MedianAgeMarriage_z;  // Confounder: age\n}\n\nparameters {\n  // Age -> Marriage\n  real aM;\n  real bAM;\n  real<lower=0> tau;\n\n  // Age -> Divorce <- Marriage\n  real a;\n  real bM;\n  real bA;\n  real<lower=0> sigma;\n}\n\nmodel {\n  vector[n] nu;\n  vector[n] mu;\n  \n  // Age -> Marriage\n  aM ~ normal(0, 0.2);\n  bAM ~ normal(0, 0.5);\n  tau ~ exponential(1);\n  \n  nu = aM + bAM*MedianAgeMarriage_z;\n  \n  Marriage_z ~ normal(nu, tau);\n\n  // Age -> Divorce <- Marriage\n  a ~ normal(0, 0.2);\n  bM ~ normal(0, 0.5);\n  bA ~ normal(0, 0.5);\n  sigma ~ exponential(1);\n  \n  mu = a + bM*Marriage_z + bA*MedianAgeMarriage_z;\n\n  Divorce_z ~ normal(mu, sigma);\n}\n\ngenerated quantities {\n  vector[n] Divorce_z_rep;\n  vector[n] Marriage_z_rep;\n  vector[n] divorce_do_marriage;\n  \n  for (i in 1:n) {\n    real nu_hat_n = aM + bAM*MedianAgeMarriage_z[i];\n    real mu_hat_n = a + bM*Marriage_z[i] + bA*MedianAgeMarriage_z[i];\n\n    Marriage_z_rep[i] = normal_rng(nu_hat_n, tau);\n    Divorce_z_rep[i] = normal_rng(mu_hat_n, sigma);\n    divorce_do_marriage[i] = normal_rng(a + bM*Marriage_z_rep[i] + bA*0, sigma);\n  }\n}\n\n\nstan_data <- WaffleDivorce %>% \n  select(Divorce_z, Marriage_z, MedianAgeMarriage_z) %>% \n  compose_data()\n\nmodel_marriage_dag_full_stan <- rstan::sampling(\n  object = marriage_dag_full_stan,\n  data = stan_data,\n  iter = 2000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_marriage_dag_full_stan,\n      pars = c(\"aM\", \"bAM\", \"tau\", \"a\", \"bM\", \"bA\", \"sigma\"))\n## Inference for Stan model: ea32b2c9a1ab179009a8845d85ea5d42.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## aM     0.00       0 0.09 -0.18 -0.07  0.00  0.06  0.18  5552    1\n## bAM   -0.69       0 0.10 -0.88 -0.75 -0.69 -0.63 -0.50  5583    1\n## tau    0.71       0 0.08  0.58  0.66  0.70  0.76  0.88  5465    1\n## a      0.00       0 0.10 -0.20 -0.07  0.00  0.07  0.20  5541    1\n## bM    -0.06       0 0.16 -0.37 -0.17 -0.06  0.05  0.25  2842    1\n## bA    -0.61       0 0.16 -0.92 -0.71 -0.61 -0.50 -0.29  3314    1\n## sigma  0.83       0 0.09  0.68  0.77  0.82  0.88  1.03  5675    1\n## \n## Samples were drawn using NUTS(diag_e) at Wed Sep 21 11:07:06 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\n\nstan_age_divorce <- model_marriage_dag_full_stan %>% \n  spread_draws(Divorce_z_rep[i]) %>% \n  mean_hdci() %>% \n  mutate(age = WaffleDivorce$MedianAgeMarriage_z)\n\nggplot(stan_age_divorce, aes(x = age, y = Divorce_z_rep)) +\n  geom_line(color = clrs[5]) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.2, fill = clrs[5]) +\n  coord_cartesian(xlim = c(-2, 2)) +\n  labs(title = \"Total counterfactual effect of age on divorce rate\",\n       subtitle = \"A → D in the DAG\")\n\n\n\n\n\nstan_age_marriage <- model_marriage_dag_full_stan %>% \n  spread_draws(Marriage_z_rep[i]) %>% \n  mean_hdci() %>% \n  mutate(age = WaffleDivorce$MedianAgeMarriage_z)\n\nggplot(stan_age_marriage, aes(x = age, y = Marriage_z_rep)) +\n  geom_line(color = clrs[6]) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.2, fill = clrs[6]) +\n  coord_cartesian(xlim = c(-2, 2)) +\n  labs(title = \"Counterfactual effect of age on marriage rate\",\n       subtitle = \"A → M in the DAG\")\n\n\n\n\n\nstan_age_marriage_divorce <- model_marriage_dag_full_stan %>% \n  spread_draws(divorce_do_marriage[i]) %>% \n  mean_hdci() %>% \n  mutate(age = WaffleDivorce$MedianAgeMarriage_z)\n\nggplot(stan_age_marriage_divorce, aes(x = age, y = divorce_do_marriage)) +\n  geom_line(color = clrs[3]) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.2, fill = clrs[3]) +\n  coord_cartesian(xlim = c(-2, 2)) +\n  labs(title = \"Total counterfactual effect of marriage rate on divorce rate\",\n       subtitle = \"M → D, after adjusting for A in the DAG, or E(D | do(M))\")\n\n\n\n\n\n\n\n\n\n\n\nThe pipe (mediators)\n\\[\nX \\rightarrow Z \\rightarrow Y\n\\]\n\\(X\\) and \\(Y\\) are associated (\\(Y \\notind X\\)) because influence of \\(X\\) is passed to \\(Y\\) through \\(Z\\). After adjusting for \\(Z\\), though, there’s no association, or \\(Y \\ind X \\mid Z\\).\n\nSimulated example\n\nn <- 1000\n\npipe_sim <- tibble(X = rbinom(n, 1, prob = 0.5)) %>% \n  # When X is 0, there's a 10% chance of Z being 1\n  # When X is 1, there's a 90% chance of Z being 1\n  # When Z is 0, there's a 10% chance of Y being 1\n  # When Z is 1, there's a 90% chance of Y being 1\n  mutate(Z = rbinom(n, 1, prob = ((1 - X) * 0.1) + (X * 0.9)),\n         Y = rbinom(n, 1, prob = ((1 - Z) * 0.1) + (Z * 0.9)))\n\npipe_sim %>% \n  select(-Z) %>% \n  table()\n##    Y\n## X     0   1\n##   0 403  92\n##   1  73 432\n\npipe_sim %>% \n  summarize(cor = cor(X, Y))\n## # A tibble: 1 × 1\n##     cor\n##   <dbl>\n## 1 0.670\n\nBut if we adjust for \\(Z\\), \\(Y \\ind X \\mid Z\\):\n\npipe_sim %>% \n  select(X, Y, Z) %>%\n  table()\n## , , Z = 0\n## \n##    Y\n## X     0   1\n##   0 401  58\n##   1  33   4\n## \n## , , Z = 1\n## \n##    Y\n## X     0   1\n##   0   2  34\n##   1  40 428\n\npipe_sim %>% \n  group_by(Z) %>% \n  summarize(cor = cor(X, Y))\n## # A tibble: 2 × 2\n##       Z     cor\n##   <int>   <dbl>\n## 1     0 -0.0145\n## 2     1 -0.0279\n\nThis also works with continuous data. When looking at all values of \\(Z\\), there’s a positive slope and relationship; when looking within each group, the relationship is 0 and flat.\n\nn <- 300\n\npipe_sim_cont <- tibble(X = rnorm(n, 0, 1)) %>% \n  mutate(Z = rbinom(n, 1, plogis(X)),\n         Y = rnorm(n, (2 * Z - 1), 1))\n\nggplot(pipe_sim_cont, aes(x = X, y = Y, color = factor(Z))) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(aes(color = NULL), method = \"lm\")\n\n\n\n\n\n\nFungus experiment example\nWith this DAG, we shouldn’t adjust for \\(F\\), since that would block the effect of the fungus, which in this case is super important since the causal mechanism pretty much only flows through \\(F\\). If we adjust for \\(F\\), we’ll get the causal effect of the treatment on height without the effect of the fungus, which is weird and probably 0.\n\nplant_fungus_dag <- dagify(\n  h1 ~ t + f + h0,\n  f ~ t,\n  exposure = \"t\",\n  outcome = \"h1\",\n  labels = c(t = \"Treatment\", h1 = \"Height, t=1\", f = \"Fungus\", h0 = \"Height, t=0\"),\n  coords = list(x = c(t = 1, h1 = 3, f = 2, h0 = 3),\n                y = c(t = 1, h1 = 1, f = 2, h0 = 2))) %>% \n  tidy_dagitty() %>% \n  node_status()\n\nggplot(plant_fungus_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = label), size = 3.5, color = \"black\") +\n  scale_color_manual(values = clrs[c(1, 4)], guide = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\nIn general this is called post-treatment bias and it is bad.\n\n\n\nThe collider (colliders, obvs)\n\\[\nX \\rightarrow Z \\leftarrow Y\n\\]\n\\(X\\) and \\(Y\\) are not associated (\\(Y \\ind X\\)), but they both influence \\(Z\\). Once you adjust for \\(Z\\), \\(X\\) and \\(Y\\) become associated and \\(Y \\notind X \\mid Z\\).\nWhen we learn about \\(Z\\) (or stratify by \\(Z\\), or only look at specific values of \\(Z\\)), we necessarily learn something about \\(X\\) and \\(Y\\), since they helped generate \\(Z\\)\n\nSimulated example\n\nn <- 1000\n\ncollider_sim <- tibble(X = rbinom(n, 1, prob = 0.5),\n                       Y = rbinom(n, 1, prob = 0.5)) %>% \n  # If either X and Y are 1, there's a 90% chance that Z will be 1\n  mutate(Z = rbinom(n, 1, prob = ifelse(X + Y > 0, 0.9, 0.2)))\n\n# These are independent\ncollider_sim %>% \n  select(-Z) %>% \n  table()\n##    Y\n## X     0   1\n##   0 248 253\n##   1 240 259\n\n# No correlation\ncollider_sim %>% \n  summarize(cor = cor(X, Y))\n## # A tibble: 1 × 1\n##      cor\n##    <dbl>\n## 1 0.0141\n\nWhen we adjust for \\(Z\\), though, \\(Y \\notind X \\mid Z\\):\n\ncollider_sim %>% \n  select(X, Y, Z) %>%\n  table()\n## , , Z = 0\n## \n##    Y\n## X     0   1\n##   0 206  25\n##   1  27  17\n## \n## , , Z = 1\n## \n##    Y\n## X     0   1\n##   0  42 228\n##   1 213 242\n\n# They're correlated!\ncollider_sim %>% \n  group_by(Z) %>% \n  summarize(cor = cor(X, Y))\n## # A tibble: 2 × 2\n##       Z    cor\n##   <int>  <dbl>\n## 1     0  0.283\n## 2     1 -0.316\n\nAs with the others, this works with continuous data too. When ignoring values of \\(Z\\), there’s no relationship between \\(X\\) and \\(Y\\). But once we adjust for or stratify by \\(Z\\), there’s a relationship within each group.\n\nn <- 300\n\ncollider_sim_cont <- tibble(X = rnorm(n, 0, 1),\n                            Y = rnorm(n, 0, 1)) %>% \n  mutate(Z = rbinom(n, 1, plogis(2*X + 2*Y - 2)))\n\nggplot(collider_sim_cont, aes(x = X, y = Y, color = factor(Z))) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(aes(color = NULL), method = \"lm\")\n\n\n\n\n\n\nGrant selection example\n\nset.seed(1914)\n\nn <- 200\n\ngrants <- tibble(newsworthiness = rnorm(n, 0, 1),\n                 trustworthiness = rnorm(n, 0, 1)) %>% \n  mutate(total = newsworthiness + trustworthiness) %>% \n  # Select just the top 10%\n  mutate(q = quantile(total, 1 - 0.1)) %>% \n  mutate(selected = total >= q)\n\n# No relationship\ngrants %>% \n  summarize(cor = cor(newsworthiness, trustworthiness))\n## # A tibble: 1 × 1\n##       cor\n##     <dbl>\n## 1 -0.0672\n\n# Relationship!\ngrants %>% \n  group_by(selected) %>% \n  summarize(cor = cor(newsworthiness, trustworthiness))\n## # A tibble: 2 × 2\n##   selected    cor\n##   <lgl>     <dbl>\n## 1 FALSE    -0.274\n## 2 TRUE     -0.768\n\nggplot(grants, aes(x = newsworthiness, y = trustworthiness, color = selected)) +\n  geom_point() +\n  geom_smooth(data = filter(grants, selected), method = \"lm\") +\n  geom_smooth(aes(color = \"Full sample\"), method = \"lm\")\n\n\n\n\n\n\n\nThe descendant\nLike a confounder if it comes from a confounder; like a mediator if it comes from a mediator; like a collider if it comes from a collider.\n\\(X\\) and \\(Y\\) are causally associated through \\(Z\\), which implies that \\(Y \\notind X\\). \\(A\\) contains information about \\(Z\\), so once we stratify by or adjust for \\(A\\), \\(X\\) and \\(Y\\) become less associated (if \\(A\\) is strong enough), implying \\(Y \\ind X \\mid A\\)\nThat can be good (if \\(A\\) is confounder-flavored) or bad (if \\(A\\) is mediator- or collider-flavored).\n\ndesc_confounder_dag <- dagify(\n  Y ~ Z,\n  X ~ Z,\n  A ~ Z,\n  coords = list(x = c(X = 1, Y = 3, Z = 2, A = 2),\n                y = c(X = 1, Y = 1, Z = 1, A = 0))) %>% \n  tidy_dagitty()\n\ndesc_mediator_dag <- dagify(\n  Y ~ Z,\n  Z ~ X,\n  A ~ Z,\n  coords = list(x = c(X = 1, Y = 3, Z = 2, A = 2),\n                y = c(X = 1, Y = 1, Z = 1, A = 0))) %>% \n  tidy_dagitty()\n\ndesc_collider_dag <- dagify(\n  Z ~ X + Y,\n  A ~ Z,\n  coords = list(x = c(X = 1, Y = 3, Z = 2, A = 2),\n                y = c(X = 1, Y = 1, Z = 1, A = 0))) %>% \n  tidy_dagitty()\n\nplot_desc_confounder <- ggplot(desc_confounder_dag, \n                               aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point() +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  ylim(c(-0.25, 1.25)) +\n  labs(subtitle = \"Confounder-flavored descendant\") +\n  theme_dag() +\n  theme(plot.subtitle = element_text(hjust = 0.5, face = \"bold\"))\n\nplot_desc_mediator <- ggplot(desc_mediator_dag, \n                             aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point() +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  ylim(c(-0.25, 1.25)) +\n  labs(subtitle = \"Mediator-flavored descendant\") +\n  theme_dag() +\n  theme(plot.subtitle = element_text(hjust = 0.5, face = \"bold\"))\n\nplot_desc_collider <- ggplot(desc_collider_dag, \n                             aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point() +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  ylim(c(-0.25, 1.25)) +\n  labs(subtitle = \"Collider-flavored descendant\") +\n  theme_dag() +\n  theme(plot.subtitle = element_text(hjust = 0.5, face = \"bold\"))\n\nplot_desc_confounder + plot_desc_mediator + plot_desc_collider"
  },
  {
    "objectID": "rethinking/06-video.html",
    "href": "rethinking/06-video.html",
    "title": "Video #6 code",
    "section": "",
    "text": "No code here; the lecture is a good overview of DAGs and good/bad controls.\n\nlibrary(tidyverse)\nlibrary(furrr)\nlibrary(ggdag)\nlibrary(ggraph)\nlibrary(patchwork)\n\n# Parallel stuff\nplan(multisession, workers = 4)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\n\n\nPost-treatment mediator\nIf we have this DAG, where \\(Z\\) is a mediator between \\(X\\) and \\(Y\\), and \\(u\\) is some unobserved confounding, should we control for \\(Z\\)? No!\n\n\nCode\ndagify(\n  Y ~ Z + u,\n  Z ~ X + u,\n  exposure = \"X\",\n  outcome = \"Y\",\n  latent = \"u\",  \n  coords = list(x = c(X = 1, Y = 4, Z = 2, u = 3),\n                y = c(X = 1, Y = 1, Z = 1, u = 2))) %>% \n  tidy_dagitty() %>% \n  node_status() %>% \n  as_tibble() %>% \n  left_join(tribble(\n    ~name, ~to, ~coef,\n    \"X\",   \"Z\", 1,\n    \"u\",   \"Z\", 1,\n    \"u\",   \"Y\", 1,\n    \"Z\",   \"Y\", 1\n  ), by = c(\"name\", \"to\")) %>% \n  mutate(latent = status == \"latent\",\n         latent = ifelse(is.na(latent), FALSE, latent)) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_linetype = latent, label = coef), \n                 angle_calc = \"along\", label_dodge = grid::unit(10, 'points')) +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  scale_color_manual(values = c(clrs[1], \"grey50\", clrs[4]), \n                     na.value = \"black\", guide = \"none\") +\n  scale_edge_linetype_manual(values = c(\"solid\", \"43\"), guide = \"none\") +\n  ylim(c(0.9, 2.1)) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nHere’s a simulation to show why. It uses completely random and independent values for \\(X\\) and \\(u\\), and \\(Z\\) and \\(Y\\) are determined by coefficients (1 in this case). When using the model Y ~ X, \\(X\\) has the correct coefficient (0); when using the model Y ~ X + Z, the coefficient for \\(X\\) is super wrong and negative:\n\nn <- 100\nbXZ <- 1\nbZY <- 1\n\nsim <- tibble(sim_id = 1:1000) %>% \n  mutate(sim = future_map(sim_id, ~{\n    tibble(\n      X = rnorm(n),\n      u = rnorm(n),\n      Z = rnorm(n, bXZ*X + u),\n      Y = rnorm(n, bZY*Z + u)\n    )\n  }, .options = furrr_options(seed = TRUE))) %>% \n  mutate(bX = future_map_dbl(sim, ~coef(lm(Y ~ X, data = .))[\"X\"]),\n         bXZ = future_map_dbl(sim, ~coef(lm(Y ~ X + Z, data = .))[\"X\"]))\n\nsim %>%\n  select(-sim) %>%\n  pivot_longer(starts_with(\"b\")) %>% \n  mutate(correct = ifelse(name == \"bX\", \"Correct\", \"Wrong\"),\n         name = recode(name, \"bX\" = \"Y ~ X\", \"bXZ\" = \"Y ~ X + Z\")) %>%\n  ggplot(aes(x = value, color = name, linetype = correct)) +\n  geom_density(size = 1) +\n  scale_color_manual(values = c(clrs[5], clrs[2])) +\n  scale_linetype_manual(values = c(\"solid\", \"dotted\")) +\n  xlim(c(-1.5, 2)) +\n  labs(x = \"β for X\", linetype = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nWe can see the same thing even if the coefficient between \\(Z\\) and \\(Y\\) is set to zero:\n\n\nCode\ndagify(\n  Y ~ Z + u,\n  Z ~ X + u,\n  exposure = \"X\",\n  outcome = \"Y\",\n  latent = \"u\",  \n  coords = list(x = c(X = 1, Y = 4, Z = 2, u = 3),\n                y = c(X = 1, Y = 1, Z = 1, u = 2))) %>% \n  tidy_dagitty() %>% \n  node_status() %>% \n  as_tibble() %>% \n  left_join(tribble(\n    ~name, ~to, ~coef,\n    \"X\",   \"Z\", 1,\n    \"u\",   \"Z\", 1,\n    \"u\",   \"Y\", 1,\n    \"Z\",   \"Y\", 0\n  ), by = c(\"name\", \"to\")) %>% \n  mutate(latent = status == \"latent\",\n         latent = ifelse(is.na(latent), FALSE, latent)) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_linetype = latent, label = coef), \n                 angle_calc = \"along\", label_dodge = grid::unit(10, 'points')) +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  scale_color_manual(values = c(clrs[1], \"grey50\", clrs[4]), \n                     na.value = \"black\", guide = \"none\") +\n  scale_edge_linetype_manual(values = c(\"solid\", \"43\"), guide = \"none\") +\n  ylim(c(0.9, 2.1)) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nn <- 100\nbXZ <- 1\nbZY <- 0\n\nsim <- tibble(sim_id = 1:1000) %>% \n  mutate(sim = future_map(sim_id, ~{\n    tibble(\n      X = rnorm(n),\n      u = rnorm(n),\n      Z = rnorm(n, bXZ*X + u),\n      Y = rnorm(n, bZY*Z + u)\n    )\n  }, .options = furrr_options(seed = TRUE))) %>% \n  mutate(bX = future_map_dbl(sim, ~coef(lm(Y ~ X, data = .))[\"X\"]),\n         bXZ = future_map_dbl(sim, ~coef(lm(Y ~ X + Z, data = .))[\"X\"]))\n\nsim %>%\n  select(-sim) %>%\n  pivot_longer(starts_with(\"b\")) %>% \n  mutate(correct = ifelse(name == \"bX\", \"Correct\", \"Wrong\"),\n         name = recode(name, \"bX\" = \"Y ~ X\", \"bXZ\" = \"Y ~ X + Z\")) %>%\n  ggplot(aes(x = value, color = name, linetype = correct)) +\n  geom_density(size = 1) +\n  scale_color_manual(values = c(clrs[5], clrs[2])) +\n  scale_linetype_manual(values = c(\"solid\", \"dotted\")) +\n  xlim(c(-1.5, 2)) +\n  labs(x = \"β for X\", linetype = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nCase-control bias\nHere \\(Z\\) comes after the outcome, like if \\(X\\) is education, \\(Y\\) is occupation, and \\(Z\\) is income. Should we control for \\(Z\\)? Surely that’s harmless?\nNope!\n\n\nCode\ndagify(\n  Y ~ X,\n  Z ~ Y,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(X = 1, Y = 2, Z = 3),\n                y = c(X = 1, Y = 1, Z = 1))) %>% \n  tidy_dagitty() %>% \n  node_status() %>% \n  as_tibble() %>% \n  left_join(tribble(\n    ~name, ~to, ~coef,\n    \"X\",   \"Y\", 1,\n    \"Y\",   \"Z\", 1\n  ), by = c(\"name\", \"to\")) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(label = coef), \n                 angle_calc = \"along\", label_dodge = grid::unit(10, 'points')) +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  scale_color_manual(values = c(clrs[1], clrs[4]), \n                     na.value = \"black\", guide = \"none\") +\n  xlim(c(0.75, 3.25)) + ylim(c(0.9, 1.1)) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nn <- 100\nbXY <- 1\nbYZ <- 1\n\nsim <- tibble(sim_id = 1:1000) %>% \n  mutate(sim = future_map(sim_id, ~{\n    tibble(\n      X = rnorm(n),\n      Z = rnorm(n, bXY*X),\n      Y = rnorm(n, bYZ*Z)\n    )\n  }, .options = furrr_options(seed = TRUE))) %>% \n  mutate(bX = future_map_dbl(sim, ~coef(lm(Y ~ X, data = .))[\"X\"]),\n         bXZ = future_map_dbl(sim, ~coef(lm(Y ~ X + Z, data = .))[\"X\"]))\n\nsim %>%\n  select(-sim) %>%\n  pivot_longer(starts_with(\"b\")) %>% \n  mutate(correct = ifelse(name == \"bX\", \"Correct\", \"Wrong\"),\n         name = recode(name, \"bX\" = \"Y ~ X\", \"bXZ\" = \"Y ~ X + Z\")) %>%\n  ggplot(aes(x = value, color = name, linetype = correct)) +\n  geom_density(size = 1) +\n  scale_color_manual(values = c(clrs[5], clrs[2])) +\n  scale_linetype_manual(values = c(\"solid\", \"dotted\")) +\n  xlim(c(-1, 2)) +\n  labs(x = \"β for X\", linetype = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nPrecision parasite\nHere \\(Z\\) comes before the treatment and doesn’t open any backdoors. Should we control for it? Again, it should be harmless?\nIn this case, it doesn’t distort the effect, but it does reduce the precision of the estimate\n\n\nCode\ndagify(\n  Y ~ X,\n  X ~ Z,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(X = 2, Y = 3, Z = 1),\n                y = c(X = 1, Y = 1, Z = 1))) %>% \n  tidy_dagitty() %>% \n  node_status() %>% \n  as_tibble() %>% \n  left_join(tribble(\n    ~name, ~to, ~coef,\n    \"X\",   \"Y\", 1,\n    \"Z\",   \"X\", 1\n  ), by = c(\"name\", \"to\")) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(label = coef), \n                 angle_calc = \"along\", label_dodge = grid::unit(10, 'points')) +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  scale_color_manual(values = c(clrs[1], clrs[4]), \n                     na.value = \"black\", guide = \"none\") +\n  xlim(c(0.75, 3.25)) + ylim(c(0.9, 1.1)) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nn <- 100\nbZX <- 1\nbXY <- 1\n\nsim <- tibble(sim_id = 1:1000) %>% \n  mutate(sim = future_map(sim_id, ~{\n    tibble(\n      Z = rnorm(n),\n      X = rnorm(n, bZX*Z),\n      Y = rnorm(n, bXY*X)\n    )\n  }, .options = furrr_options(seed = TRUE))) %>% \n  mutate(bX = future_map_dbl(sim, ~coef(lm(Y ~ X, data = .))[\"X\"]),\n         bXZ = future_map_dbl(sim, ~coef(lm(Y ~ X + Z, data = .))[\"X\"]))\n\nsim %>%\n  select(-sim) %>%\n  pivot_longer(starts_with(\"b\")) %>% \n  mutate(correct = ifelse(name == \"bX\", \"Correct\", \"Wrong\"),\n         name = recode(name, \"bX\" = \"Y ~ X\", \"bXZ\" = \"Y ~ X + Z\")) %>%\n  ggplot(aes(x = value, color = name, linetype = correct)) +\n  geom_density(size = 1) +\n  scale_color_manual(values = c(clrs[5], clrs[2])) +\n  scale_linetype_manual(values = c(\"solid\", \"dotted\")) +\n  xlim(c(0.5, 1.5)) +\n  labs(x = \"β for X\", linetype = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nBias amplification\nLike the precision parasite situation, but with an unobserved confounder \\(u\\). Really bad stuff happens here (“something truly awful” in the lecture). The true coefficient between \\(X\\) and \\(Y\\) here is 0, but the estimate is wrong with both models!\nIncluding causes of the exposure is generally a really bad idea. Covariation in \\(X\\) and \\(Y\\) requires variation in their causes, but within different levels of \\(Z\\) (since we’re stratifying by or adjusting for \\(Z\\)), there’s less variation in \\(X\\). That makes the unobserved \\(u\\) confounder more important when determining \\(X\\).\n\n\nCode\ndagify(\n  Y ~ X + u,\n  X ~ Z + u,\n  exposure = \"X\",\n  outcome = \"Y\",\n  latent = \"u\",  \n  coords = list(x = c(X = 2, Y = 4, Z = 1, u = 3),\n                y = c(X = 1, Y = 1, Z = 1, u = 2))) %>% \n  tidy_dagitty() %>% \n  node_status() %>% \n  as_tibble() %>% \n  left_join(tribble(\n    ~name, ~to, ~coef,\n    \"X\",   \"Y\", 0,\n    \"u\",   \"X\", 1,\n    \"u\",   \"Y\", 1,\n    \"Z\",   \"X\", 1\n  ), by = c(\"name\", \"to\")) %>% \n  mutate(latent = status == \"latent\",\n         latent = ifelse(is.na(latent), FALSE, latent)) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_linetype = latent, label = coef), \n                 angle_calc = \"along\", label_dodge = grid::unit(10, 'points')) +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = name), size = 3.5, color = \"white\") +\n  scale_color_manual(values = c(clrs[1], \"grey50\", clrs[4]), \n                     na.value = \"black\", guide = \"none\") +\n  scale_edge_linetype_manual(values = c(\"solid\", \"43\"), guide = \"none\") +\n  ylim(c(0.9, 2.1)) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nn <- 100\nbZX <- 1\nbXY <- 0\n\nsim <- tibble(sim_id = 1:1000) %>% \n  mutate(sim = future_map(sim_id, ~{\n    tibble(\n      Z = rnorm(n),\n      u = rnorm(n),\n      X = rnorm(n, bZX*Z + u),\n      Y = rnorm(n, bXY*X + u)\n    )\n  }, .options = furrr_options(seed = TRUE))) %>% \n  mutate(bX = future_map_dbl(sim, ~coef(lm(Y ~ X, data = .))[\"X\"]),\n         bXZ = future_map_dbl(sim, ~coef(lm(Y ~ X + Z, data = .))[\"X\"]))\n\nsim %>%\n  select(-sim) %>%\n  pivot_longer(starts_with(\"b\")) %>% \n  mutate(correct = ifelse(name == \"bX\", \"Wrong\", \"Wrong but worse\"),\n         name = recode(name, \"bX\" = \"Y ~ X\", \"bXZ\" = \"Y ~ X + Z\")) %>%\n  ggplot(aes(x = value, color = name, linetype = correct)) +\n  geom_density(size = 1) +\n  scale_color_manual(values = c(clrs[5], clrs[2])) +\n  scale_linetype_manual(values = c(\"dashed\", \"dotted\")) +\n  geom_vline(xintercept = 0) +\n  annotate(geom = \"text\", x = -0.02, y = 2.5, label = \"Actual value\", angle = 90) +\n  xlim(c(-0.15, 1)) +\n  labs(x = \"β for X\", linetype = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nThis whole idea that controlling for \\(Z\\) in the presence of unmeasured confounding amplifies the bias is really weird. Here’s another simulation from McElreath’s slides, where there is no relationship between \\(X\\) and \\(Y\\). There is a slight relationship between \\(X\\) and \\(Y\\) because of \\(u\\), but once we stratify by \\(Z\\), those slopes get bigger within each group!\n\ntibble(Z = rbinom(1000, 1, 0.5),\n       u = rnorm(1000)) %>% \n  mutate(X = rnorm(1000, 7*Z + u),\n         Y = rnorm(1000, 0*X + u)) %>% \n  ggplot(aes(x = X, y = Y, color = factor(Z))) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(aes(color = NULL), method = \"lm\")"
  },
  {
    "objectID": "rethinking/07-video.html#finding-outliers-with-psis-and-waic",
    "href": "rethinking/07-video.html#finding-outliers-with-psis-and-waic",
    "title": "Video #7 code",
    "section": "Finding outliers with PSIS and WAIC",
    "text": "Finding outliers with PSIS and WAIC\n\nggplot(WaffleDivorce, aes(x = MedianAgeMarriage_z, y = Divorce_z)) +\n  geom_point(aes(color = Loc %in% c(\"ME\", \"ID\")), size = 2) +\n  geom_text(data = filter(WaffleDivorce, Loc %in% c(\"ME\", \"ID\")), \n            aes(label = Location), hjust = -0.25) +\n  scale_color_manual(values = c(\"grey40\", clrs[4]), guide = \"none\") +\n  labs(x = \"Age at marriage (standardized)\", y = \"Divorce rate (standardized)\")\n\n\n\n\nRun a model:\n\npriors <- c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b, coef = \"Marriage_z\"),\n            prior(normal(0, 0.5), class = b, coef = \"MedianAgeMarriage_z\"),\n            prior(exponential(1), class = sigma))\n\nmarriage_divorce_normal <- brm(\n  bf(Divorce_z ~ Marriage_z + MedianAgeMarriage_z),\n  data = WaffleDivorce,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\nCheck the LOO stats. One value is fairly influential with k > 0.5, but the others are okay:\n\nloo(marriage_divorce_normal)\n## \n## Computed from 4000 by 50 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo    -63.8  6.4\n## p_loo         4.8  1.9\n## looic       127.7 12.8\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     49    98.0%   688       \n##  (0.5, 0.7]   (ok)        1     2.0%   176       \n##    (0.7, 1]   (bad)       0     0.0%   <NA>      \n##    (1, Inf)   (very bad)  0     0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\n\nWhich observation has the high PSIS k value?\n\nloo(marriage_divorce_normal) |> \n  pareto_k_ids()\n## [1] 13\n\nRow 13! Which is…\n\nWaffleDivorce |> \n  slice(13) |> \n  select(Location)\n##   Location\n## 1    Idaho\n\nIdaho.\nHow big is the actual k value?\n\nloo(marriage_divorce_normal) |> \n  pareto_k_values() |> \n  pluck(13)\n## [1] 0.6065822\n\nWe can embed these diagnostics into the brms object with add_criterion():\n\nmarriage_divorce_normal <- add_criterion(marriage_divorce_normal, criterion = \"loo\")\nmarriage_divorce_normal <- add_criterion(marriage_divorce_normal, criterion = \"waic\")\n## Warning: \n## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nAnd that lets us access things in deeply nested lists, like the 13th Pareto k value:\n\nmarriage_divorce_normal$criteria$loo$diagnostics$pareto_k[13]\n## [1] 0.6065822\n\nNeat. Now we can plot these k values and WAIC values and recreate Figure 7.10 from the book and from 1:03:00 in lecture video 7.\n\ntibble(psis = marriage_divorce_normal$criteria$loo$diagnostics$pareto_k,\n       p_waic = marriage_divorce_normal$criteria$waic$pointwise[, \"p_waic\"],\n       Location = pull(WaffleDivorce, Location),\n       Loc = pull(WaffleDivorce, Loc)) %>%\n  ggplot(aes(x = psis, y = p_waic)) +\n  geom_point(aes(color = Loc %in% c(\"ME\", \"ID\")), size = 2) +\n  geom_text(data = . %>% filter(Loc %in% c(\"ME\", \"ID\")), \n            aes(label = Location), hjust = 1.25) +\n  geom_vline(xintercept = 0.5, linetype = 32) +\n  scale_color_manual(values = c(\"grey40\", clrs[4]), guide = \"none\") +\n  labs(x = \"PSIS Pareto k\", y = \"WAIC penalty\")"
  },
  {
    "objectID": "rethinking/07-video.html#robust-regression",
    "href": "rethinking/07-video.html#robust-regression",
    "title": "Video #7 code",
    "section": "Robust regression",
    "text": "Robust regression\nWe can do robust regression with family = student(), which has thicker tails and expects larger values out in the tails\n\npriors <- c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b, coef = \"Marriage_z\"),\n            prior(normal(0, 0.5), class = b, coef = \"MedianAgeMarriage_z\"),\n            prior(exponential(1), class = sigma))\n\nmarriage_divorce_student <- brm(\n  bf(Divorce_z ~ Marriage_z + MedianAgeMarriage_z,\n     nu = 2),  # Tail thickness\n  data = WaffleDivorce,\n  family = student(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n\nAdd penalty statistics to the model object:\n\nmarriage_divorce_student <- add_criterion(marriage_divorce_student, criterion = c(\"loo\", \"waic\"))\n## Warning: \n## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nplot_data <- tibble(psis = marriage_divorce_student$criteria$loo$diagnostics$pareto_k,\n                    p_waic = marriage_divorce_student$criteria$waic$pointwise[, \"p_waic\"],\n                    Location = pull(WaffleDivorce, Location),\n                    Loc = pull(WaffleDivorce, Loc)) \nplot_data %>%\n  ggplot(aes(x = psis, y = p_waic)) +\n  geom_point(aes(color = Loc %in% c(\"ME\", \"ID\")), size = 2) +\n  geom_text(data = . %>% filter(Loc %in% c(\"ME\", \"ID\")), \n            aes(label = Location), hjust = 1.25) +\n  geom_vline(xintercept = 0.5, linetype = 32) +\n  scale_color_manual(values = c(\"grey40\", clrs[4]), guide = \"none\") +\n  labs(x = \"PSIS Pareto k\", y = \"WAIC penalty\")\n\n\n\n\nHey hey, Idaho and Maine have much lower PSIS k values now. There are some weird observations with really high WAIC values for some reason:\n\nplot_data |> \n  arrange(desc(p_waic))\n## # A tibble: 50 × 4\n##        psis p_waic Location     Loc  \n##       <dbl>  <dbl> <fct>        <fct>\n##  1  0.0790   0.709 Wyoming      WY   \n##  2 -0.00816  0.608 Utah         UT   \n##  3  0.00618  0.344 Arkansas     AR   \n##  4  0.0451   0.327 North Dakota ND   \n##  5  0.0223   0.309 Alaska       AK   \n##  6  0.0775   0.244 Maine        ME   \n##  7  0.102    0.240 Rhode Island RI   \n##  8  0.0301   0.215 Idaho        ID   \n##  9  0.181    0.212 Minnesota    MN   \n## 10  0.0640   0.211 New Jersey   NJ   \n## # … with 40 more rows\n\nWyoming and Utah! Why? I don’t know :("
  },
  {
    "objectID": "rethinking/07-video.html#compare-the-models",
    "href": "rethinking/07-video.html#compare-the-models",
    "title": "Video #7 code",
    "section": "Compare the models",
    "text": "Compare the models\nWe can compare the two models’ LOO statistics:\n\nloo_compare(marriage_divorce_normal, marriage_divorce_student, criterion = \"loo\")\n##                          elpd_diff se_diff\n## marriage_divorce_normal   0.0       0.0   \n## marriage_divorce_student -2.5       3.0\nloo_compare(marriage_divorce_normal, marriage_divorce_student, criterion = \"waic\")\n##                          elpd_diff se_diff\n## marriage_divorce_normal   0.0       0.0   \n## marriage_divorce_student -2.7       2.9\n\nThe normal model has the higher ELPD score (so it’s better), but the standard error is huge and makes the models indistinguishable (so it’s not necessarily better)\nWe can also compare the posterior distributions for the effect of age on marriage. The coefficient for age in the Student-t model is more negative and more precise. Idaho was making the normal model too skeptical and too surprised; the Student-t model was less surprised by it.\n\nnormal_coefs <- marriage_divorce_normal |> \n  spread_draws(b_MedianAgeMarriage_z) |> \n  mutate(model = \"Gaussian model\")\n\nstudent_coefs <- marriage_divorce_student |> \n  spread_draws(b_MedianAgeMarriage_z) |> \n  mutate(model = \"Student-t model\")\n\nbind_rows(normal_coefs, student_coefs) |> \n  ggplot(aes(x = b_MedianAgeMarriage_z, fill = model)) +\n  stat_halfeye(slab_alpha = 0.75) +\n  scale_fill_manual(values = c(clrs[6], clrs[4]))"
  },
  {
    "objectID": "rethinking/10-video.html#the-weirdness-of-ordered-categories",
    "href": "rethinking/10-video.html#the-weirdness-of-ordered-categories",
    "title": "Video #11 code",
    "section": "The weirdness of ordered categories",
    "text": "The weirdness of ordered categories\nOur main estimand: How do action, intention, and contact influence someone’s response to a trolley story? (Where response is measured on an ordered categorical scale ranging from 1 to 7)\nI don’t know what action, intention, and contact really actually mean—I’m not a moral philosopher or anything. So I’ll just go with the mechanics of model fitting here.\nTo get an overview of the data, here’s the distribution of the possible trolley problem responses. The combination of its categoricalness and its orderedness makes it tricky to work with, since something like 4 inherently implies values of 1, 2, and 4 (i.e. the responses are cumulative), and since the distance between these categories isn’t the same (i.e. moving from 3 → 4 isn’t the same as moving from 6 → 7).\n\np1 <- trolley |> \n  ggplot(aes(x = resp_ord)) +\n  stat_count(width = 0.5, fill = clrs[3]) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Response\", y = \"Count\")\np1\n\n\n\n\nThese responses build on each other—if someone responds with a 4, it means that they also were a 1, 2, and 3. Each value is bigger than the previous value. We can calculate the probability of each category on its own as well as the cumulative probability, or the probability of at least 4, or 5, or whatever\n\np2 <- trolley |> \n  count(resp_ord) |> \n  mutate(pr_k = n / sum(n),\n         cum_pr_k = cumsum(pr_k)) |> \n  ggplot(aes(x = resp_ord, y = cum_pr_k)) +\n  geom_line(aes(group = 0), color = clrs[2], size = 1) +\n  geom_point(shape = 21, fill = clrs[2], color = \"white\", size = 5, stroke = 1) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Response\", y = \"Cumulative proportion\")\np2\n\n\n\n\nBut modeling probabilities is tricky, so instead we use a link function to transform the probabilities into something more amenable to linear modeling, like log odds or logits (just like logistic regression). We use the log cumulative odds that a response value \\(y_i\\) is some possible outcome value \\(k\\) (1–7 here), and each \\(k\\) has its own “intercept”, or \\(\\alpha_k\\), that represents the boundary between categories\n\\[\n\\log \\frac{\\Pr(y_i \\leq k)}{1 - \\Pr(y_i \\leq k)} = \\alpha_k\n\\] We can convert probabilities to this log cumulative odds scale with qlogis(). (McElreath makes his own logit() function like logit <- function(x) log(x / (1 - x)), but that’s the same as qlogis().) Note that \\(\\alpha_7\\) here is infinity; it’s the end of of the distribution and already contains 100% of the previous values.\n\ntrolley |> \n  count(resp_ord) |> \n  mutate(pr_k = n / sum(n),\n         cum_pr_k = cumsum(pr_k)) |> \n  mutate(alpha_k = qlogis(cum_pr_k))\n##   resp_ord    n       pr_k  cum_pr_k    alpha_k\n## 1        1 1274 0.12829809 0.1282981 -1.9160912\n## 2        2  909 0.09154079 0.2198389 -1.2666056\n## 3        3 1071 0.10785498 0.3276939 -0.7186340\n## 4        4 2323 0.23393756 0.5616314  0.2477857\n## 5        5 1462 0.14723061 0.7088620  0.8898637\n## 6        6 1445 0.14551863 0.8543807  1.7693809\n## 7        7 1446 0.14561934 1.0000000        NaN\n\n\np3 <- trolley |> \n  count(resp_ord) |> \n  mutate(pr_k = n / sum(n),\n         cum_pr_k = cumsum(pr_k)) |> \n  mutate(alpha_k = qlogis(cum_pr_k)) |> \n  ggplot(aes(x = resp_ord, y = alpha_k)) +\n  geom_line(aes(group = 0), size = 1, color = clrs[1]) +\n  geom_point(shape = 21, fill = clrs[1], color = \"white\", size = 5, stroke = 1) +\n  labs(x = \"Response\", y = \"Log cumulative odds\")\np3\n\n\n\n\nHere are all three plots simultaneously to see how to translate from category counts into logit-scale cumulative odds:\n\np1 | p2 | p3"
  },
  {
    "objectID": "rethinking/10-video.html#intercept-only-model",
    "href": "rethinking/10-video.html#intercept-only-model",
    "title": "Video #11 code",
    "section": "Intercept-only model",
    "text": "Intercept-only model\nWe can write a formal model for this intercept-only approach like so:\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Categorical}(p) \\\\\n\\operatorname{logit}(p_k) &= \\alpha_k - \\phi \\\\\n\\phi &= 0 \\\\\n\\alpha_k &\\sim \\mathcal{N}(1, 1.5)\n\\end{aligned}\n\\]\nThat \\(\\phi\\) is a placeholder for the offsets that come from a linear model; here it’s just 0. We can also write this using the “Ordered distribution” shorthand distribution:\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Ordered\\ logit}(\\phi_i, \\alpha) \\\\\n\\phi_i &= 0 \\\\\n\\alpha_k &\\sim \\mathcal{N}(1, 1.5)\n\\end{aligned}\n\\]\nIn brms we can fit this with family = cumulative(). rstanarm supports ordered logit too with stan_polr(), but I won’t use it here because it’s less fully featured than brms for these more advanced models (and it wasn’t working with an intercept-only model :shrug:).\nSolomon Kurz does a neat thing with setting initial chain values for each of the intercepts at their approximate locations on the logit scale, which helps with model fit. In his code, he does this with bracketed subscripts:\ninits <- list(`Intercept[1]` = -2,\n              `Intercept[2]` = -1,\n              `Intercept[3]` = 0,\n              `Intercept[4]` = 1,\n              `Intercept[5]` = 2,\n              `Intercept[6]` = 2.5)\nNowadays, though, Stan gets mad at that and wants just one named element with a vector of values, like this:\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5))\nLet’s make the official model:\n\npriors <- c(prior(normal(0, 1.5), class = Intercept))\n\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5))\n\nmodel_int_only <- brm(\n  bf(resp_ord ~ 1),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  init = rep(list(inits), 4),\n  save_warmup = TRUE,\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", refresh = 0,\n  file = \"11-manual-cache/model-int-only\"\n)\n\ntidybayes doesn’t have a way to extract warmup samples (I don’t think?), but we can look at them with posterior::as_draws_df(). We’ll look at the first 100 draws just to see how the custom inits worked. This is actually pretty neat—the chains for each of the intercepts start at c(-2, -1, 0, 1, 2, 2.5) respectively (the first draws aren’t any of those values precisely, since they’re the draws that come after starting at those values). After the first couple dozen draws, the chains start converging on the real posterior space and dance around it. But we helped the chains get there faster than if we had set all the intercepts to 0 or some random numbers.\n\nint_only_samples <- as_draws_df(model_int_only, inc_warmup = TRUE) |> \n  pivot_longer(starts_with(\"b_Intercept\"))\nhead(int_only_samples, 6)\n## # A tibble: 6 × 8\n##    disc lprior    lp__ .chain .iteration .draw name             value\n##   <dbl>  <dbl>   <dbl>  <int>      <int> <int> <chr>            <dbl>\n## 1     1  -10.3 -19252.      1          1     1 b_Intercept[1] -2.22  \n## 2     1  -10.3 -19252.      1          1     1 b_Intercept[2] -1.48  \n## 3     1  -10.3 -19252.      1          1     1 b_Intercept[3] -0.756 \n## 4     1  -10.3 -19252.      1          1     1 b_Intercept[4]  0.0863\n## 5     1  -10.3 -19252.      1          1     1 b_Intercept[5]  0.889 \n## 6     1  -10.3 -19252.      1          1     1 b_Intercept[6]  1.42\n\nint_only_samples |> \n  filter(.iteration < 100) |>\n  ggplot(aes(x = .iteration, y = value, color = factor(.chain))) +\n  geom_line(size = 0.5) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(name), scales = \"free_y\")\n\n\n\n\nAnyway, here’s the actual post-warmup traceplot using the normal tidybayes approach, since that’s all we really care about:\n\nmodel_int_only |> \n  gather_draws(`^b_Intercept.*`, regex = TRUE) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n\n\n\n\n…and the actual posteriors for these intercept cutpoints:\n\nmodel_int_only\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ 1 \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -1.92      0.03    -1.97    -1.86 1.00     2553     2626\n## Intercept[2]    -1.27      0.02    -1.31    -1.22 1.00     3728     3257\n## Intercept[3]    -0.72      0.02    -0.76    -0.67 1.00     4235     3490\n## Intercept[4]     0.25      0.02     0.21     0.29 1.00     4496     3365\n## Intercept[5]     0.89      0.02     0.85     0.93 1.00     4385     3551\n## Intercept[6]     1.77      0.03     1.71     1.83 1.00     4671     3605\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nmodel_int_only |> \n  gather_draws(`^b_Intercept.*`, regex = TRUE) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_viridis_d(option = \"rocket\", end = 0.85, guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n\n\n\n\nThese cutpoints are on the logit scale, so we can unlogit them with plogis() to get probabilities:\n\nmodel_int_only |> \n  gather_draws(`^b_Intercept.*`, regex = TRUE) |> \n  mutate(.value = plogis(.value)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_viridis_d(option = \"rocket\", end = 0.85, guide = \"none\") +\n  scale_x_continuous(labels = label_percent()) +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n\n\n\n\nWoot!"
  },
  {
    "objectID": "rethinking/10-video.html#thinking-about-shifts-in-thresholds",
    "href": "rethinking/10-video.html#thinking-about-shifts-in-thresholds",
    "title": "Video #11 code",
    "section": "Thinking about shifts in thresholds",
    "text": "Thinking about shifts in thresholds\nIn the intercept-only model we left the \\(\\phi\\) part of the overall model blank (\\(\\phi = 0\\)), but in practice, we want to use that part of the model to see how covariates influence the probabilities of specific categories. \\(\\phi\\) shifts the different \\(\\alpha\\) cutpoints around. The non-shortcut-y model shows this:\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Categorical}(p) \\\\\n\\operatorname{logit}(p_k) &= \\alpha_k - \\phi \\\\\n\\phi &= \\beta_1 x_i + \\beta_2 x_2 + \\dots \\\\\n\\alpha_k &\\sim \\mathcal{N}(1, 1.5)\n\\end{aligned}\n\\]\nTo help with the intuition of why we subtract \\(\\phi\\) from each \\(\\alpha\\), let’s convert the cutpoints from the intercept-only model to probabilities using rethinking::dordlogit():\n\nprob_k <- model_int_only |> \n  tidy(effects = \"fixed\") |> \n  pull(estimate) |> \n  rethinking::dordlogit(1:7, 0, a = _)\nprob_k\n## [1] 0.12837374 0.09155969 0.10794372 0.23384904 0.14722269 0.14547466 0.14557646\n\nThese probabilities imply an average response outcome of 4ish:\n\nsum(prob_k * 1:7)\n## [1] 4.198717\n\nHere’s what that distribution looks like:\n\ntibble(prob_k = prob_k,\n       response = 1:7) |> \n  ggplot(aes(x = response, y = prob_k)) +\n  geom_col(width = 0.5, fill = clrs[4]) +\n  labs(title = \"Response probabilities\",\n       subtitle = \"Converted from original log odds in model\")\n\n\n\n\nIf the cumulative log odds increases each of these intercepts, the probability of lower values increases:\n\nprob_k_plus_5 <- model_int_only |> \n  tidy(effects = \"fixed\") |>\n  mutate(estimate = estimate + 0.5) |> \n  pull(estimate) |> \n  rethinking::dordlogit(1:7, 0, a = _)\n\nsum(prob_k_plus_5 * 1:7)\n## [1] 3.655766\n\ntibble(prob_k = prob_k_plus_5,\n       response = 1:7) |> \n  ggplot(aes(x = response, y = prob_k)) +\n  geom_col(width = 0.5, fill = colorspace::darken(clrs[4], 0.3)) +\n  labs(title = \"Response probabilities\",\n       subtitle = \"Log odds + 0.5\")\n\n\n\n\nAnd if the cumulative log odds decreases each of these intercepts, the probability of higher values increases:\n\nprob_k_minus_5 <- model_int_only |> \n  tidy(effects = \"fixed\") |>\n  mutate(estimate = estimate - 0.5) |> \n  pull(estimate) |> \n  rethinking::dordlogit(1:7, 0, a = _)\n\nsum(prob_k_minus_5 * 1:7)\n## [1] 4.729218\n\ntibble(prob_k = prob_k_minus_5,\n       response = 1:7) |> \n  ggplot(aes(x = response, y = prob_k)) +\n  geom_col(width = 0.5, fill = colorspace::lighten(clrs[4], 0.3)) +\n  labs(title = \"Response probabilities\",\n       subtitle = \"Log odds − 0.5\")\n\n\n\n\nWith a linear model, positive coefficients like \\(\\beta_1\\) increase the value of the outcome \\(\\phi\\), and we want that to match the direction of the shifts in category probabilities so that an increase in one of the \\(\\beta\\)s leads to a higher probability of bigger categories. To maintain that direction, we need to subtract \\(\\phi\\) from each of the intercepts. McElreath says this:\n\nThis way, a positive \\(\\beta\\) value indicates that an increase in the predictor variable \\(x\\) results in an increase in the average response (p. 387).\n\nAll this is just to say that in the formal model we use \\(\\operatorname{logit}(p_k) = \\alpha_k - \\phi\\). Neat."
  },
  {
    "objectID": "rethinking/10-video.html#model-with-covariates",
    "href": "rethinking/10-video.html#model-with-covariates",
    "title": "Video #11 code",
    "section": "Model with covariates",
    "text": "Model with covariates\nHere we’ll look at the effect of action, contact, and intention (whatever those are) on the distribution of possible responses to the trolley problem.\nFor fun, here’s the formal model in two different styles:\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Categorical}(p) \\\\\n\\operatorname{logit}(p_k) &= \\alpha_k - \\phi_i \\\\\n\\phi_i &= \\beta_1 \\text{Action}_i + \\beta_2 \\text{Contact}_i + \\beta_3 \\text{Intention}_i \\\\\n\\\\\nB_{1, 2, 3} &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\alpha_k &\\sim \\mathcal{N}(0, 1)\n\\end{aligned}\n\\]\nOrdered logit shortcut-y thing:\n\\[\n\\begin{aligned}\nR_i &\\sim \\operatorname{Ordered\\ logit}(\\phi_i, \\alpha) \\\\\n\\phi_i &= \\beta_1 \\text{Action}_i + \\beta_2 \\text{Contact}_i + \\beta_3 \\text{Intention}_i \\\\\n\\\\\nB_{1, 2, 3} &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\alpha_k &\\sim \\mathcal{N}(0, 1)\n\\end{aligned}\n\\]\nbrms time!\n\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b))\n\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5),\n              b = 0)\n\nmodel_aci <- brm(\n  bf(resp_ord ~ action + intention + contact),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  init = rep(list(inits), 4),\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", refresh = 0,\n  file = \"11-manual-cache/model-aci-only\"\n)\n\nThat takes a couple minutes to run, but it works great. To verify we can look at the trace plots and trank plots and see that everything converged.\n\nmodel_aci |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n\n\n\n\n\nmodel_aci |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank, color = factor(.chain))) +\n  stat_bin(geom = \"step\", binwidth = 250, position = position_identity(), boundary = 0) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\nThe Rhats and ESS are all fantastic too:\n\nmodel_aci\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ action + intention + contact \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -2.83      0.05    -2.92    -2.73 1.00     2892     2799\n## Intercept[2]    -2.15      0.04    -2.23    -2.06 1.00     3114     2989\n## Intercept[3]    -1.56      0.04    -1.64    -1.49 1.00     3421     3208\n## Intercept[4]    -0.54      0.04    -0.61    -0.47 1.00     3667     3307\n## Intercept[5]     0.12      0.04     0.05     0.20 1.00     3857     3736\n## Intercept[6]     1.03      0.04     0.95     1.11 1.00     4275     3232\n## action          -0.70      0.04    -0.78    -0.62 1.00     3835     3063\n## intention       -0.72      0.04    -0.79    -0.64 1.00     4295     3033\n## contact         -0.95      0.05    -1.04    -0.85 1.00     3861     2987\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nSo what the heck does this all mean? In all my past work with ordered logit models, I’ve just looked at predicted probabilities and marginal effects for each of the categories across a range of some predictor \\(X\\) because that seemed the easiest. Trying to interpret all these cumulative log odds scale things is really hard. We can inverse logit the cutpoints, but the coefficients for action, intention, and contact are all doing… something?… to them? Easiest way to see what’s going on is to simulate.\n\nsimulated_conditions <- tribble(\n  ~title, ~newdata,\n  \"A = 0, I = 0, C = 0\", tibble(action = 0, intention = 0, contact = 0),\n  \"A = 0, I = 1, C = 0\", tibble(action = 0, intention = 1, contact = 0),\n  \"A = 1, I = 0, C = 0\", tibble(action = 1, intention = 0, contact = 0),\n  \"A = 1, I = 1, C = 0\", tibble(action = 1, intention = 1, contact = 0),\n  \"A = 0, I = 0, C = 1\", tibble(action = 0, intention = 0, contact = 1),\n  \"A = 0, I = 1, C = 1\", tibble(action = 0, intention = 1, contact = 1)\n) |> \n  mutate(pred_plot = map2(newdata, title, ~{\n    model_aci |> \n      add_predicted_draws(newdata = .x) |> \n      ungroup() |> \n      count(.prediction) |> \n      mutate(prop = n / sum(n),\n             prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n      ggplot(aes(x = .prediction, y = n)) +\n      geom_col(aes(fill = .prediction)) +\n      geom_text(aes(y = 50, label = prop_nice), color = \"white\", size = 2.5, \n                angle = 90, hjust = 0) +\n      scale_y_continuous(labels = label_comma()) +\n      scale_fill_viridis_d(option = \"rocket\", end = 0.85, guide = \"none\") +\n      labs(x = \"Response\", y = \"Count\", \n           title = .y) +\n      theme(plot.title = element_text(size = rel(1), hjust = 0.5))\n  }))\n\nwrap_plots(simulated_conditions$pred_plot, nrow = 2, byrow = FALSE)\n\n\n\n\nWhen we move from all the conditions being off (A = 0, I = 0, C = 0) and then flip on Intention, the probability of the story seeming appropriate drops. Originally the most common response was a 7; now it’s a 4. Flipping A to 1 also increases the probability of lower categories, and using both A and I makes 1s really common. The same thing happens with C—flipping C to 1 increases the probability of 4, and when using both I and C, 1 becomes the most common response. The coefficients for all these covariates are negative, so enabling them shifts the distribution down and makes lower response values more likely."
  },
  {
    "objectID": "rethinking/10-video.html#model-with-covariates-and-gender-based-stratification",
    "href": "rethinking/10-video.html#model-with-covariates-and-gender-based-stratification",
    "title": "Video #11 code",
    "section": "Model with covariates and gender-based stratification",
    "text": "Model with covariates and gender-based stratification\nWe can include other competing causes here, like stratifying by gender. McElreath uses ulam() to directly stratify these terms by gender like bA[G]*A + bI[G]*I + bC[G]*C. We can do the same thing with brms either with some weird nl syntax or by including an interaction term with : instead of * (just like in video 4 here).\nHowever, neither the nl syntax nor the : approaches seem to work here. With regular regression, we can remove the intercept term (with 0 + ...) and get group-specific coefficients. With ordered logit, though, the model for \\(\\phi\\) doesn’t have an intercept, so including 0 + ... doesn’t work. The coefficients we get are still correct—the model isn’t wrong. It’s just that the pairs of coefficients aren’t the full values—one is an offset. Like with action, the mean of the two action coefficients look like this:\naction       -0.89\naction:male   0.35\nIn McElreath’s video, they look like this:\nbA[1]        -0.88\nbA[2]        -0.53\nThese are the same. It’s just that action:male is the offset from action when male is set to 1, making its coefficient -0.89 + 0.35 = -0.54, just like bA[2].\n\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b))\n\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5),\n              b = 0)\n\nmodel_aci_gender <- brm(\n  bf(resp_ord ~ action + action:male + intention + \n       intention:male + contact + contact:male),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  init = rep(list(inits), 4),\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", threads = threading(2),\n  file = \"11-manual-cache/model-aci-gender\", refresh = 0\n)\n\nBy default we get the offsets for male:\n\nmodel_aci_gender\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ action + action:male + intention + intention:male + contact + contact:male \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]      -2.85      0.05    -2.94    -2.75 1.00     3918     3277\n## Intercept[2]      -2.16      0.04    -2.25    -2.08 1.00     4372     3395\n## Intercept[3]      -1.58      0.04    -1.66    -1.50 1.00     4746     3541\n## Intercept[4]      -0.54      0.04    -0.62    -0.47 1.00     5084     3577\n## Intercept[5]       0.13      0.04     0.06     0.20 1.00     5203     4048\n## Intercept[6]       1.05      0.04     0.97     1.12 1.00     5526     4131\n## action            -0.89      0.05    -0.99    -0.79 1.00     3884     3272\n## intention         -0.90      0.05    -0.99    -0.81 1.00     4021     2910\n## contact           -1.07      0.07    -1.20    -0.93 1.00     3989     3030\n## action:male        0.35      0.06     0.23     0.46 1.00     4153     3340\n## intention:male     0.34      0.06     0.22     0.45 1.00     3300     3157\n## contact:male       0.21      0.08     0.04     0.38 1.00     3781     2904\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nBut we can wrangle the data a little to replicate what McElreath has with his precis() output:\n\n\n\n\n\n\nmodel_aci_gender |> \n  spread_draws(`^b_.*`, regex = TRUE) |> \n  mutate(`b_action:male` = b_action + `b_action:male`,\n         `b_intention:male` = b_intention + `b_intention:male`,\n         `b_contact:male` = b_contact + `b_contact:male`) |> \n  pivot_longer(starts_with(\"b_\"), names_to = \".variable\") |> \n  group_by(.variable) |> \n  summarize(avg = mean(value))\n## # A tibble: 12 × 2\n##    .variable           avg\n##    <chr>             <dbl>\n##  1 b_action         -0.890\n##  2 b_action:male    -0.544\n##  3 b_contact        -1.07 \n##  4 b_contact:male   -0.858\n##  5 b_intention      -0.901\n##  6 b_intention:male -0.563\n##  7 b_Intercept[1]   -2.85 \n##  8 b_Intercept[2]   -2.16 \n##  9 b_Intercept[3]   -1.58 \n## 10 b_Intercept[4]   -0.545\n## 11 b_Intercept[5]    0.131\n## 12 b_Intercept[6]    1.05\n\nCool cool. Everything here makes people rate things lower, but with different magnitudes—men shift less than women."
  },
  {
    "objectID": "rethinking/10-video.html#ordered-predictors-and-monotonic-effects",
    "href": "rethinking/10-video.html#ordered-predictors-and-monotonic-effects",
    "title": "Video #11 code",
    "section": "Ordered predictors and monotonic effects",
    "text": "Ordered predictors and monotonic effects\nAccording to the DAG in this example, we need to also control for age and education to close backdoors for identification. That seems straightforward enough—here’s how those two variables are distributed:\n\np1 <- trolley |> \n  ggplot(aes(x = edu)) +\n  stat_count(width = 0.5, fill = clrs[6]) +\n  scale_x_discrete(labels = label_wrap(15)) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Education\", y = \"Count\")\n\np2 <- trolley |> \n  ggplot(aes(x = age)) +\n  stat_count(fill = clrs[5]) +\n  labs(x = \"Age\", y = \"Count\")\n\np1 / p2\n\n\n\n\nAge is fine and continuous and we can just throw + age into the model. Education is a little trickier though. It’s an ordered category, which means it is analogous to the outcome variable on the lefthand side of the model. We don’t want to assume that the distance between each ordinal value in the response is the same; we also don’t want to assume that the distance between values in an ordinal predictor is the same either. Including a factor version of education in a regression model gives us category-specific coefficients (+ factor(educ)), this removes the inherent cumulative ordering (where someone with a doctorate also has some high school). Including a numeric version of education (+ as.numeric(educ)) in a regression model forces the correct cumulative ordering, but then it assumes that distance between 1→2 is the same as 5→6, and so on, and we don’t want that. We want to somehow maintain the ordered categorical properties of education on the righthand side of the model.\nThis is all super neat and new to me! I’ve never worried about the weirdness inherent in working with righthand side ordered variables and have just lived with their categorical coefficients. But there’s a magical way of dealing with this that mostly goes over my head and beyond my math skills. We can define the probability of specific levels of education as a series of cumulative cutpoints (super similar to what we do with categorical outcomes), like this:\n\n\n\n\n\n\n\nLevel\nEquation\n\n\n\n\nElementary school\n\\(\\phi = 0\\)\n\n\nMiddle school\n\\(\\phi = \\delta_1\\)\n\n\nSome high school\n\\(\\phi = \\delta_1 + \\delta_2\\)\n\n\nHigh school\n\\(\\phi = \\delta_1 + \\delta_2 + \\delta_3\\)\n\n\nSome college\n\\(\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4\\)\n\n\nCollege\n\\(\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4 + \\delta_5\\)\n\n\nMasters\n\\(\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4 + \\delta_5 + \\delta_6\\)\n\n\nDoctorate\n\\(\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4 + \\delta_5 + \\delta_6 + \\delta_7\\)\n\n\n\nWe can then collapse all these levels into a single sum that adds up to 1, creating something called a simplex (or a vector that adds to 1), and we can include that as a term in the regression model:\n\\[\n\\phi_i = \\beta_\\text{E} \\sum_{j = 0}^{\\text{E}_i - 1} \\delta_j\n\\]\nPhew, that’s wild. This new term takes a new set of priors, both for the \\(\\beta\\) coefficient and for each of the \\(\\delta\\) cutpoints. In the video and book, McElreath uses a Dirichlet prior for these \\(\\delta\\) values.\nMcElreath’s ulam() function requires manual construction of the simplex parameter for education, but brms can do this all automatically if we put the education term inside mo(), which forces it to be a monotonic categorical variable.\n\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(normal(0, 0.143), class = b, coef = moedu),\n            prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu1))\n\nmodel_monotonic <- brm(\n  bf(resp_ord ~ 1 + action + contact + intention + mo(edu)),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", threads = threading(2), \n  file = \"11-manual-cache/model-monotonic\"\n)\n\nOooh look at all these parameters now! We have a whole new section I’ve never seen before called “Simplex Parameters”\n\nmodel_monotonic\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ 1 + action + contact + intention + mo(edu) \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -3.12      0.17    -3.51    -2.84 1.00     1832     1857\n## Intercept[2]    -2.44      0.17    -2.83    -2.16 1.00     1834     1819\n## Intercept[3]    -1.86      0.17    -2.24    -1.58 1.00     1833     1941\n## Intercept[4]    -0.84      0.16    -1.22    -0.57 1.00     1860     1941\n## Intercept[5]    -0.17      0.16    -0.55     0.10 1.00     1864     2021\n## Intercept[6]     0.74      0.17     0.35     1.01 1.00     1814     1916\n## action          -0.70      0.04    -0.78    -0.62 1.00     3961     3329\n## contact         -0.95      0.05    -1.05    -0.86 1.00     3685     2854\n## intention       -0.72      0.04    -0.79    -0.65 1.00     4434     3307\n## moedu           -0.05      0.03    -0.11    -0.01 1.00     1852     1895\n## \n## Simplex Parameters: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## moedu1[1]     0.26      0.15     0.04     0.59 1.00     2300     2342\n## moedu1[2]     0.14      0.09     0.02     0.36 1.00     3906     2317\n## moedu1[3]     0.19      0.11     0.03     0.43 1.00     3915     2433\n## moedu1[4]     0.16      0.09     0.03     0.38 1.00     3701     2816\n## moedu1[5]     0.04      0.04     0.00     0.13 1.00     2793     2354\n## moedu1[6]     0.09      0.06     0.01     0.24 1.00     3433     2752\n## moedu1[7]     0.12      0.07     0.02     0.30 1.00     4205     3054\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe can even interact the education monotonic simplex thing with other variables like age:\n\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(normal(0, 0.143), class = b, coef = moedu),\n            prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu1))\n\nmodel_monotonic_age <- brm(\n  bf(resp_ord ~ action*male + intention*male + contact*male + \n       age*male + mo(edu)*male),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", threads = threading(2), \n  file = \"11-manual-cache/model-monotonic-age\"\n)\n\nThough it takes a while to fit these mo() models! (This is on an 8-core 2021 M1 MacBook Pro)\n\n# No interactions\nrstan::get_elapsed_time(model_monotonic$fit) |> \n  as_tibble(rownames = \"chain\") |> \n  mutate(total = lubridate::as.duration(warmup + sample))\n## # A tibble: 4 × 4\n##   chain   warmup sample total                   \n##   <chr>    <dbl>  <dbl> <Duration>              \n## 1 chain:1   294.   165. 459.86s (~7.66 minutes) \n## 2 chain:2   295.   136. 430.263s (~7.17 minutes)\n## 3 chain:3   279.   169. 448.288s (~7.47 minutes)\n## 4 chain:4   236.   189. 425.502s (~7.09 minutes)\n\n# With age*education interaction\nrstan::get_elapsed_time(model_monotonic_age$fit) |> \n  as_tibble(rownames = \"chain\") |> \n  mutate(total = lubridate::as.duration(warmup + sample))\n## # A tibble: 4 × 4\n##   chain   warmup sample total                    \n##   <chr>    <dbl>  <dbl> <Duration>               \n## 1 chain:1   407.   220. 626.811s (~10.45 minutes)\n## 2 chain:2   377.   255. 632.486s (~10.54 minutes)\n## 3 chain:3   449.   233. 681.639s (~11.36 minutes)\n## 4 chain:4   437.   218. 654.839s (~10.91 minutes)\n\nWild. Absolutely bonkers. I have no idea what to do with all these moving parts lol.\n\nmodel_monotonic_age\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ action * male + intention * male + contact * male + age * male + mo(edu) * male \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]      -3.12      0.26    -3.71    -2.69 1.00     1222     1722\n## Intercept[2]      -2.43      0.26    -3.02    -2.01 1.00     1226     1829\n## Intercept[3]      -1.85      0.26    -2.44    -1.43 1.00     1221     1750\n## Intercept[4]      -0.80      0.26    -1.39    -0.39 1.00     1225     1772\n## Intercept[5]      -0.11      0.26    -0.69     0.30 1.00     1231     1829\n## Intercept[6]       0.82      0.26     0.24     1.24 1.00     1240     1864\n## action            -0.55      0.06    -0.67    -0.44 1.00     2307     2667\n## male               0.53      0.19     0.13     0.87 1.00     1637     2178\n## intention         -0.66      0.05    -0.76    -0.56 1.00     2725     2680\n## contact           -0.77      0.07    -0.90    -0.63 1.00     2560     2896\n## age               -0.00      0.00    -0.00     0.00 1.00     5184     3384\n## action:male       -0.29      0.08    -0.45    -0.14 1.00     2192     2740\n## male:intention    -0.12      0.07    -0.27     0.02 1.00     2664     3138\n## male:contact      -0.37      0.09    -0.56    -0.18 1.00     2612     3033\n## male:age          -0.01      0.00    -0.01    -0.00 1.00     5852     2837\n## moedu             -0.12      0.04    -0.21    -0.06 1.00     1163     1756\n## moedu:male         0.13      0.03     0.07     0.19 1.00     1402     2017\n## \n## Simplex Parameters: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## moedu1[1]          0.32      0.14     0.06     0.58 1.00     1592     2122\n## moedu1[2]          0.10      0.06     0.01     0.25 1.00     3579     2303\n## moedu1[3]          0.15      0.08     0.03     0.35 1.00     4135     2789\n## moedu1[4]          0.20      0.08     0.06     0.39 1.00     2237     2240\n## moedu1[5]          0.03      0.02     0.00     0.07 1.00     2811     2136\n## moedu1[6]          0.15      0.07     0.04     0.30 1.00     2521     2965\n## moedu1[7]          0.06      0.04     0.01     0.18 1.00     3165     2425\n## moedu:male1[1]     0.07      0.06     0.00     0.22 1.00     3129     1703\n## moedu:male1[2]     0.17      0.11     0.01     0.41 1.00     2647     2341\n## moedu:male1[3]     0.19      0.11     0.01     0.41 1.00     2808     1838\n## moedu:male1[4]     0.04      0.03     0.00     0.13 1.00     3275     1825\n## moedu:male1[5]     0.31      0.10     0.15     0.53 1.00     2006     2331\n## moedu:male1[6]     0.19      0.09     0.04     0.37 1.00     2191     1994\n## moedu:male1[7]     0.03      0.03     0.00     0.12 1.00     4564     2097\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "rethinking/index.html",
    "href": "rethinking/index.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "General schedule:\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nReading\nVideos\n\n\n\n\n1\nBayesian inference\nChapters 1, 2, 3\n1, 2\n\n\n2\nLinear models & causal inference\nChapter 4\n3, 4\n\n\n3\nCauses, confounds, & colliders\nChapters 5, 6\n5, 6\n\n\n4\nOverfitting & interactions\nChapters 7, 8\n7, 8\n\n\n5\nMCMC & generalized linear models\nChapters 9, 10, 11\n9, 10\n\n\n6\nIntegers & other monsters\nChapters 11, 12\n11, 12\n\n\n7\nMultilevel models I\nChapter 13\n13, 14\n\n\n8\nMultilevel models II\nChapter 14\n15, 16\n\n\n9\nMeasurement & missingness\nChapter 15\n17, 18\n\n\n10\nGeneralized linear madness\nChapter 16\n19, 20"
  },
  {
    "objectID": "bayes-rules/06-stan/why.html",
    "href": "bayes-rules/06-stan/why.html",
    "title": "`bayesf22` Notebook",
    "section": "",
    "text": "# Make it so Stan chunks in Rmd files use cmdstanr instead of rstan\n# This works when knitting, but not when running RStudio interactively\n# register_knitr_engine()\n\n# See ?register_knitr_engine for more on how to make it work interactively\n#\n# For interactive work, we can use override = FALSE and then specify engine =\n# \"cmdstan\" in the stan chunk options\nregister_knitr_engine(override = FALSE)"
  }
]