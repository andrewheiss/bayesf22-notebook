[
  {
    "objectID": "bayes-rules/02-chapter.html",
    "href": "bayes-rules/02-chapter.html",
    "title": "Reading notes",
    "section": "",
    "text": "# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import article data\ndata(fake_news)\n\nperc <- scales::label_percent(accuracy = 1)\nperc2 <- scales::label_percent(accuracy = 0.01)\nHow many are fake vs. real?\n60/150 or 40% of news articles are fake .\nHow is the use of exclamation marks distributed across fake and real news articles?\n16/60 or 26.67% of news articles with !s are fake; only 2/90 or 2.22% of real articles with have !s.\nOur prior is thus that 40% of news articles are fake. We have new data, that !s are more common in fake news articles. So what’s the posterior if we find an article with a !?"
  },
  {
    "objectID": "bayes-rules/02-chapter.html#conditional-probabilities",
    "href": "bayes-rules/02-chapter.html#conditional-probabilities",
    "title": "Reading notes",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nWe have two variables:\n\nFake vs. real status\nUse of exclamation points\n\nThese features can vary at random across different articles, so we have to represent that randomness with probabilities models\n\nPrior probability model\nWe know from previous data that 40% are fake and 60% are real. If \\(B\\) means the article is fake, we can write that as\n\\[\nP(B) = 0.40 \\text{ and } P(B^c) = 0.60\n\\]\n\n\nConditional probability\nThe occurrence of !s depends on fakeness. Conditional probabilities of !s and fakeness, where \\(A\\) is the use of an exclamation mark:\n\\[\nP(A \\mid B) = 0.2667 \\text{ and } P(A \\mid B^c) = 0.0222\n\\]\nBy comparing conditional vs. unconditional probabilities, we learn how much \\(B\\) can inform our understanding of \\(A\\).\nAn event \\(A\\) might increase in probability given \\(B\\), like how the probability of joining an orchestra is greater if we know someonw practices daily:\n\\[\nP(\\text{join orchestra} \\mid \\text{practice daily}) > P(\\text{join orchestra})\n\\]\nOr the probability of getting the flu is lower if you know someone washes their hands a lot:\n\\[\nP(\\text{get flu} \\mid \\text{wash hands regularly}) < P(\\text{get flu})\n\\]\n\n\nLikelihood\nLikelihood is kind of like the inverse of probability (not really! just that the order of A and B matters)\n\nIf we know \\(B\\), the conditional probability \\(P(\\cdot \\mid B)\\) lets us compare the probabilities of an unknown event \\(A\\) (or \\(A^c\\)) occurring with \\(B\\), or\n\\[\nP(A \\mid B) \\text{ vs. } P(A^c \\mid B)\n\\]\nIf we know \\(A\\), the likelihood function \\(L(\\cdot \\mid A) = P(A \\mid \\cdot)\\) lets us compare the relative compatibility of data \\(A\\) with events \\(B\\) or \\(B^c\\)\n\\[\nL(B \\mid A) \\text{ vs. } L(B^c \\mid A)\n\\]\n\nWhat this looks like in practice, where \\(A\\) means having an exclamation mark and \\(B\\) means being fake:\n\n# Prior probability\nrow_prior <- fake_news %>% \n  count(type) %>% \n  mutate(prop = n / sum(n)) %>% \n  select(-n) %>% \n  pivot_wider(names_from = type, values_from = prop)\n\n# Likelihood\nrow_likelihood <- fake_news %>% \n  count(type, title_has_excl) %>% \n  pivot_wider(names_from = title_has_excl, values_from = n) %>% \n  mutate(likelihood = `TRUE` / (`TRUE` + `FALSE`)) %>% \n  select(-c(`FALSE`, `TRUE`)) %>%\n  pivot_wider(names_from = type, values_from = likelihood)\n\nbind_cols(Statistic = c(\"Prior probability\", \"Likelihood\"),\n          bind_rows(row_prior, row_likelihood)) %>% \n  mutate(Total = fake + real) %>% \n  rename(`Fake ($B$)` = fake, \n         `Real ($B^c$)` = real) %>% \n  knitr::kable(digits = 3)\n\n\n\n\nStatistic\nFake (\\(B\\))\nReal (\\(B^c\\))\nTotal\n\n\n\n\nPrior probability\n0.400\n0.600\n1.000\n\n\nLikelihood\n0.267\n0.022\n0.289\n\n\n\n\n\n\n\nNormalizing constants\nThe last piece we need is the marginal probability of observing exclamation points across all articles, or \\(P(A)\\), which is the normalizing constant, or \\(P(B)L(B \\mid A) + P(B^c)L(B^c \\mid A)\\)\n\nfake_news %>% \n  count(type, title_has_excl) %>% \n  mutate(prop = n / sum(n)) %>% \n  filter(title_has_excl == TRUE) %>% \n  summarize(normalizing_constant = sum(prop))\n##   normalizing_constant\n## 1                 0.12\n\n\n\nFinal analytical posterior\nThus, given this formula:\n\\[\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\\]\n…we have\n\\[\n\\text{posterior} = \\frac{0.4 \\times 0.2667}{0.12} = 0.889\n\\]"
  },
  {
    "objectID": "bayes-rules/02-chapter.html#simulation",
    "href": "bayes-rules/02-chapter.html#simulation",
    "title": "Reading notes",
    "section": "Simulation",
    "text": "Simulation\nWe can simulate this too\n\nsim_params <- tibble(type = c(\"real\", \"fake\"),\n                     prior = c(0.6, 0.4))\n\nset.seed(1234)\n\nsims <- sample(sim_params$type, size = 10000, prob = sim_params$prior, replace = TRUE) %>% \n  enframe(value = \"type\") %>% \n  mutate(data_model = case_when(type == \"fake\" ~ 0.2667,\n                                type == \"real\" ~ 0.0222)) %>% \n  rowwise() %>% \n  mutate(usage = sample(c(\"no\", \"yes\"), size = 1,\n                        prob = c(1 - data_model, data_model))) %>% \n  ungroup()\n\nsims %>% \n  tabyl(usage, type) %>% \n  adorn_totals(c(\"col\", \"row\"))\n##  usage fake real Total\n##     no 2914 5872  8786\n##    yes 1075  139  1214\n##  Total 3989 6011 10000\n\nggplot(sims, aes(x = type, fill = usage)) +\n  geom_bar(position = position_fill())\n\n\n\n\n# Posterior\nsims %>% \n  filter(usage == \"yes\") %>% \n  count(type) %>% \n  mutate(prop = n / sum(n))\n## # A tibble: 2 × 3\n##   type      n  prop\n##   <chr> <int> <dbl>\n## 1 fake   1075 0.886\n## 2 real    139 0.114"
  },
  {
    "objectID": "bayes-rules/02-chapter.html#chess-simulation",
    "href": "bayes-rules/02-chapter.html#chess-simulation",
    "title": "Reading notes",
    "section": "Chess simulation",
    "text": "Chess simulation\n\nchess <- c(0.2, 0.5, 0.8)\nprior <- c(0.1, 0.25, 0.65)\n\nset.seed(1234)\nchess_sim <- tibble(pi = sample(chess, size = 10000, prob = prior, replace = TRUE)) %>% \n  mutate(y = rbinom(n(), size = 6, prob = pi))\n\nchess_sim %>% \n  count(pi) %>% \n  mutate(prop = n / sum(n))\n## # A tibble: 3 × 3\n##      pi     n   prop\n##   <dbl> <int>  <dbl>\n## 1   0.2   986 0.0986\n## 2   0.5  2523 0.252 \n## 3   0.8  6491 0.649\n\nchess_sim %>% \n  ggplot(aes(x = y)) +\n  stat_count(aes(y = ..prop..)) +\n  facet_wrap(vars(pi))"
  },
  {
    "objectID": "bayes-rules/03-chapter.html",
    "href": "bayes-rules/03-chapter.html",
    "title": "Reading notes",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#beta-prior-model",
    "href": "bayes-rules/03-chapter.html#beta-prior-model",
    "title": "Reading notes",
    "section": "3.1: Beta prior model",
    "text": "3.1: Beta prior model\n\\[\n\\pi \\sim \\operatorname{Beta}(45, 55)\n\\]\n\nggplot() +\n  geom_function(fun = ~dbeta(., 45, 55), n = 1001) +\n  labs(title = \"dbeta(45, 55)\")\n\nggplot() +\n  geom_function(fun = ~dprop(., mean = 0.45, size = 100), n = 1001) +\n  labs(title = \"dprop(0.45, 100)\")"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#binomial-data-model-and-likelihood",
    "href": "bayes-rules/03-chapter.html#binomial-data-model-and-likelihood",
    "title": "Reading notes",
    "section": "3.2: Binomial data model and likelihood",
    "text": "3.2: Binomial data model and likelihood\n\\[\nY \\mid \\pi = \\operatorname{Binomial}(50, \\pi)\n\\]"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#beta-posterior-model",
    "href": "bayes-rules/03-chapter.html#beta-posterior-model",
    "title": "Reading notes",
    "section": "3.3: Beta posterior model",
    "text": "3.3: Beta posterior model\n\\[\n\\begin{aligned}\nY \\mid \\pi &= \\operatorname{Binomial}(50, \\pi) \\\\\n\\pi &\\sim \\operatorname{Beta}(45, 55)\n\\end{aligned}\n\\]\n\nmodel_election <- brm(\n  bf(support | trials(n_in_poll) ~ 0 + Intercept),\n  data = list(support = 30, n_in_poll = 50),\n  family = binomial(link = \"identity\"),\n  prior(beta(45, 55), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_election %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clrs[6]) +\n  labs(x = \"π\", title = \"Plausible values of π that could produce\\na poll where 30/50 voters support Michelle\")\n\n\n\n\n\nmodel_election %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dbeta(., 45, 55), aes(fill = \"Beta(45, 55) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6])"
  },
  {
    "objectID": "bayes-rules/03-chapter.html#milgrams-experiment",
    "href": "bayes-rules/03-chapter.html#milgrams-experiment",
    "title": "Reading notes",
    "section": "Milgram’s experiment",
    "text": "Milgram’s experiment\n\\[\n\\begin{aligned}\nY \\mid \\pi &= \\operatorname{Binomial}(40, \\pi) \\\\\n\\pi &\\sim \\operatorname{Beta}(1, 10)\n\\end{aligned}\n\\]\n\nmodel_milgram <- brm(\n  bf(obey | trials(participants) ~ 0 + Intercept),\n  data = list(obey = 26, participants = 40),\n  family = binomial(link = \"identity\"),\n  prior(beta(1, 10), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_milgram %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dbeta(., 1, 10), aes(fill = \"Beta(1, 10) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(0, 1))"
  },
  {
    "objectID": "bayes-rules/04-practice.html",
    "href": "bayes-rules/04-practice.html",
    "title": "Exercises",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)"
  },
  {
    "objectID": "bayes-rules/04-practice.html#section",
    "href": "bayes-rules/04-practice.html#section",
    "title": "Exercises",
    "section": "4.9.2",
    "text": "4.9.2\n\nThe local ice cream shop is open until it runs out of ice cream for the day. It’s 2 p.m. and Chad wants to pick up an ice cream cone. He asks his coworkers about the chance (π) that the shop is still open. Their Beta priors for π are below:\n\n\nworker_priors <- tribble(\n  ~coworker, ~prior,\n  \"Kimya\", \"Beta(1, 2)\",\n  \"Fernando\", \"Beta(0.5, 1)\",\n  \"Ciara\", \"Beta(3, 10)\",\n  \"Taylor\", \"Beta(2, 0.1)\"\n)\n\nworker_priors %>% knitr::kable()\n\n\n\n\ncoworker\nprior\n\n\n\n\nKimya\nBeta(1, 2)\n\n\nFernando\nBeta(0.5, 1)\n\n\nCiara\nBeta(3, 10)\n\n\nTaylor\nBeta(2, 0.1)\n\n\n\n\n\n\n4.4: Choice of prior\n\nVisualize and summarize (in words) each coworker’s prior understanding of Chad’s chances to satisfy his ice cream craving.\n\n\nworker_prior_densities <- worker_priors %>% \n  mutate(shapes = str_match_all(prior, \"Beta\\\\((\\\\d+\\\\.?\\\\d*), (\\\\d+\\\\.?\\\\d*)\\\\)\")) %>% \n  mutate(shape1 = map_dbl(shapes, ~as.numeric(.x[2])),\n         shape2 = map_dbl(shapes, ~as.numeric(.x[3]))) %>% \n  mutate(range = list(seq(0, 1, length.out = 1001))) %>% \n  mutate(density = pmap(list(range, shape1, shape2), ~dbeta(..1, ..2, ..3)))\n\nworker_prior_densities %>% \n  unnest(c(range, density)) %>% \n  # Truncate this a little for plotting\n  filter(range <= 0.99, range >= 0.01) %>%\n  ggplot(aes(x = range, y = density, fill = coworker, color = coworker)) +\n  geom_line(size = 1) +\n  geom_area(position = position_identity(), alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\nKimya is relatively uncertain, but leaning towards believing that π is low\nFernando puts heavy weight on very low probabilities of π—moreso than Kimya\nCiara believes that π is clustered around 20%, and no higher than 50%\nTaylor is the most optimistic, believing that π is exceptionally high (like 90+%)\n\n\n\n4.5 & 4.6: Simulating and identifying the posterior\n\nChad peruses the shop’s website. On 3 of the past 7 days, they were still open at 2 p.m.. Complete the following for each of Chad’s coworkers:\n\nsimulate their posterior model;\ncreate a histogram for the simulated posterior; and\nuse the simulation to approximate the posterior mean value of π\n\n\nand\n\nComplete the following for each of Chad’s coworkers:\n\nidentify the exact posterior model of π;\ncalculate the exact posterior mean of π; and\ncompare these to the simulation results in the previous exercise.\n\n\n\nCiara only\n\\[\n\\begin{aligned}\n(Y = 3) \\mid \\pi &= \\operatorname{Binomial}(7, \\pi) \\\\\n\\pi &\\sim \\operatorname{Beta}(3, 10)\n\\end{aligned}\n\\]\n\nBayes RulesGrid approximationbrmsRaw StanExact posterior\n\n\nThe Bayes Rules simulation approach is to simulate all the possibly likelihoods (here 0–7 / 7) and then filter to just choose one (3)\n\npi_ciara <- tibble(pi = rbeta(10000, 3, 10)) %>% \n  mutate(y = rbinom(10000, size = 7, prob = pi))\n\nggplot(pi_ciara, aes(x = pi, y = y)) +\n  geom_point(aes(color = (y == 3)))\n\n\n\n\n\npi_ciara %>% \n  filter(y == 3) %>% \n  summarize(across(pi, lst(mean, median)))\n## # A tibble: 1 × 2\n##   pi_mean pi_median\n##     <dbl>     <dbl>\n## 1   0.302     0.296\n\npi_ciara %>% \n  filter(y == 3) %>% \n  ggplot(aes(x = pi)) +\n  geom_density()\n\n\n\n\n\n\n\npi_grid <- tibble(pi_grid = seq(0, 1, length.out = 1001)) %>%\n  mutate(prior_beta = dbeta(pi_grid, 3, 10)) %>% \n  mutate(likelihood = dbinom(3, size = 7, prob = pi_grid)) %>% \n  mutate(posterior = (likelihood * prior_beta) / sum(likelihood * prior_beta))\n\npi_samples <- pi_grid %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE)\n\npi_samples %>% \n  summarize(across(pi_grid, lst(mean, median)))\n## # A tibble: 1 × 2\n##   pi_grid_mean pi_grid_median\n##          <dbl>          <dbl>\n## 1        0.301          0.295\n\nggplot(pi_samples, aes(x = pi_grid)) +\n  geom_density()\n\n\n\n\n\n\n\nmodel_pi <- brm(\n  bf(days_open | trials(weekdays) ~ 0 + Intercept),\n  data = list(days_open = 3, weekdays = 7),\n  family = binomial(link = \"identity\"),\n  prior(beta(3, 10), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n\n\nmodel_pi %>% \n  spread_draws(b_Intercept) %>%\n  summarize(across(b_Intercept, lst(mean, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 1 × 8\n##   b_Intercept_mean b_Intercept_median     y  ymin  ymax .width .point .interval\n##              <dbl>              <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>    \n## 1            0.298              0.292 0.292 0.110 0.493   0.95 median hdci\n\n\nmodel_pi %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dbeta(., 3, 10), aes(fill = \"Beta(3, 10) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(0, 1))\n\n\n\n\n\n\npi_stan.stan:\n\n// Things coming in from R\ndata {\n  int<lower=0> total_days;  // Possible days open (corresponds to binomial trials)\n  int<lower=0> days_open;  // Outcome variable\n}\n\n// Things to estimate\nparameters {\n  real<lower=0, upper=1> pi;  // Probability the store is open\n}\n\n// Models and distributions\nmodel {\n  // Prior\n  pi ~ beta(3, 10);\n  \n  // Likelihood\n  days_open ~ binomial(total_days, pi);\n  \n  // Internally, these ~ formulas really look like this:\n  //   target += beta_lpdf(pi | 3, 10);\n  //   target += binomial_lpmf(days_open | total_days, pi);\n  // The ~ notation is a lot nicer and maps onto the notation more directly\n}\n\n\nmodel_stan <- rstan::sampling(\n  pi_stan,\n  data = list(days_open = 3, total_days = 7),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4\n)\n\n\nmodel_stan %>% \n  spread_draws(pi) %>% \n  summarize(across(pi, lst(mean, median)))\n## # A tibble: 1 × 2\n##   pi_mean pi_median\n##     <dbl>     <dbl>\n## 1   0.298     0.292\n\n\nmodel_stan %>% \n  spread_draws(pi) %>% \n  ggplot(aes(x = pi)) +\n  stat_halfeye()\n\n\n\n\n\n\nFrom equation 3.10 in Bayes Rules!:\n\\[\n\\begin{aligned}\n\\pi \\mid (Y = y) &\\sim \\operatorname{Beta}(\\alpha + y, \\quad \\beta + n - y) \\\\\n\\pi \\mid (Y = 3) &\\sim \\operatorname{Beta}(3 + 3, \\quad 10 + 7 - 3) \\\\\n&\\sim \\operatorname{Beta}(6, 14)\n\\end{aligned}\n\\]\n\nggplot() +\n  geom_function(fun = ~dbeta(., 6, 14))\n\n\n\n\nAnd from equation 3.11 in Bayes Rules!:\n\\[\n\\begin{aligned}\nE(\\pi \\mid Y = y) &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\nE(\\pi \\mid Y = 3) &= \\frac{3 + 3}{3 + 10 + 7} \\\\\n&= \\frac{6}{20} = 0.3 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\operatorname{Mode}(\\pi \\mid Y = y) &= \\frac{\\alpha + y - 1}{\\alpha + \\beta + n - 2} \\\\\n\\operatorname{Mode}(\\pi \\mid Y = 3) &= \\frac{3 + 3 - 1}{3 + 10 + 7 - 2} \\\\\n&= \\frac{5}{18} = 0.27\\bar{7} \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\operatorname{Var}(\\pi \\mid Y = y) &= \\frac{(\\alpha + y)~(\\beta + n - y)}{(\\alpha + \\beta + n)^2~(\\alpha + \\beta + n + 1)} \\\\\n\\operatorname{Var}(\\pi \\mid Y = 3) &= \\frac{(3 + 3)~(10 + 7 - 3)}{(3 + 10 + 7)^2~(3 + 10 + 7 + 1)} \\\\\n&= \\frac{6 \\times 14}{20^2 \\times 21} = \\frac{84}{8,400} = 0.01 \\\\\n\\operatorname{SD}(\\pi \\mid Y = 3) &= \\sqrt{0.01} = 0.1\n\\end{aligned}\n\\]\nVerify using summarize_beta_binomial():\n\nsummarize_beta_binomial(alpha = 3, beta = 10, y = 3, n = 7)\n##       model alpha beta      mean      mode        var        sd\n## 1     prior     3   10 0.2307692 0.1818182 0.01267963 0.1126039\n## 2 posterior     6   14 0.3000000 0.2777778 0.01000000 0.1000000\n\n\n\n\n\n\nAll four coworkers\n(sans raw Stan; brms is fine and great for this anyway)\n\n# Clean this table of coworkers up a bit\npriors_clean <- worker_prior_densities %>% \n  select(coworker, prior, shape1, shape2)\n\npriors_clean %>% \n  knitr::kable()\n\n\n\n\ncoworker\nprior\nshape1\nshape2\n\n\n\n\nKimya\nBeta(1, 2)\n1.0\n2.0\n\n\nFernando\nBeta(0.5, 1)\n0.5\n1.0\n\n\nCiara\nBeta(3, 10)\n3.0\n10.0\n\n\nTaylor\nBeta(2, 0.1)\n2.0\n0.1\n\n\n\n\n\n\nBayes RulesGrid approximationbrmsExact posterior\n\n\n\nbr_simulation_pi <- priors_clean %>% \n  mutate(pi_sim = map2(shape1, shape2, ~{\n    tibble(pi = rbeta(10000, .x, .y)) %>% \n      mutate(y = rbinom(10000, size = 7, prob = pi))\n  }))\n\nbr_simulation_pi %>% \n  unnest(pi_sim) %>% \n  ggplot(aes(x = pi, y = y)) +\n  geom_point(aes(color = (y == 3))) +\n  facet_wrap(vars(coworker))\n\n\n\n\n\nbr_simulation_pi %>% \n  unnest(pi_sim) %>% \n  filter(y == 3) %>% \n  group_by(coworker) %>% \n  summarize(across(pi, lst(mean, median)))\n## # A tibble: 4 × 3\n##   coworker pi_mean pi_median\n##   <chr>      <dbl>     <dbl>\n## 1 Ciara      0.300     0.291\n## 2 Fernando   0.419     0.413\n## 3 Kimya      0.401     0.397\n## 4 Taylor     0.549     0.550\n\nbr_simulation_pi %>% \n  unnest(pi_sim) %>% \n  filter(y == 3) %>% \n  ggplot(aes(x = pi, fill = coworker, color = coworker)) +\n  geom_density(size = 1, alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\nCool cool. Fernando and Kimya’s flatter priors end up leading to posteriors of ≈40%. Taylor’s extreme optimism leads to a posterior mean of 57%! Ciara’s more reasonable range leads to a posterior of 30%.\n\n\n\ngrid_simulation_pi <- priors_clean %>% \n  mutate(pi_grid = list(seq(0.01, 0.99, length.out = 1001))) %>% \n  mutate(prior_beta = pmap(list(pi_grid, shape1, shape2), ~{\n    dbeta(..1, ..2, ..3)\n  })) %>% \n  mutate(likelihood = map(pi_grid, ~{\n    dbinom(3, size = 7, prob = .x)\n  })) %>% \n  mutate(posterior = map2(prior_beta, likelihood, ~{\n    (.y * .x) / sum(.y * .x)\n  })) \n\ngrid_simulation_samples <- grid_simulation_pi %>% \n  unnest(c(pi_grid, posterior)) %>% \n  group_by(coworker) %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE)\n\ngrid_simulation_samples %>% \n  group_by(coworker) %>% \n  summarize(across(pi_grid, lst(mean, median)))\n## # A tibble: 4 × 3\n##   coworker pi_grid_mean pi_grid_median\n##   <chr>           <dbl>          <dbl>\n## 1 Ciara           0.300          0.294\n## 2 Fernando        0.412          0.404\n## 3 Kimya           0.399          0.393\n## 4 Taylor          0.550          0.556\n\ngrid_simulation_samples %>% \n  ggplot(aes(x = pi_grid, fill = coworker, color = coworker)) +\n  geom_density(size = 1, alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\n\n\nbrms_pi <- priors_clean %>% \n  mutate(stan_prior = map2(shape1, shape2, ~{\n    prior_string(glue::glue(\"beta({.x}, {.y})\"), class = \"b\", lb = 0, ub = 1)\n  })) %>% \n  mutate(model = map(stan_prior, ~{\n    brm(\n      bf(days_open | trials(weekdays) ~ 0 + Intercept),\n      data = list(days_open = 3, weekdays = 7),\n      family = binomial(link = \"identity\"),\n      prior = .x,\n      iter = 5000, warmup = 1000, seed = BAYES_SEED,\n      backend = \"rstan\", cores = 4\n    )\n  }))\n## Compiling Stan program...\n## Start sampling\n## Compiling Stan program...\n## Start sampling\n## Compiling Stan program...\n## Start sampling\n## Compiling Stan program...\n## Start sampling\n\n\nbrms_pi %>% \n  mutate(draws = map(model, ~spread_draws(., b_Intercept))) %>% \n  unnest(draws) %>% \n  group_by(coworker) %>% \n  summarize(across(b_Intercept, lst(mean, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 4 × 9\n##   coworker b_Intercept_mean b_Intercep…¹     y  ymin  ymax .width .point .inte…²\n##   <chr>               <dbl>        <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>  \n## 1 Ciara               0.298        0.292 0.292 0.110 0.493   0.95 median hdci   \n## 2 Fernando            0.411        0.403 0.403 0.125 0.726   0.95 median hdci   \n## 3 Kimya               0.399        0.392 0.392 0.130 0.693   0.95 median hdci   \n## 4 Taylor              0.547        0.552 0.552 0.258 0.850   0.95 median hdci   \n## # … with abbreviated variable names ¹​b_Intercept_median, ²​.interval\n\n\nbrms_pi %>% \n  mutate(draws = map(model, ~gather_draws(., b_Intercept))) %>% \n  unnest(draws) %>% \n  ggplot(aes(x = .value, fill = coworker)) +\n  stat_halfeye(alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\n\n\nlik_y <- 3\nlik_n <- 7\n\nposteriors_manual <- priors_clean %>% \n  mutate(posterior_shape1 = shape1 + lik_y,\n         posterior_shape2 = shape2 + lik_n - lik_y)\n\nposteriors_manual %>% \n  mutate(posterior = glue::glue(\"Beta({posterior_shape1}, {posterior_shape2})\")) %>% \n  select(coworker, prior, posterior) %>% \n  knitr::kable()\n\n\n\n\ncoworker\nprior\nposterior\n\n\n\n\nKimya\nBeta(1, 2)\nBeta(4, 6)\n\n\nFernando\nBeta(0.5, 1)\nBeta(3.5, 5)\n\n\nCiara\nBeta(3, 10)\nBeta(6, 14)\n\n\nTaylor\nBeta(2, 0.1)\nBeta(5, 4.1)\n\n\n\n\n\n\nposteriors_manual_plot <- posteriors_manual %>% \n  mutate(range = list(seq(0.01, 0.99, length.out = 1001))) %>% \n  mutate(density = pmap(list(range, posterior_shape1, posterior_shape2), ~dbeta(..1, ..2, ..3)))\n\nposteriors_manual_plot %>% \n  unnest(c(range, density)) %>% \n  ggplot(aes(x = range, y = density, fill = coworker, color = coworker)) +\n  geom_line(size = 1) +\n  geom_area(position = position_identity(), alpha = 0.4) +\n  scale_fill_manual(values = clrs[c(1, 2, 3, 5)]) +\n  scale_color_manual(values = clrs[c(1, 2, 3, 5)])\n\n\n\n\n\nposteriors_manual_summary <- posteriors_manual %>% \n  group_by(coworker) %>% \n  summarize(summary = map2(shape1, shape2, ~{\n    summarize_beta_binomial(alpha = .x, beta = .y, y = lik_y, n = lik_n)\n  })) %>% \n  unnest(summary) \n\nposteriors_manual_summary %>% \n  select(-coworker) %>% \n  kableExtra::kbl(digits = 3) %>% \n  kableExtra::pack_rows(index = table(posteriors_manual_summary$coworker)) %>% \n  kableExtra::kable_styling()\n\n\n\n \n  \n    model \n    alpha \n    beta \n    mean \n    mode \n    var \n    sd \n  \n \n\n  Ciara\n\n    prior \n    3.0 \n    10.0 \n    0.231 \n    0.182 \n    0.013 \n    0.113 \n  \n  \n    posterior \n    6.0 \n    14.0 \n    0.300 \n    0.278 \n    0.010 \n    0.100 \n  \n  Fernando\n\n    prior \n    0.5 \n    1.0 \n    0.333 \n    1.000 \n    0.089 \n    0.298 \n  \n  \n    posterior \n    3.5 \n    5.0 \n    0.412 \n    0.385 \n    0.025 \n    0.160 \n  \n  Kimya\n\n    prior \n    1.0 \n    2.0 \n    0.333 \n    0.000 \n    0.056 \n    0.236 \n  \n  \n    posterior \n    4.0 \n    6.0 \n    0.400 \n    0.375 \n    0.022 \n    0.148 \n  \n  Taylor\n\n    prior \n    2.0 \n    0.1 \n    0.952 \n    1.000 \n    0.015 \n    0.121 \n  \n  \n    posterior \n    5.0 \n    4.1 \n    0.549 \n    0.563 \n    0.025 \n    0.157"
  },
  {
    "objectID": "bayes-rules/05-chapter.html",
    "href": "bayes-rules/05-chapter.html",
    "title": "Reading notes",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\nConjugate families are neat because they let us calculate exact posteriors without difficult integration, like the beta binomial trick:\n\\[\n\\pi \\mid (Y = y) \\sim \\operatorname{Beta}(\\alpha + y, \\quad \\beta + n - y)\n\\]"
  },
  {
    "objectID": "bayes-rules/05-chapter.html#gamma-poisson-conjugate-family",
    "href": "bayes-rules/05-chapter.html#gamma-poisson-conjugate-family",
    "title": "Reading notes",
    "section": "5.2 Gamma-Poisson conjugate family",
    "text": "5.2 Gamma-Poisson conjugate family\nUseful for modeling rates and counts, like fraudulent phone calls per day.\n\nRate of calls per day = \\(\\lambda\\). Any positive value.\nNumber of calls per day = \\(Y_i\\). Any non-negative integer.\n\n\nPoisson distributions\nSee this from program evaluation too, where I give up and say\n\nI have absolutely zero mathematical intuition for how [\\(\\lambda\\)] works. The two shape parameters for a Beta distribution at least fit in a fraction and you can wrap your head around that, but the lambda in a Poisson distribution is just a mystery to me.\n\nIn general, as the rate of events \\(\\lambda\\) increases…\n\nthe typical number of events increases,\nthe variability increases, and\nthe skew decreases\n\n\nexpand_grid(y = 0:12, lambda = c(1, 2, 5)) %>% \n  mutate(density = dpois(y, lambda)) %>% \n  ggplot(aes(x = y, y = density)) +\n  geom_col() + \n  facet_wrap(vars(lambda), \n             labeller = as_labeller(function(x) glue::glue(\"Poisson(λ = {x})\")))\n\n\n\n\n\n\nGamma distributions\nGamma models are positive and right skewed, and conjugate to Poisson. They take two hyperparameters: \\(s\\) (shape) and \\(r\\) (rate). Exponential models are Gammas with \\(s = 1\\).\n\nexpand_grid(y = seq(0, 7, length.out = 1001), s = c(1, 2, 4), r = c(1, 2)) %>% \n  mutate(density = dgamma(y, shape = s, rate = r)) %>% \n  mutate(panel_name = glue::glue(\"Gamma(s = {s}, r = {r})\"),\n         panel_name = fct_inorder(panel_name)) %>% \n  ggplot(aes(x = y, y = density)) +\n  geom_area() + \n  facet_wrap(vars(panel_name), dir = \"v\", nrow = 2)\n\n\n\n\nTuning the gamma prior: We think the rate of calls is 5 a day, with a range of 2–7ish. Through trial and error, it looks like \\(\\lambda \\sim \\operatorname{Gamma}(10, 2)\\) fits that well:\n\nggplot() +\n  geom_function(fun = ~dgamma(., shape = 10, rate = 2)) +\n  xlim(c(0, 15)) +\n  labs(x = \"λ\")\n\n\n\n\n\n\nGamma-Poisson conjugacy\nGamma and Poisson families work together like Beta and binomial!\nModel:\n\\[\n\\begin{aligned}\nY_i \\mid \\lambda &\\stackrel{\\text{ind}}{\\sim} \\operatorname{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\operatorname{Gamma}(s, r)\n\\end{aligned}\n\\]\nAnd the magical posterior based on the two distributions’ conjugacy:\n\\[\n\\lambda \\mid y \\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n)\n\\]\nAnd summary statistics:\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{s}{r} \\\\\n\\operatorname{Mode}(\\lambda) &= \\frac{s - 1}{r} \\\\\n\\operatorname{Var}(\\lambda) &= \\frac{s}{r^2} \\\\\n\\operatorname{SD}(\\lambda) &= \\sqrt{\\frac{s}{r^2}}\n\\end{aligned}\n\\]\nTime to try it!\nWe’ll assume that the daily rate of calls \\(\\lambda\\) is distributed with \\(\\operatorname{Gamma}(10, 2)\\). Over 4 days, we receive 6, 2, 2, and 1 calls. That’s an \\(n\\) of 4 and a \\(\\sum y_i\\) of (6 + 2 + 2 + 1), or 11, and an average of \\(\\frac{11}{4}\\), or 2.75.\nThe posterior model is then the \\(\\operatorname{Gamma}(10, 2)\\) prior mixed with the likelihood in fancy conjugate-y ways:\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (6, 2, 2, 1) &\\sim \\operatorname{Gamma}(10 + 11,\\quad 2 + 4) \\\\\n&\\sim \\operatorname{Gamma}(21, 6)\n\\end{aligned}\n\\]\nThis new data changes our understanding of the rate of calls per day:\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{10}{2} = 5 \\text{ calls a day, from prior} \\\\\nE[\\lambda \\mid (6, 2, 2, 1)] &= \\frac{21}{6} = 3.5 \\text{ calls a day, from posterior}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nSD(\\lambda) &= \\sqrt{\\frac{10}{2^2}} = 1.581 \\\\\nSD[\\lambda \\mid (6, 2, 2, 1)] &= \\sqrt{\\frac{21}{6^2}} = 0.764\n\\end{aligned}\n\\]\nAnd here’s what that looks like:\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 10, rate = 2), \n                geom = \"area\", aes(fill = \"Gamma(10, 2) prior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = (10 + 11), rate = (2 + 4)),\n                geom = \"area\", aes(fill = \"Gamma(21, 6) posterior\"), alpha = 0.75) +\n  xlim(c(0, 15)) +\n  scale_fill_manual(values = clrs[5:6])\n\n\n\n\nAnd confirming with the summarize_gamma_poisson() helper function:\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4)\n##       model shape rate mean     mode       var        sd\n## 1     prior    10    2  5.0 4.500000 2.5000000 1.5811388\n## 2 posterior    21    6  3.5 3.333333 0.5833333 0.7637626\n\nAnd confirming with brms, just because this conjugate prior stuff feels like dark magic:\n\nmodel_rate <- brm(\n  bf(fraud_calls ~ 0 + Intercept),\n  data = list(fraud_calls = c(6, 2, 2, 1)),\n  family = poisson(link = \"identity\"),\n  prior = prior(gamma(10, 2), class = b, lb = 0),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_rate %>% \n  spread_draws(b_Intercept) %>%\n  summarize(across(b_Intercept, lst(mean, sd, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 1 × 9\n##   b_Intercept_mean b_Intercept…¹ b_Int…²     y  ymin  ymax .width .point .inte…³\n##              <dbl>         <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>  \n## 1             3.50         0.756    3.44  3.44  2.09  5.00   0.95 median hdci   \n## # … with abbreviated variable names ¹​b_Intercept_sd, ²​b_Intercept_median,\n## #   ³​.interval\n\n\nmodel_rate %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dgamma(., 10, 2), aes(fill = \"Gamma(10, 2) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(0, 15))\n\n\n\n\nAHHH it works!"
  },
  {
    "objectID": "bayes-rules/05-chapter.html#normal-normal-conjugate-family",
    "href": "bayes-rules/05-chapter.html#normal-normal-conjugate-family",
    "title": "Reading notes",
    "section": "5.3: Normal-Normal conjugate family",
    "text": "5.3: Normal-Normal conjugate family\nGeneral story here: we’re interested in \\(\\mu\\)m or the average volume of the hippocampus. Wikipedia says that one half is between 3–3.5 cm3, so the total volume is between 6–7 cm3.\n\nNormal distributions\nNormal distribution defined with \\(\\mu\\) and \\(\\sigma\\) (I’ve got this intuition, but I’ll plot it anyway):\n\nexpand_grid(y = seq(-2, 10, length.out = 1001), \n            params = list(list(mu = 2, sigma = 0.5), \n                          list(mu = 2, sigma = 1),\n                          list(mu = 4, sigma = 2))) %>%\n  mutate(density = map2_dbl(y, params, ~dnorm(.x, .y$mu, .y$sigma))) %>% \n  mutate(panel_name = map_chr(params, ~glue::glue(\"N({.x$mu}, {.x$sigma})\"))) %>% \n  ggplot(aes(x = y, y = density)) +\n  geom_area() +\n  facet_wrap(vars(panel_name))\n\n\n\n\n\n\nNormal prior\nSo, if we think the volume of the hippocampus is 6.5 cm3, ± 0.8, we can do 6.5 ± (2 * 0.4), or:\n\\[\n\\mu \\sim \\mathcal{N}(6.5, 0.4^2)\n\\]\nHere’s what that looks like:\n\nggplot() +\n  geom_function(fun = ~dnorm(., mean = 6.5, sd = 0.4)) +\n  xlim(c(5, 8))\n\n\n\n\n\n\nNormal-normal conjugacy\nNormal-normal situations are conjugates, which means we can find exact posteriors without complex integration. This is a little more complicated than the nice easy Beta-binomial or even the Gamma-Poisson conjugates, though.\nHere’s the model:\n\\[\n\\begin{aligned}\nY_i \\mid \\mu &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}({\\mu, \\sigma^2}) \\\\\n\\mu &\\sim \\mathcal{N}(\\theta, \\tau^2)\n\\end{aligned}\n\\]\nAnd the magical posterior:\n\\[\n\\mu \\mid \\vec{y} \\; \\sim \\;  \\mathcal{N}\\bigg(\\theta\\frac{\\sigma^2}{n\\tau^2+\\sigma^2} + \\bar{y}\\frac{n\\tau^2}{n\\tau^2+\\sigma^2}, \\; \\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2}\\bigg)\n\\]\nWow that’s a mess. We need these things:\n\nPrior mean (\\(\\theta\\))\nPrior sd (\\(\\tau\\))\nObserved mean (\\(\\bar{y}\\))\nObserved sd (\\(\\sigma\\))\nNumber of observations (\\(n\\))\n\nLet’s try it with real data, with the football data from bayesrules. What’s the average hippocampus volume for football players with concussions? This is our \\(\\bar{y}\\).\n\nconcussion_subjects <- bayesrules::football %>% \n  filter(group == \"fb_concuss\")\n\nconcussion_subjects %>% \n  summarize(across(volume, lst(mean, sd)))\n##   volume_mean volume_sd\n## 1      5.7346 0.5933976\n\nIn the book, they look at the distribution and figure that a standard deviation of 0.5 seems reasonable (and it’s basically that in the data too)\n\nconcussion_subjects %>% \n  ggplot(aes(x = volume)) +\n  geom_density()\n\n\n\n\nWith that, we have all these pieces:\n\nPrior mean (\\(\\theta\\)): 6.5\nPrior sd (\\(\\tau\\)): 0.4\nObserved mean (\\(\\bar{y}\\)): 5.735\nObserved (assumed) sd (\\(\\sigma\\)): 0.5\nNumber of observations (\\(n\\)): 25\n\nMath time!\n\\[\n\\begin{aligned}\n\\mu \\mid \\vec{y} \\; &\\sim \\; \\mathcal{N}\\bigg(\\theta\\frac{\\sigma^2}{n\\tau^2+\\sigma^2} + \\bar{y}\\frac{n\\tau^2}{n\\tau^2+\\sigma^2}, \\; \\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2}\\bigg) \\\\\n&\\sim \\; \\mathcal{N}\\bigg(6.5\\frac{0.5^2}{25 \\times 0.4^2+0.5^2} + 5.735\\frac{25 \\times 0.4^2}{25 \\times 0.4^2+0.5^2}, \\; \\frac{0.4^2 \\times 0.5^2}{25 \\times 0.4^2+0.5^2}\\bigg) \\\\\n&\\sim \\; \\mathcal{N}\\bigg(5.78, 0.009^2\\bigg)\n\\end{aligned}\n\\]\nOr, with the summarize_normal_normal() helper function:\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,\n                        y_bar = 5.735, n = 25)\n##       model mean mode         var         sd\n## 1     prior 6.50 6.50 0.160000000 0.40000000\n## 2 posterior 5.78 5.78 0.009411765 0.09701425\n\nAnd here’s what that looks like:\n\nggplot() +\n  stat_function(fun = ~dnorm(., mean = 6.5, sd = 0.4), \n                geom = \"area\", aes(fill = \"N(6.5, 0.4) prior\"), alpha = 0.75,\n                n = 1001) +\n  stat_function(fun = ~dnorm(., mean = 5.78, sd = 0.097),\n                geom = \"area\", aes(fill = \"N(5.78, 0.097) posterior\"), alpha = 0.75,\n                n = 1001) +\n  xlim(c(5, 8)) +\n  scale_fill_manual(values = clrs[6:5], guide = guide_legend(reverse = TRUE))\n\n\n\n\nAnd confirming with brms:\n\nmodel_volume <- brm(\n  bf(volume ~ 0 + Intercept),\n  data = concussion_subjects,\n  family = gaussian(),\n  prior = prior(normal(6.5, 0.4), class = b),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_volume %>% \n  spread_draws(b_Intercept) %>%\n  summarize(across(b_Intercept, lst(mean, sd, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(b_Intercept_hdci)\n## # A tibble: 1 × 9\n##   b_Intercept_mean b_Intercept…¹ b_Int…²     y  ymin  ymax .width .point .inte…³\n##              <dbl>         <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>  \n## 1             5.80         0.124    5.80  5.80  5.57  6.05   0.95 median hdci   \n## # … with abbreviated variable names ¹​b_Intercept_sd, ²​b_Intercept_median,\n## #   ³​.interval\n\n\nmodel_volume %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dnorm(., 6.5, 0.4), aes(fill = \"N(6.5, 0.4) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(5, 8))\n\n\n\n\nSO COOL.\n\n\nOverriding the observed sd with an assumed sd\nBUT that’s not actually correct because it’s using the actual observed standard deviation (0.5934) instead of the assumed standard deviation (0.5) from the conjugate calculation earlier. I can’t figure out how to override brms’s sd, but we can use raw Stan:\nnormal_normal.stan:\n\ndata {\n  int<lower = 1> N;  // Number of observations\n  vector[N] volume;  // Observed hippocampus volumes\n  real volume_sd;    // Assumed sd of hippocampus volumes\n}\n\nparameters {\n  real mu;  // Posterior average hippocampus volume\n}\n\nmodel {\n  // Prior\n  mu ~ normal(6.5, 0.4);\n\n  // Likelihood\n  volume ~ normal(mu, volume_sd);\n}\n\n\nmodel_volume_stan <- rstan::sampling(\n  normal_normal,\n  data = list(volume = concussion_subjects$volume, \n              volume_sd = 0.5,\n              N = nrow(concussion_subjects)),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4\n)\n\nThe results are basically identical to the math-based version!\n\nmodel_volume_stan %>% \n  spread_draws(mu) %>%\n  summarize(across(mu, lst(mean, sd, median, hdci = ~median_hdci(., width = 0.89)))) %>% \n  unnest(mu_hdci)\n## # A tibble: 1 × 9\n##   mu_mean  mu_sd mu_median     y  ymin  ymax .width .point .interval\n##     <dbl>  <dbl>     <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>  <chr>    \n## 1    5.78 0.0978      5.78  5.78  5.59  5.98   0.95 median hdci\n\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,\n                        y_bar = 5.735, n = 25)\n##       model mean mode         var         sd\n## 1     prior 6.50 6.50 0.160000000 0.40000000\n## 2 posterior 5.78 5.78 0.009411765 0.09701425\n\nAnd the distribution is the same too:\n\nmodel_volume_stan %>% \n  gather_draws(mu) %>% \n  ggplot(aes(x = .value)) +\n  geom_density(aes(fill = \"Posterior\"), color = NA, alpha = 0.75) +\n  stat_function(geom = \"area\", fun = ~dnorm(., 6.5, 0.4), aes(fill = \"N(6.5, 0.4) prior\"), alpha = 0.75) +\n  scale_fill_manual(values = clrs[5:6]) +\n  xlim(c(5, 8))"
  },
  {
    "objectID": "bayes-rules/05-practice.html",
    "href": "bayes-rules/05-practice.html",
    "title": "Exercises",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234"
  },
  {
    "objectID": "bayes-rules/05-practice.html#practice-gamma-poisson",
    "href": "bayes-rules/05-practice.html#practice-gamma-poisson",
    "title": "Exercises",
    "section": "Practice: Gamma-Poisson",
    "text": "Practice: Gamma-Poisson\n\n5.1: Tuning a Gamma prior\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{s}{r} \\\\\n\\operatorname{Mode}(\\lambda) &= \\frac{s - 1}{r} \\\\\n\\operatorname{Var}(\\lambda) &= \\frac{s}{r^2}\n\\end{aligned}\n\\]\n\nThe most common value of λ is 4, and the mean is 7.\n\\[\n\\begin{cases}\n\\frac{s}{r} = 7 & [\\text{Mean}] \\\\\n\\frac{s-1}{r} = 4 & [\\text{Mode}] \\\\\n\\end{cases}\n\\]\nSolving the system of equations gives:\n\\[\n\\begin{aligned}\nr &= \\frac{1}{3} \\\\\ns &= \\frac{7}{3}\n\\end{aligned}\n\\]\nOr with code:\n\neq_mean <- function(r) 7 * r\neq_mode <- function(r) (4 * r) + 1\n\nr <- uniroot(function(x) eq_mean(x) - eq_mode(x), c(0, 100))$root\ns <- eq_mean(r)\n\nr; s\n## [1] 0.3333333\n## [1] 2.333333\n\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 7/3, rate = 1/3), geom = \"area\",\n                fill = clrs[2]) +\n  geom_vline(xintercept = 4, linetype = \"21\", color = clrs[6]) +\n  annotate(geom = \"label\", x = 4, y = 0.01, label = \"Mode\", \n           fill = clrs[6], color = \"white\") +\n  geom_vline(xintercept = 7, linetype = \"23\", color = clrs[3]) +\n  annotate(geom = \"label\", x = 7, y = 0.01, label = \"Mean\", \n           fill = clrs[3], color = \"white\") +\n  xlim(c(0, 20))\n\n\n\n\n\nThe most common value of λ is 10 and the mean is 12.\n\\[\n\\begin{cases}\n\\frac{s}{r} = 12 & [\\text{Mean}] \\\\\n\\frac{s-1}{r} = 10 & [\\text{Mode}] \\\\\n\\end{cases}\n\\]\nSolving the system of equations gives:\n\\[\n\\begin{aligned}\nr &= \\frac{1}{2} \\\\\ns &= 6\n\\end{aligned}\n\\]\nOr with code:\n\neq_mean <- function(r) 12 * r\neq_mode <- function(r) (10 * r) + 1\n\nr <- uniroot(function(x) eq_mean(x) - eq_mode(x), c(0, 100))$root\ns <- eq_mean(r)\n\nr; s\n## [1] 0.5\n## [1] 6\n\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 6, rate = 0.5), geom = \"area\",\n                fill = clrs[2]) +\n  geom_vline(xintercept = 10, linetype = \"21\", color = clrs[6]) +\n  annotate(geom = \"label\", x = 10, y = 0.01, label = \"Mode\", \n           fill = clrs[6], color = \"white\") +\n  geom_vline(xintercept = 12, linetype = \"23\", color = clrs[3]) +\n  annotate(geom = \"label\", x = 12, y = 0.02, label = \"Mean\", \n           fill = clrs[3], color = \"white\") +\n  xlim(c(0, 40))\n\n\n\n\n\nThe most common value of λ is 5, and the variance is 3.\n\\[\n\\begin{cases}\n\\frac{s-1}{r} = 5 & [\\text{Mode}] \\\\\n\\frac{s}{r^2} = 3 & [\\text{Variance}] \\\\\n\\end{cases}\n\\]\nSolving the system of equations gives:\n\\[\n\\begin{aligned}\nr &= \\frac{5 \\pm \\sqrt{37}}{6} \\\\\ns &= \\frac{31 \\pm 5\\sqrt{37}}{6}\n\\end{aligned}\n\\]\nlol k\n\nr <- (5 + sqrt(37)) / 6\ns <- (31 + 5*sqrt(37)) / 6\n\nr; s\n## [1] 1.847127\n## [1] 10.23564\n\nYep, it works:\n\ntable(round(rgamma(10000, shape = s, rate = r)))\n## \n##    1    2    3    4    5    6    7    8    9   10   11   12   13 \n##    2  149  845 1871 2427 2108 1338  692  355  143   42   22    6\nvar(rgamma(10000, shape = s, rate = r))\n## [1] 3.013767\n\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = s, rate = r), geom = \"area\",\n                fill = clrs[2]) +\n  geom_vline(xintercept = 5, linetype = \"21\", color = clrs[6]) +\n  annotate(geom = \"label\", x = 5, y = 0.01, label = \"Mode\",\n           fill = clrs[6], color = \"white\") +\n  xlim(c(0, 12))\n\n\n\n\n\nAnd so on… I get the point :)\n\n\n5.2: Poisson likelihood functions\nFrom equation 5.6 in Bayes Rules:\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{\\sum y_i}e^{-n\\lambda}\n\\]\nIn code:\n\nL <- function(lambda, sum_y, n) lambda^sum_y * exp(-n * lambda)\n\n(3, 7, 19)\n\nobserved_data <- c(3, 7, 19)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 3\n\\(\\sum y_i\\) = 29\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{29} e^{-3\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 20))\n\n\n\n\n\n(12, 12, 12, 0)\n\nobserved_data <- c(12, 12, 12, 0)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 4\n\\(\\sum y_i\\) = 36\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{36} e^{-4\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 20))\n\n\n\n\n\n(12)\n\nobserved_data <- c(12)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 1\n\\(\\sum y_i\\) = 12\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{12} e^{-\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 30))\n\n\n\n\n\n(16, 10, 17, 11, 11)\n\nobserved_data <- c(16, 10, 17, 11, 11)\nn <- length(observed_data)\nsum_y <- sum(observed_data)\n\n\n\\(n\\) = 5\n\\(\\sum y_i\\) = 65\n\n\\[\nL(\\lambda \\mid \\vec{y}) ~ \\propto ~ \\lambda^{65} e^{-5\\lambda}\n\\]\n\nggplot() +\n  stat_function(fun = ~L(., sum_y, n), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 20))\n\n\n\n\n\n\n5.3: Gamma-Poisson posteriors\nPrior:\n\\[\n\\lambda \\sim \\operatorname{Gamma}(24, 2)\n\\]\nModel:\n\\[\n\\begin{aligned}\nY_i \\mid \\lambda &\\stackrel{\\text{ind}}{\\sim} \\operatorname{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\operatorname{Gamma}(s, r)\n\\end{aligned}\n\\]\nConjugate posterior:\n\\[\n\\lambda \\mid y \\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n)\n\\]\n(3, 7, 19)\n\nobserved_data <- c(3, 7, 19)\n\n\n\\(n\\) = 3\n\\(\\sum y_i\\) = 29\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (3, 7, 19) &\\sim \\operatorname{Gamma}(24 + 29,\\quad 2 + 3) \\\\\n&\\sim \\operatorname{Gamma}(53, 5)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (3, 7, 19)] &= \\frac{53}{5} = 10.6 \\text{ from posterior}\n\\end{aligned}\n\\]\n\n(12, 12, 12, 0)\n\nobserved_data <- c(12, 12, 12, 0)\n\n\n\\(n\\) = 4\n\\(\\sum y_i\\) = 36\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (12, 12, 12, 0) &\\sim \\operatorname{Gamma}(24 + 36,\\quad 2 + 4) \\\\\n&\\sim \\operatorname{Gamma}(60, 6)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (12, 12, 12, 0)] &= \\frac{60}{6} = 10 \\text{ from posterior}\n\\end{aligned}\n\\]\n\n(12)\n\nobserved_data <- c(12)\n\n\n\\(n\\) = 1\n\\(\\sum y_i\\) = 12\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (12) &\\sim \\operatorname{Gamma}(24 + 12,\\quad 2 + 1) \\\\\n&\\sim \\operatorname{Gamma}(36, 3)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (12)] &= \\frac{36}{3} = 12 \\text{ from posterior}\n\\end{aligned}\n\\]\n\n(16, 10, 17, 11, 11)\n\nobserved_data <- c(16, 10, 17, 11, 11)\n\n\n\\(n\\) = 5\n\\(\\sum y_i\\) = 65\n\n\\[\n\\begin{aligned}\n\\lambda \\mid y &\\sim \\operatorname{Gamma}(s + \\sum y_i,\\quad r + n) \\\\\n\\lambda \\mid (16, 10, 17, 11, 11) &\\sim \\operatorname{Gamma}(24 + 65,\\quad 2 + 5) \\\\\n&\\sim \\operatorname{Gamma}(89, 7)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(\\lambda) &= \\frac{24}{2} = 12 \\text{ from prior} \\\\\nE[\\lambda \\mid (16, 10, 17, 11, 11)] &= \\frac{89}{7} = 12.714 \\text{ from posterior}\n\\end{aligned}\n\\]\n\nggplot() +\n  stat_function(fun = ~dgamma(., shape = 24, rate = 2), \n                geom = \"area\", aes(fill = \"Gamma(24, 2) prior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 53, rate = 5),\n                geom = \"area\", aes(fill = \"Gamma(53, 5) posterior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 60, rate = 6),\n                geom = \"area\", aes(fill = \"Gamma(60, 6) posterior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 36, rate = 3),\n                geom = \"area\", aes(fill = \"Gamma(36, 3) posterior\"), alpha = 0.75) +\n  stat_function(fun = ~dgamma(., shape = 89, rate = 7),\n                geom = \"area\", aes(fill = \"Gamma(89, 7) posterior\"), alpha = 0.75) +\n  xlim(c(5, 25)) +\n  scale_fill_manual(values = clrs[1:5])\n\n\n\n\n\n\n5.5 & 5.6: Text messages"
  },
  {
    "objectID": "bayes-rules/05-practice.html#practice-normal-normal",
    "href": "bayes-rules/05-practice.html#practice-normal-normal",
    "title": "Exercises",
    "section": "Practice: Normal-Normal",
    "text": "Practice: Normal-Normal\n\n5.8: Normal likelihood functions\n\n\n5.9 & 5.10: Investing in stock\n\n\n5.11: Normal-normal calculation\n\n\n5.12: Control brains"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bayesf22 Notebook",
    "section": "",
    "text": "My notebook for working through the readings and exercises in my Bayesian Statistics readings class in Fall 2022"
  },
  {
    "objectID": "rethinking/03-chapter.html",
    "href": "rethinking/03-chapter.html",
    "title": "Reading notes",
    "section": "",
    "text": "library(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(patchwork)\nlibrary(posterior)\nlibrary(broom.mixed)\n\nset.seed(1234)\nAssuming 9 globe tosses, 6 are water:\nOr in code:\nGiven this data, what’s the proportion of water on the globe?"
  },
  {
    "objectID": "rethinking/03-chapter.html#grid-approximation",
    "href": "rethinking/03-chapter.html#grid-approximation",
    "title": "Reading notes",
    "section": "Grid approximation",
    "text": "Grid approximation\nFor each possible value of \\(p\\), compute the product \\(\\operatorname{Pr}(W, L \\mid p) \\times \\operatorname{Pr}(p)\\). The relative sizes of each of those products are the posterior probabilities.\n\nBase R with Rethinking\n\nUniform flat prior\n\n# List of possible explanations of p to consider\np_grid <- seq(from = 0, to = 1, length.out = 10000)\nplot(p_grid, main = \"Possible proportions (p)\")\n#\n\n# Probability of each value of p\n# Super vague uniform prior: just 1 at each possible p\nprob_p_uniform <- rep(1, 10000)\nplot(prob_p_uniform, main = \"Uniform flat prior\")\n#\n\n# Probability of each proportion, given 6/9 water draws\nprob_data <- dbinom(6, size = 9, prob = p_grid)\n\n# Unnormalized posterior\nposterior_raw <- prob_data * prob_p_uniform\nplot(posterior_raw, main = \"Unnormalized posterior\")\n#\n\n# Normalized posterior that sums to 1\nposterior_normalized <- posterior_raw / sum(posterior_raw)\nplot(posterior_normalized, main = \"Normalized posterior\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeta prior\n\n# Beta distribution with 3 / (3 + 1)\nprob_p_beta <- dbeta(p_grid, shape1 = 3, shape2 = 1)\nplot(prob_p_beta, main = \"Beta(3, 1) prior\")\n#\n# Posterior that sums to 1\nposterior_normalized_beta <- (prob_data * prob_p_beta) / sum(posterior_raw)\nplot(posterior_normalized_beta, main = \"Normalized postiorior with beta prior\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse style from Solomon Kurz\n\nglobe_tossing <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1001),\n                        prior_uniform = 1) %>%  # prob_p_uniform from earlier\n  mutate(prior_beta = dbeta(p_grid, shape1 = 3, shape2 = 1)) %>%  # prob_p_beta from earlier\n  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%   # prob_data from earlier\n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform),\n         posterior_beta = (likelihood * prior_beta) / sum(likelihood * prior_beta))\nglobe_tossing\n## # A tibble: 1,001 × 6\n##    p_grid prior_uniform prior_beta likelihood posterior_uniform posterior_beta\n##     <dbl>         <dbl>      <dbl>      <dbl>             <dbl>          <dbl>\n##  1  0                 1  0           0                 0              0       \n##  2  0.001             1  0.000003    8.37e-17          8.37e-19       1.97e-24\n##  3  0.002             1  0.000012    5.34e-15          5.34e-17       5.04e-22\n##  4  0.003             1  0.0000270   6.07e-14          6.07e-16       1.29e-20\n##  5  0.004             1  0.000048    3.40e-13          3.40e-15       1.28e-19\n##  6  0.005             1  0.000075    1.29e-12          1.29e-14       7.62e-19\n##  7  0.006             1  0.000108    3.85e-12          3.85e-14       3.27e-18\n##  8  0.007             1  0.000147    9.68e-12          9.68e-14       1.12e-17\n##  9  0.008             1  0.000192    2.15e-11          2.15e-13       3.24e-17\n## 10  0.009             1  0.000243    4.34e-11          4.34e-13       8.30e-17\n## # … with 991 more rows\n\n\nglobe_tossing %>% \n  pivot_longer(starts_with(\"posterior\")) %>% \n  ggplot(aes(x = p_grid, y = value, fill = name)) +\n  geom_area(position = position_identity(), alpha = 0.5)"
  },
  {
    "objectID": "rethinking/03-chapter.html#working-with-the-posterior",
    "href": "rethinking/03-chapter.html#working-with-the-posterior",
    "title": "Reading notes",
    "section": "Working with the posterior",
    "text": "Working with the posterior\nWe now have a posterior! We typically can’t use the posterior alone. We have to average any inference across the entire posterior. This requires calculus, which is (1) hard, and (2) often impossible. So instead, we can use samples from the distribution and make inferences based on those.\n\n3.2: Sampling to summarize\nHere are 10,000 samples from the posterior (based on the uniform flat prior). These are the sampling distributions.\n\nsamples <- sample(p_grid, prob = posterior_normalized, size = 10000, replace = TRUE)\nplot(samples, main = \"10,000 posterior samples\")\n#\n\nplot(density(samples), main = \"Distribution of 10,000 posterior samples\")\n\n\n\n\n\n\n\n\n\n\n\n\nsamples_tidy <- globe_tossing %>% \n  slice_sample(n = 10000, weight_by = posterior_uniform, replace = T)\n\n\n3.2.1: Intervals of defined boundaries\n\nBase RTidyverse\n\n\nWhat’s the probability that the proportion of water is less than 50%?\n\nsum(samples < 0.5) / 10000\n## [1] 0.1745\n\nHow much of the posterior is between 50% and 75%?\n\nsum(samples > 0.5 & samples < 0.75) / 10000\n## [1] 0.6037\n\n\n\nWhat’s the probability that the proportion of water is less than 50%?\n\nglobe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_line() +\n  geom_area(data = filter(globe_tossing, p_grid < 0.5))\n\n\n\n\nsamples_tidy %>% \n  count(p_grid < 0.5) %>% \n  mutate(probability = n / sum(n))\n## # A tibble: 2 × 3\n##   `p_grid < 0.5`     n probability\n##   <lgl>          <int>       <dbl>\n## 1 FALSE           8321       0.832\n## 2 TRUE            1679       0.168\n\nHow much of the posterior is between 50% and 75%?\n\nglobe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_line() +\n  geom_area(data = filter(globe_tossing, p_grid > 0.5 & p_grid < 0.75))\n\n\n\n\nsamples_tidy %>% \n  count(p_grid > 0.5 & p_grid < 0.75) %>% \n  mutate(probability = n / sum(n))\n## # A tibble: 2 × 3\n##   `p_grid > 0.5 & p_grid < 0.75`     n probability\n##   <lgl>                          <int>       <dbl>\n## 1 FALSE                           4000         0.4\n## 2 TRUE                            6000         0.6\n\n\n\n\n\n\n3.2.2: Intervals of defined mass\n\nBase RTidyverse\n\n\nLower 80% posterior probability lies below this number:\n\nquantile(samples, 0.8)\n##      80% \n## 0.759576\n\nMiddle 80% posterior probability lies between these numbers:\n\nquantile(samples, c(0.1, 0.9))\n##       10%       90% \n## 0.4448445 0.8106911\n\n50% percentile interval vs. 50% HPDI\n\nquantile(samples, c(0.25, 0.75))\n##       25%       75% \n## 0.5416292 0.7387989\nrethinking::HPDI(samples, prob = 0.5)\n##      |0.5      0.5| \n## 0.5659566 0.7592759\n\n\n\nLower 80% posterior probability lies below this number:\n\nsamples_tidy %>% \n  summarize(`80th percentile` = quantile(p_grid, 0.8))\n## # A tibble: 1 × 1\n##   `80th percentile`\n##               <dbl>\n## 1             0.762\n\nMiddle 80% posterior probability lies between these numbers:\n\nsamples_tidy %>% \n  summarize(q = c(0.1, 0.9), percentile = quantile(p_grid, q)) %>% \n  pivot_wider(names_from = q, values_from = percentile)\n## # A tibble: 1 × 2\n##   `0.1` `0.9`\n##   <dbl> <dbl>\n## 1  0.45 0.811\n\n50% percentile interval vs. 50% HPDI\n\nsamples_tidy %>% \n  summarize(q = c(0.25, 0.75), \n            percentile = quantile(p_grid, q),\n            hpdi = HDInterval::hdi(p_grid, 0.5))\n## # A tibble: 2 × 3\n##       q percentile  hpdi\n##   <dbl>      <dbl> <dbl>\n## 1  0.25      0.544 0.563\n## 2  0.75      0.742 0.756\n\n\n\n\n\n\n3.2.3: Point estimates\n\nBase RTidyverse\n\n\n\nmean(samples)\n## [1] 0.6352952\nmedian(samples)\n## [1] 0.6448645\n\n\n\n\nsamples_tidy %>% \n  summarize(mean = mean(p_grid),\n            median = median(p_grid))\n## # A tibble: 1 × 2\n##    mean median\n##   <dbl>  <dbl>\n## 1 0.638  0.646\n\n\n\n\n\n\n\n3.3: Sampling to simulate prediction\n\nBase R\nWe can use the uncertainty inherent in the sampling distributions from above to generate a posterior predictive distribution, based on a 9-toss situation:\n\n# Posterior predictive distribution\nposterior_predictive_dist <- rbinom(10000, size = 9, prob = samples)\nhist(posterior_predictive_dist, breaks = 0:9)\n\n\n\n\n\n\nTidyverse style\n\n# Generate 100,000 samples from the posterior\nsamples_tidy <- globe_tossing %>% \n  slice_sample(n = 100000, weight_by = posterior_uniform, replace = T)\n#\nsamples_tidy %>% \n  mutate(sample_number = 1:n()) %>% \n  ggplot(aes(x = sample_number, y = p_grid)) +\n  geom_point(alpha = 0.05) +\n  labs(title = \"100,000 posterior samples\", x = \"Sample number\")\n#\nsamples_tidy %>% \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"grey50\", color = NA) +\n  labs(title = \"Distribution of 100,000 posterior samples\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6 with ggplot\n\n# Posterior probability\nglobe_smaller <- globe_tossing %>% \n  filter(p_grid %in% c(seq(0.1, 0.9, 0.1), 0.3))\n\npanel_top <- globe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) + \n  geom_area(fill = \"grey50\", color = NA) +\n  geom_segment(data = globe_smaller, aes(xend = p_grid, yend = 0, size = posterior_uniform)) +\n  geom_point(data = globe_smaller) +\n  scale_size_continuous(range = c(0, 1), guide = \"none\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"Proportion/probability of water\",\n       title = \"Posterior probability\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n\n# Sampling distributions\nglobe_sample_dists <- tibble(probability = seq(0.1, 0.9, 0.1)) %>% \n  mutate(draws = map(probability, ~{\n    set.seed(1234)\n    rbinom(10000, size = 9, prob = .x)\n  })) %>% \n  unnest(draws) %>% \n  mutate(label = paste0(\"p = \", probability))\n\npanel_middle <- ggplot(globe_sample_dists, aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.1) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = NULL, y = NULL, title = \"Sampling distributions\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\")) +\n  facet_wrap(vars(label), ncol = 9)\n\n# Posterior predictive distribution\nglobe_samples <- globe_tossing %>% \n  slice_sample(n = 10000, weight_by = posterior_uniform, replace = TRUE) %>% \n  mutate(prediction = map_dbl(p_grid, rbinom, n = 1, size = 9))\n\npanel_bottom <- globe_samples %>% \n  ggplot(aes(x = prediction)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.5) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = \"Number of water samples\", y = NULL, title = \"Posterior predictive distribution\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n\nlayout <- \"\nAAAAAAAAAAA\n#BBBBBBBBB#\n###CCCCC###\n\"\n\npanel_top / panel_middle / panel_bottom +\n  plot_layout(design = layout)"
  },
  {
    "objectID": "rethinking/03-chapter.html#brms-and-tidybayes-version-of-all-this",
    "href": "rethinking/03-chapter.html#brms-and-tidybayes-version-of-all-this",
    "title": "Reading notes",
    "section": "brms and tidybayes version of all this",
    "text": "brms and tidybayes version of all this\nOoh neat, you can pass single values as data instead of a data frame! Everything else here looks like regular brms stuff.\n\nmodel_globe <- brm(\n  bf(water | trials(9) ~ 0 + Intercept),\n  data = list(water = 6),\n  family = binomial(link = \"identity\"),\n  # Flat uniform prior\n  prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = 1234,\n  # TODO: Eventually switch to cmdstanr once this issue is fixed\n  # https://github.com/quarto-dev/quarto-cli/issues/2258\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\nCredible intervals / HPDI / etc.\n\n# Using broom.mixed\ntidy(model_globe, effects = \"fixed\",\n     conf.level = 0.5, conf.method = \"HPDinterval\")\n## # A tibble: 1 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   <chr>  <chr>     <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  cond      (Intercept)    0.639     0.137    0.568     0.761\n\n# Using the posterior package\ndraws <- as_draws_array(model_globe)\nsummarize_draws(draws, default_summary_measures()) %>% \n  filter(variable == \"b_Intercept\")\n## # A tibble: 1 × 7\n##   variable     mean median    sd   mad    q5   q95\n##   <chr>       <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 b_Intercept 0.639  0.648 0.137 0.144 0.400 0.848\n\n# Using tidybayes\n# get_variables(model_globe)\nmodel_globe %>% \n  spread_draws(b_Intercept) %>% \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.648  0.568  0.761   0.5  median hdci     \n## 2       0.648  0.430  0.864   0.89 median hdci     \n## 3       0.648  0.374  0.892   0.95 median hdci\n\nmodel_globe %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye()\n\n\n\n\nPredictions\n\nmodel_globe %>% \n  predicted_draws(newdata = tibble(nothing = 1)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.5) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = \"Number of water samples\", y = NULL, title = \"Posterior predictive distribution\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))"
  },
  {
    "objectID": "rethinking/03-practice.html",
    "href": "rethinking/03-practice.html",
    "title": "Exercises",
    "section": "",
    "text": "library(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(broom.mixed)\nlibrary(glue)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)\nThis births data shows the sex of the first and second children born to 100 different families (1 = boy, 2 = girl). The first family had a boy then a girl, the second had a girl and then a boy, the thrid had no boys, etc.\nIn these exercises we don’t care about birth order, so we’ll pool all the births into one long 200-birth vector:\nWhat proportion of births were boys?"
  },
  {
    "objectID": "rethinking/03-practice.html#h1",
    "href": "rethinking/03-practice.html#h1",
    "title": "Exercises",
    "section": "3H1",
    "text": "3H1\n\nUsing grid approximation, compute the posterior distribution for the probability of being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?\n\n\nbirth_grid <- tibble(p_grid = seq(0, 1, length.out = 1001),\n                     prior_uniform = 1) %>% \n  mutate(likelihood = dbinom(n_boys, size = total_births, prob = p_grid)) %>% \n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform))\nbirth_grid\n## # A tibble: 1,001 × 4\n##    p_grid prior_uniform likelihood posterior_uniform\n##     <dbl>         <dbl>      <dbl>             <dbl>\n##  1  0                 1  0                 0        \n##  2  0.001             1  2.48e-275         4.98e-276\n##  3  0.002             1  5.89e-242         1.18e-242\n##  4  0.003             1  1.89e-222         3.81e-223\n##  5  0.004             1  1.28e-208         2.57e-209\n##  6  0.005             1  6.68e-198         1.34e-198\n##  7  0.006             1  3.76e-189         7.56e-190\n##  8  0.007             1  9.28e-182         1.86e-182\n##  9  0.008             1  2.32e-175         4.66e-176\n## 10  0.009             1  1.01e-169         2.03e-170\n## # … with 991 more rows\n\n\nbirth_grid %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_area(fill = clrs[6])\n\n\n\n\nParameter that maximizes the probability:\n\nbirth_grid %>% \n  filter(posterior_uniform == max(posterior_uniform))\n## # A tibble: 1 × 4\n##   p_grid prior_uniform likelihood posterior_uniform\n##    <dbl>         <dbl>      <dbl>             <dbl>\n## 1  0.555             1     0.0567            0.0114\n\n\nWith brms\n\nmodel_births <- brm(\n  bf(boy | trials(total_births) ~ 0 + Intercept),\n  data = list(boy = n_boys, total_births = total_births),\n  family = binomial(link = \"identity\"),\n  prior = prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, cores = 4\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_births %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye(fill = clrs[6])\n\n\n\n\n\nmodel_births %>% \n  gather_draws(b_Intercept) %>% \n  summarize(median = median(.value))\n## # A tibble: 1 × 2\n##   .variable   median\n##   <chr>        <dbl>\n## 1 b_Intercept  0.555"
  },
  {
    "objectID": "rethinking/03-practice.html#h2",
    "href": "rethinking/03-practice.html#h2",
    "title": "Exercises",
    "section": "3H2",
    "text": "3H2\n\nUsing the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals.\n\n\nbirth_samples <- sample(birth_grid$p_grid, prob = birth_grid$posterior_uniform, \n                        size = 10000, replace = TRUE)\n\ntibble(x = birth_samples) %>% \n  ggplot(aes(x = x)) +\n  stat_slab(\n    aes(fill_ramp = stat(level)),\n        .width = c(0.02, 0.5, 0.89, 0.97, 1),\n    fill = clrs[3]\n  ) +\n  scale_fill_ramp_discrete(range = c(0.2, 1), guide = \"none\")\n\n\n\n\nHDInterval::hdi(birth_samples, credMass = 0.5)\n## lower upper \n## 0.530 0.577 \n## attr(,\"credMass\")\n## [1] 0.5\nHDInterval::hdi(birth_samples, credMass = 0.89)\n## lower upper \n## 0.494 0.605 \n## attr(,\"credMass\")\n## [1] 0.89\nHDInterval::hdi(birth_samples, credMass = 0.97)\n## lower upper \n## 0.473 0.626 \n## attr(,\"credMass\")\n## [1] 0.97\n\n\nWith brms\n\nmodel_births %>% \n  spread_draws(b_Intercept) %>% \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.97))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.555  0.532  0.578   0.5  median hdci     \n## 2       0.555  0.501  0.612   0.89 median hdci     \n## 3       0.555  0.480  0.629   0.97 median hdci\n\n\nmodel_births %>% \n  tidy_draws() %>% \n  ggplot(aes(x = b_Intercept)) +\n  stat_slab(\n    aes(fill_ramp = stat(level)),\n        .width = c(0.02, 0.5, 0.89, 0.97, 1),\n    fill = clrs[3]\n  ) +\n  scale_fill_ramp_discrete(range = c(0.2, 1), guide = \"none\")"
  },
  {
    "objectID": "rethinking/03-practice.html#h3",
    "href": "rethinking/03-practice.html#h3",
    "title": "Exercises",
    "section": "3H3",
    "text": "3H3\n\nUse rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a acount of boyts out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). … Does it look like the model fits the data well? That is, does the distribution of predcitions include the actual observation as a central, likely outcome?\n\nLooks good!\n\n# Posterior predictive distribution\nposterior_pred_births <- rbinom(10000, size = 200, prob = birth_samples)\n\nposterior_pred_births %>% \n  enframe() %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[1]) +\n  geom_vline(xintercept = n_boys, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys ({n_boys})\"))\n\n\n\n\n\nWith brms\n\nmodel_births %>% \n  predicted_draws(newdata = tibble(total_births = 200)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[1]) +\n  geom_vline(xintercept = n_boys, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys ({n_boys})\"))"
  },
  {
    "objectID": "rethinking/03-practice.html#h4",
    "href": "rethinking/03-practice.html#h4",
    "title": "Exercises",
    "section": "3H4",
    "text": "3H4\n\nNow compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?\n\nWe need to just look at first births:\n\nn_boys_first <- sum(birth1)\ntotal_births_first <- length(birth1)\n\nbirth_grid_first <- tibble(p_grid = seq(0, 1, length.out = 1001),\n                           prior_uniform = 1) %>% \n  mutate(likelihood = dbinom(n_boys_first, size = total_births_first, prob = p_grid)) %>% \n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform))\n\nfirst_samples <- sample(birth_grid_first$p_grid, prob = birth_grid_first$posterior_uniform, \n                        size = 10000, replace = TRUE)\n\nposterior_pred_first <- rbinom(10000, size = 100, prob = first_samples)\n\nposterior_pred_first %>% \n  enframe() %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[6]) +\n  geom_vline(xintercept = n_boys_first, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of firstborn boys ({n_boys_first})\"))\n\n\n\n\nLooks good still!\n\nmodel_births_first <- brm(\n  bf(boy | trials(total_births) ~ 0 + Intercept),\n  data = list(boy = n_boys_first, total_births = total_births_first),\n  family = binomial(link = \"identity\"),\n  prior = prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, cores = 4\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n\n\nWith brms\nLooks the same with brms too:\n\nmodel_births_first %>% \n  predicted_draws(newdata = tibble(total_births = 100)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[6]) +\n  geom_vline(xintercept = n_boys_first, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of firstborn boys ({n_boys_first})\"))"
  },
  {
    "objectID": "rethinking/03-practice.html#h5",
    "href": "rethinking/03-practice.html#h5",
    "title": "Exercises",
    "section": "3H5",
    "text": "3H5\n\nThe model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated conts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?\n\n\nn_girls_first <- length(birth1) - sum(birth1)\nn_boys_after_girls <- all_births %>%\n  filter(birth1 == 0) %>%  # All families with a firstborn girl\n  summarize(boy_after_girl = sum(birth2)) %>% \n  pull(boy_after_girl)\n\nposterior_pred_first_girl <- rbinom(10000, size = n_girls_first, prob = first_samples)\n\nposterior_pred_first_girl %>% \n  enframe() %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[5]) +\n  geom_vline(xintercept = n_boys_after_girls, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys born after girls ({n_boys_after_girls})\"))\n\n\n\n\n\nWith brms\n\nmodel_births_first_girls <- brm(\n  bf(girl | trials(total_births) ~ 0 + Intercept),\n  data = list(girl = n_girls_first, total_births = total_births_first),\n  family = binomial(link = \"identity\"),\n  prior = prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, cores = 4\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_births_first_girls %>% \n  predicted_draws(newdata = tibble(total_births = n_girls_first)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 2, color = \"white\", size = 0.25, fill = clrs[5]) +\n  geom_vline(xintercept = n_boys_after_girls, color = \"red\") +\n  labs(caption = glue(\"Red line shows actual observed number of boys born after girls ({n_boys_after_girls})\"))"
  },
  {
    "objectID": "rethinking/03-video.html#linear-regression",
    "href": "rethinking/03-video.html#linear-regression",
    "title": "Video #3 code",
    "section": "Linear regression",
    "text": "Linear regression\nThe general process for drawing the linear regression owl:\n\nQuestion/goal/estimand\nScientific model\nStatistical model(s)\nValidate model\nAnalyze data\n\n\n1. Question/goal/estimand\nWe want to describe the association between adult weight and height\n\nggplot(d, aes(x = height, y = weight)) +\n  geom_point(color = clrs[3])\n\n\n\n\n\n\n2. Scientific model\nHow does height influence weight?\nHeight has a causal relationship with weight:\n\\[\nH \\rightarrow W\n\\]\nWeight is a function of height. Plug in some value of height, get some weight:\n\\[\nW = f(H)\n\\]\nWe need to write down that model somehow. The normal \\(y = mx + b\\) way of writing a linear model looks like this, where \\(\\alpha\\) is the intercept and \\(\\beta\\) is the slope:\n\\[\ny_i = \\alpha + \\beta x_i\n\\]\nWe can also write it like this, where \\(\\mu\\) is the expectation (\\(E(y \\mid x) = \\mu\\)), and \\(\\sigma\\) is the standard deviation:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\n\\end{aligned}\n\\]\nSo, we can think of a generative model for height causing weight like this:\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta H_i\n\\end{aligned}\n\\]\nThis can map directly to code:\n\nset.seed(1234)\n\nalpha <- 0\nbeta <- 0.5\nsigma <- 5\nn_individuals <- 100\n\nfake_people <- tibble(height = runif(n_individuals, 130, 170),\n                      mu = alpha + beta*height,\n                      weight = rnorm(n_individuals, mu, sigma))\n\nlm(weight ~ height, data = fake_people) %>% \n  tidy()\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    1.22     6.37       0.191 8.49e- 1\n## 2 height         0.494    0.0431    11.5   7.64e-20\n\nggplot(fake_people, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n3: Statistical model\n\nSimulating and checking the priors\nWe don’t actually know \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) - they all have priors and limits and bounds. For instance, \\(\\sigma\\) is a scale parameter - it shifts distributions up and down - it has to be positive (hence the uniform distribution here). We can write a generic model like this:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 1)\n\\end{aligned}\n\\]\nBut if we sample from that prior distribution, we get lines that are all over the place!\n\nset.seed(1234)\n\nn_samples <- 10\n\ntibble(alpha = rnorm(n_samples, 0, 1),\n       beta = rnorm(n_samples, 0, 1)) %>% \n  ggplot() +\n  geom_abline(aes(slope = beta, intercept = alpha)) +\n  xlim(c(-2, 2)) + ylim(c(-2, 2))\n\n\n\n\nInstead, we can set some more specific priors and rescale variables so that the intercept makes more sense.\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (H_i - \\bar{H}) \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\nHere’s what those priors look like:\n\nPlain old ggplotbrms::prior() and ggdist::parse_dist()\n\n\n\nplot_prior_alpha <- ggplot() +\n  stat_function(fun = ~dnorm(., 60, 10), geom = \"area\", fill = clrs[1]) +\n  xlim(c(30, 90)) +\n  labs(title = \"Normal(60, 10)\", subtitle = \"Prior for intercept (α)\", caption = \"Average adult weight\")\n\nplot_prior_beta <- ggplot() +\n  stat_function(fun = ~dnorm(., 0, 10), geom = \"area\", fill = clrs[2]) +\n  xlim(c(-30, 30)) +\n  labs(title = \"Normal(0, 10)\", subtitle = \"Prior for slope (β)\", caption = \"kg per cm\")\n\nplot_prior_sigma <- ggplot() +\n  stat_function(fun = ~dunif(., 0, 10), geom = \"area\", fill = clrs[3]) +\n  xlim(c(0, 10)) +\n  labs(title = \"Uniform(0, 10)\", subtitle = \"Prior for sd (σ)\")\n\nplot_prior_alpha | plot_prior_beta | plot_prior_sigma\n\n\n\n\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\npriors %>% \n  parse_dist() %>% \n  ggplot(aes(y = 0, dist = .dist, args = .args, fill = prior)) +\n  stat_slab() +\n  scale_fill_manual(values = clrs[1:3]) +\n  facet_wrap(vars(prior), scales = \"free\")\n\n\n\n\n\n\n\nLet’s see how well those priors work!\n\nManual plottingbrms and sample_prior = \"only\"\n\n\n\nset.seed(1234)\n\nn <- 100\nHbar <- 150\nHseq <- seq(130, 170, length.out = 30)\n\ntibble(alpha = rnorm(n, 60, 10),\n       beta = rnorm(n, 0, 10)) %>% \n  mutate(weight = map2(alpha, beta, ~.x + .y*(Hseq - Hbar)),\n         height = list(Hseq),\n         id = 1:n) %>% \n  unnest(c(weight, height)) %>% \n  ggplot(aes(x = height, y = weight)) + \n  geom_line(aes(group = id), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_prior_only_normal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\"\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 100)) %>% \n  add_epred_draws(height_weight_prior_only_normal, ndraws = 100) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\ndraws_prior %>% \n  ggplot(aes(x = height_unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\nLots of those lines are completely implausible, so we’ll use a lognormal β prior instead:\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (H_i - \\bar{H}) \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\nThe lognormal distribution forces βs to be > 0 and it’s clusttered down at low values like 1ish:\n\nggplot() +\n  stat_function(fun = ~dlnorm(., 0, 1), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 5)) +\n  labs(x = \"Simulated β values\", y = \"Density\")\n\n\n\n\n\nManual plottingbrms and sample_prior = \"only\"\n\n\n\nset.seed(1234)\n\nn <- 100\nHbar <- 150\nHseq <- seq(130, 170, length.out = 30)\n\ntibble(alpha = rnorm(n, 60, 10),\n       beta = rlnorm(n, 0, 1)) %>% \n  mutate(weight = map2(alpha, beta, ~.x + .y*(Hseq - Hbar)),\n         height = list(Hseq),\n         id = 1:n) %>% \n  unnest(c(weight, height)) %>% \n  ggplot(aes(x = height, y = weight)) + \n  geom_line(aes(group = id), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_prior_only_lognormal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 100)) %>% \n  add_epred_draws(height_weight_prior_only_lognormal, ndraws = 100) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\ndraws_prior %>% \n  ggplot(aes(x = height_unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n\n\n\n\n\n\n\n\n\nFitting the model\nFitting the model is super complex because it’s the joint posterior of all those distributions:\n\\[\n\\begin{aligned}\n\\operatorname{Pr}(\\alpha, \\beta, \\sigma \\mid W, H) \\propto~ & \\mathcal{N}(W \\mid \\mu, \\sigma) \\\\\n&\\times \\mathcal{N}(\\alpha \\mid 60, 10) \\\\\n&\\times \\operatorname{LogNormal}(\\beta \\mid 0, 1) \\\\\n&\\times \\operatorname{Uniform}(\\sigma \\mid 0, 10)\n\\end{aligned}\n\\]\nIf we do this with grid approximation, 100 values of each parameter = 1 million calculations. In the book he shows it with grid approximation and with quadratic approximation.\nI’ll do it with brms and Stan instead.\n\nbrmsStan\n\n\n\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_lognormal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n\n\nprint(height_weight_lognormal)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 1 + height_z \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    45.05      0.23    44.61    45.50 1.00     3979     3023\n## height_z      4.83      0.23     4.38     5.29 1.00     3802     2716\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.27      0.17     3.96     4.62 1.00     4119     2987\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nThis is a little trickier because add_epred_draws() and predicted_draws() and all those nice functions don’t work with raw Stan samples, by design.\nInstead, we need to generate predictions that incorporate sigma using a generated quantities block in Stan (see the official Stan documentation and this official example and this neat blog post by Monica Alexander and this Medium post).\nHere we return two different but similar things: weight_rep[i] for the posterior predictive (which corresponds to predicted_draws() in tidybayes) and mu[i] for the expectation of the posterior predictive (which corresponds to epred_draws() in tidybayes):\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] height;  // Explanatory variable\n  vector[n] weight;  // Outcome variable\n  real height_bar;   // Average height\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  real beta;\n  real alpha;\n}\n\ntransformed parameters {\n  // Regression-y model of mu with scaled height\n  vector[n] mu;\n  mu = alpha + beta * (height - height_bar);\n}\n\nmodel {\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  alpha ~ normal(60, 10);\n  beta ~ lognormal(0, 1);\n  sigma ~ uniform(0, 10);\n}\n\ngenerated quantities {\n  // Generate a posterior predictive distribution\n  vector[n] weight_rep;\n\n  for (i in 1:n) {\n    // Calculate a new predicted mu for each iteration here\n    real mu_hat_n = alpha + beta * (height[i] - height_bar);\n    weight_rep[i] = normal_rng(mu_hat_n, sigma);\n    \n    // Alternatively, we can use mu[i] from the transformed parameters\n    // weight_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n\n\n# compose_data listifies things for Stan\n# stan_data <- d %>% compose_data()\n# stan_data$height_bar <- mean(stan_data$height)\n\n# Or we can manually build the list\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  height_bar = mean(d$height))\n\nmodel_lognormal_stan <- rstan::sampling(\n  object = lognormal_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_lognormal_stan, \n      pars = c(\"sigma\", \"beta\", \"alpha\", \"mu[1]\", \"mu[2]\"))\n## Inference for Stan model: 3c0d0064926253eea8547fd513e8019e.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  4.27       0 0.16  3.97  4.16  4.27  4.38  4.60 13529    1\n## beta   0.62       0 0.03  0.57  0.60  0.62  0.64  0.68 15170    1\n## alpha 45.06       0 0.23 44.61 44.90 45.05 45.21 45.51 15781    1\n## mu[1] 43.26       0 0.24 42.78 43.09 43.26 43.42 43.74 15594    1\n## mu[2] 35.72       0 0.50 34.73 35.39 35.72 36.06 36.69 15326    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep  8 13:44:25 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\n\n\n\n\n\n\n4 & 5: Validate model and analyze data\n\nbrmsStan\n\n\nExpectation of the posterior (plotting uncertainty of the mean):\n\ndraws_posterior_epred <- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 100)) %>% \n  add_epred_draws(height_weight_lognormal, ndraws = 50) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  geom_line(data = draws_posterior_epred,\n            aes(x = height_unscaled, y = .epred, group = .draw), alpha = 0.2, color = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nPosterior predictions (plotting uncertainty of the predictions):\n\ndraws_posterior_pred <- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 500)) %>% \n  add_predicted_draws(height_weight_lognormal, ndraws = 100) %>%\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  stat_lineribbon(data = draws_posterior_pred,\n                  aes(x = height_unscaled, y = .prediction), .width = 0.95, \n                  alpha = 0.2, color = clrs[5], fill = clrs[5]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nAnd here’s the posterior predictive check, just for fun:\n\npp_check(height_weight_lognormal, type = \"dens_overlay\", ndraws = 25)\n\n\n\n\n\n\nSummarized predictions:\n\npredicted_values_lognormal <- model_lognormal_stan %>% \n  spread_draws(mu[i], weight_rep[i]) %>%\n  mean_qi() %>%\n  mutate(weight = d$weight,\n         height = d$height)\n\nExpectation of the posterior (plotting uncertainty of the mean):\n\nggplot(predicted_values_lognormal, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_line(aes(y = mu), color = clrs[6]) +\n  geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.2, fill = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nPosterior predictions (plotting uncertainty of the predictions):\n\nggplot(predicted_values_lognormal, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_line(aes(y = mu), color = clrs[5]) +\n  geom_ribbon(aes(ymin = weight_rep.lower, ymax = weight_rep.upper), alpha = 0.2, fill = clrs[5]) +\n  coord_cartesian(ylim = c(30, 65))\n\n\n\n\nAnd here’s the posterior predictive check, just for fun:\n\nyrep1 <- rstan::extract(model_lognormal_stan)[[\"weight_rep\"]]\nsamp25 <- sample(nrow(yrep1), 25)\nbayesplot::ppc_dens_overlay(d$weight, yrep1[samp25, ])"
  },
  {
    "objectID": "rethinking/04-video.html",
    "href": "rethinking/04-video.html",
    "title": "Video #4 code",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(ggdag)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)\n\n# Data\ndata(Howell1, package = \"rethinking\")\n\nd <- Howell1 %>% \n  filter(age > 18) %>% \n  # Stan doesn't like working with columns with attributes, but I want to keep\n  # the attributes for unscaling later, so there are two scaled height columns\n  mutate(height_scaled = scale(height),\n         height_z = as.numeric(height_scaled)) %>% \n  mutate(sex = factor(male),\n         sex_nice = factor(male, labels = c(\"Female\", \"Male\")))\n\nheight_scale <- attributes(d$height_scaled) %>% \n  set_names(janitor::make_clean_names(names(.)))\n\n\nhead(Howell1)\n##    height   weight age male\n## 1 151.765 47.82561  63    1\n## 2 139.700 36.48581  63    0\n## 3 136.525 31.86484  65    0\n## 4 156.845 53.04191  41    1\n## 5 145.415 41.27687  51    0\n## 6 163.830 62.99259  35    1\n\nggplot(d, aes(x = height, y = weight, color = sex_nice)) +\n  geom_point() +\n  scale_color_manual(values = clrs[1:2]) +\n  labs(x = \"Height (cm)\", y = \"Weight (kg)\", color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nSex only\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{S[i]} \\\\\n\\\\\n\\alpha_j &\\sim \\mathcal{N}(60, 10) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\n\nbrmsStan\n\n\nCreate a model with no intercept; use a factor version of sex to get the indexes like he does with \\(\\alpha_{S[i]}\\).\n\npriors <- c(prior(normal(60, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nsex_weight <- brm(\n  bf(weight ~ 0 + sex),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nsex_weight\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 0 + sex \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sex0    41.87      0.41    41.08    42.65 1.00     3580     3109\n## sex1    48.63      0.43    47.81    49.49 1.00     4090     2920\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     5.52      0.22     5.12     5.98 1.00     4135     2820\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPosterior mean weights:\n\nsw_post_means <- sex_weight %>% \n  gather_draws(b_sex0, b_sex1)\n\nsw_post_means %>% \n  mean_hdci()\n## # A tibble: 2 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 b_sex0      41.9   41.1   42.7   0.95 mean   hdci     \n## 2 b_sex1      48.6   47.7   49.4   0.95 mean   hdci\n\nsw_post_means %>% \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye() +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior mean weight (kg)\\n(Coefficient for sex)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior mean contrast in weights:\n\nsw_post_means_wide <- sex_weight %>% \n  spread_draws(b_sex0, b_sex1) %>% \n  mutate(diff = b_sex1 - b_sex0)\n\nsw_post_means_wide %>% \n  select(diff) %>% \n  mean_hdci()\n## # A tibble: 1 × 6\n##    diff .lower .upper .width .point .interval\n##   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1  6.77   5.63   7.95   0.95 mean   hdci\n\nsw_post_means_wide %>% \n  ggplot(aes(x = diff)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nPosterior predicted weights:\n\nsw_post_pred <- tibble(sex = c(\"0\", \"1\")) %>% \n  add_predicted_draws(sex_weight, ndraws = 1000)\n\nsw_post_pred %>% \n  mean_hdci()\n## # A tibble: 2 × 8\n##   sex    .row .prediction .lower .upper .width .point .interval\n##   <chr> <int>       <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 0         1        41.6   30.7   50.9   0.95 mean   hdci     \n## 2 1         2        49.0   38.2   59.3   0.95 mean   hdci\n\nsw_post_pred %>% \n  ungroup() %>% \n  mutate(sex_nice = factor(sex, labels = c(\"Female\", \"Male\"))) %>% \n  ggplot(aes(x = .prediction, fill = sex_nice)) +\n  stat_halfeye(alpha = 0.75) +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior predicted weight (kg)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior predicted contrast in weights:\n\nsw_post_pred_diff <- tibble(sex = c(\"0\", \"1\")) %>% \n  add_predicted_draws(sex_weight, ndraws = 1000) %>% \n  compare_levels(variable = .prediction, by = sex)\n\nsw_post_pred_diff %>% \n  mean_hdci()\n## # A tibble: 1 × 7\n##   sex   .prediction .lower .upper .width .point .interval\n##   <chr>       <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 1 - 0        7.21  -7.29   21.9   0.95 mean   hdci\n\nsw_post_pred_diff %>% \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\nsw_stan.stan\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] weight;\n  int sex[n];\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  vector[2] a;\n}\n\ntransformed parameters {\n  vector[n] mu;\n  mu = a[sex];\n}\n\nmodel {\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  sigma ~ uniform(0, 10);\n  a ~ normal(60, 10);\n}\n\ngenerated quantities {\n  real diff;\n  matrix[n, 2] weight_rep;\n  vector[n] diff_rep;\n  \n  // Calculate the contrasts / difference between group means\n  diff = a[2] - a[1];\n  \n  // Generate a posterior predictive distribution for each sex\n  // To do this we have to create a matrix, with a column per sex\n  for (j in 1:2) {\n    for (i in 1:n) {\n      weight_rep[i, j] = normal_rng(a[j], sigma);\n    }\n  }\n  \n  // Generate a posterior predictive distribution of group contrasts\n  for (i in 1:n) {\n    diff_rep[i] = normal_rng(a[2], sigma) - normal_rng(a[1], sigma);\n  }\n}\n\n\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  sex = d$male + 1)\n\nmodel_sw_stan <- rstan::sampling(\n  object = sw_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_sw_stan,\n      pars = c(\"sigma\", \"a[1]\", \"a[2]\", \"diff\"))\n## Inference for Stan model: ce4f92ccf1e48f603c1b01a6bf1ee94d.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  5.52       0 0.21  5.12  5.37  5.51  5.66  5.96 14336    1\n## a[1]  41.86       0 0.41 41.05 41.58 41.86 42.14 42.67 14671    1\n## a[2]  48.63       0 0.43 47.79 48.34 48.64 48.92 49.49 15314    1\n## diff   6.77       0 0.60  5.60  6.37  6.77  7.17  7.94 14697    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep 15 01:54:27 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\nPosterior mean weights:\n\nsw_stan_post_means <- model_sw_stan %>% \n  gather_draws(a[i])\n## Warning: `gather_()` was deprecated in tidyr 1.2.0.\n## Please use `gather()` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\nsw_stan_post_means %>% \n  mean_hdci()\n## # A tibble: 2 × 8\n##       i .variable .value .lower .upper .width .point .interval\n##   <int> <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1     1 a           41.9   41.1   42.7   0.95 mean   hdci     \n## 2     2 a           48.6   47.7   49.4   0.95 mean   hdci\n\nsw_stan_post_means %>% \n  ungroup() %>% \n  mutate(nice_i = factor(i, labels = c(\"a_female\", \"a_male\"))) %>% \n  ggplot(aes(x = .value, fill = nice_i)) +\n  stat_halfeye() +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior mean weight (kg)\\n(Coefficient for sex)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior mean contrast in weights:\n\nsw_stan_post_diff_means <- model_sw_stan %>% \n  gather_draws(diff)\n\nsw_stan_post_diff_means %>% \n  mean_hdci()\n## # A tibble: 1 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 diff        6.77   5.58   7.92   0.95 mean   hdci\n\nsw_stan_post_diff_means %>% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nPosterior predicted weights:\n\npredicted_weights_stan <- model_sw_stan %>% \n  spread_draws(weight_rep[i, sex])\n\npredicted_weights_stan %>% \n  group_by(sex) %>% \n  mean_hdci()\n## # A tibble: 2 × 10\n##     sex     i i.lower i.upper weight_rep weight_…¹ weigh…² .width .point .inte…³\n##   <int> <dbl>   <int>   <int>      <dbl>     <dbl>   <dbl>  <dbl> <chr>  <chr>  \n## 1     1  174.       1     329       41.9      30.9    52.7   0.95 mean   hdci   \n## 2     2  174.       1     329       48.6      37.8    59.5   0.95 mean   hdci   \n## # … with abbreviated variable names ¹​weight_rep.lower, ²​weight_rep.upper,\n## #   ³​.interval\n\npredicted_weights_stan %>% \n  ungroup() %>% \n  mutate(sex_nice = factor(sex, labels = c(\"Female\", \"Male\"))) %>% \n  ggplot(aes(x = weight_rep, fill = sex_nice)) +\n  stat_halfeye(alpha = 0.75) +\n  scale_fill_manual(values = clrs[1:2]) +\n  labs(x = \"Posterior predicted weight (kg)\", y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nPosterior predicted contrast in weights:\n\nsw_post_pred_diff_stan <- model_sw_stan %>% \n  gather_draws(diff_rep[i])\n\nsw_post_pred_diff_stan %>%\n  group_by(.variable) %>%\n  mean_hdci() %>% \n  select(-starts_with(\"i\"))\n## # A tibble: 1 × 7\n##   .variable .value .value.lower .value.upper .width .point .interval\n##   <chr>      <dbl>        <dbl>        <dbl>  <dbl> <chr>  <chr>    \n## 1 diff_rep    6.77        -8.63         22.1   0.95 mean   hdci\n\nsw_post_pred_diff_stan %>% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\n\n\n\nSex + height\n\\[\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{S[i]} + \\beta_{S[i]}(H_i - \\bar{H}) \\\\\n\\\\\n\\alpha_j &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta_j &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\n\nbrmsStan\n\n\nThis is the wonkiest syntax ever, but it works! We can hack the nl capabilities of brms to create indexed parameters.\n\npriors <- c(prior(normal(60, 10), class = b, nlpar = a),\n            prior(lognormal(0, 1), class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex <- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_height_sex\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 0 + a + b * height_z \n##          a ~ 0 + sex\n##          b ~ 0 + sex\n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## a_sex0    45.10      0.46    44.19    45.99 1.00     3021     3038\n## a_sex1    45.19      0.47    44.30    46.11 1.00     2828     2799\n## b_sex0     4.90      0.50     3.88     5.89 1.00     3018     2442\n## b_sex1     4.65      0.45     3.79     5.53 1.00     2964     2861\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.28      0.17     3.97     4.62 1.00     3600     2789\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nsex_height_weight_post_pred <- expand_grid(\n  height_z = seq(min(d$height_z), max(d$height_z), length.out = 50),\n  sex = 0:1\n) %>% \n  add_predicted_draws(model_height_sex) %>%\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"0\", \"1\"))) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nOverall distribution of predictive posterior contrasts:\n\nggplot(sex_height_weight_post_pred, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nDistribution of predictive posterior contrasts across range of heights:\n\nggplot(sex_height_weight_post_pred, aes(x = height_unscaled, y = .prediction)) +\n  stat_lineribbon(aes(fill_ramp = stat(.width)), .width = ppoints(50),\n                  fill = clrs[3], color = colorspace::darken(clrs[3], 0.5), \n                  show.legend = FALSE) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_ramp_continuous(from = \"transparent\", range = c(1, 0)) +\n  labs(x = \"Height (cm)\", y = \"Posterior weight contrast (kg)\\nWomen − Men\")\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Stan code from Rethinking models\n\n\n\n\n\nThe ulam() function is super helpful for translating McElreath’s quap() syntax into Stan!\n\nm_SHW <- rethinking::quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu <- a[S] + b[S] * (H - Hbar),\n    a[S] ~ dnorm(60, 10),\n    b[S] ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 10)\n  ), \n  data = list(\n    W = d$weight,\n    H = d$height,\n    Hbar = mean(d$height),\n    S = d$male + 1\n  )\n)\n\ncat(rethinking::ulam(m_SHW, sample = FALSE)$model)\n## data{\n##     vector[346] W;\n##     real Hbar;\n##     vector[346] H;\n##     int S[346];\n## }\n## parameters{\n##     vector[2] a;\n##     vector<lower=0>[2] b;\n##     real<lower=0,upper=10> sigma;\n## }\n## model{\n##     vector[346] mu;\n##     sigma ~ uniform( 0 , 10 );\n##     b ~ lognormal( 0 , 1 );\n##     a ~ normal( 60 , 10 );\n##     for ( i in 1:346 ) {\n##         mu[i] = a[S[i]] + b[S[i]] * (H[i] - Hbar);\n##     }\n##     W ~ normal( mu , sigma );\n## }\n\n\n\n\nsex_height.stan\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] weight;\n  vector[n] height;\n  int sex[n];\n}\n\ntransformed data {\n  // Center and standardize height\n  vector[n] height_z;\n  height_z = (height - mean(height)) / sd(height);\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  vector[2] a;\n  vector<lower=0>[2] b;\n}\n\nmodel {\n  vector[n] mu;\n  \n  // Model for mu with intercepts (a) and coefficients (b) for each sex\n  for (i in 1:n) {\n    mu[i] = a[sex[i]] + b[sex[i]] * height_z[i];\n  }\n\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  sigma ~ uniform(0, 10);\n  a ~ normal(60, 10);\n  b ~ lognormal(0, 1);\n}\n\ngenerated quantities {\n  matrix[n, 2] weight_rep;\n  vector[n] diff_rep;\n  \n  // Generate a posterior predictive distribution for each sex\n  // To do this we have to create a matrix, with a column per sex\n  for (j in 1:2) {\n    for (i in 1:n) {\n      real mu_hat_n = a[sex[i]] + b[sex[i]] * height_z[i];\n      weight_rep[i, j] = normal_rng(mu_hat_n, sigma);\n    }\n  }\n  \n  // Generate a posterior predictive distribution of group contrasts\n  for (i in 1:n) {\n    diff_rep[i] = weight_rep[i, 1] - weight_rep[i, 2];\n  }\n}\n\n\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  sex = d$male + 1)\n\nmodel_sex_height_stan <- rstan::sampling(\n  object = sex_height_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_sex_height_stan,\n      pars = c(\"sigma\", \"a[1]\", \"a[2]\", \"b[1]\", \"b[2]\"))\n## Inference for Stan model: f8b7828023f48992dd5011f1f1fa456e.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  4.28       0 0.17  3.97  4.17  4.28  4.39  4.62 15332    1\n## a[1]  45.10       0 0.45 44.22 44.80 45.10 45.41 45.99 10776    1\n## a[2]  45.20       0 0.46 44.29 44.89 45.20 45.51 46.09 10499    1\n## b[1]   4.91       0 0.49  3.95  4.57  4.91  5.24  5.86 10718    1\n## b[2]   4.65       0 0.43  3.80  4.36  4.65  4.95  5.50 10394    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep 15 01:55:38 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\n\noriginal_hw <- tibble(height = d$height,\n                      weight = d$weight) %>% \n  mutate(i = 1:n())\n\npredicted_diffs_sex_height_stan <- model_sex_height_stan %>% \n  spread_draws(diff_rep[i]) %>% \n  left_join(original_hw, by = \"i\")\n\nOverall distribution of predictive posterior contrasts:\n\nggplot(predicted_diffs_sex_height_stan, aes(x = diff_rep)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nDistribution of predictive posterior contrasts across range of heights:\n(The y-values are way off from the video here :shrug:)\n\nggplot(predicted_diffs_sex_height_stan, aes(x = height, y = diff_rep)) +\n  stat_lineribbon(aes(fill_ramp = stat(.width)), .width = ppoints(50),\n                  fill = clrs[3], color = colorspace::darken(clrs[3], 0.5), \n                  show.legend = FALSE) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_ramp_continuous(from = \"transparent\", range = c(1, 0)) +\n  labs(x = \"Height (cm)\", y = \"Posterior weight contrast (kg)\\nWomen − Men\")\n\n\n\n\n\n\n\n\n\nFull luxury Bayes!\nGiven this DAG:\n\nheight_sex_dag <- dagify(\n  x ~ z,\n  y ~ x + z,\n  exposure = \"x\",\n  outcome = \"y\",\n  labels = c(x = \"Height\", y = \"Weight\", z = \"Sex\"),\n  coords = list(x = c(x = 1, y = 3, z = 2),\n                y = c(x = 1, y = 1, z = 2))) %>% \n  tidy_dagitty() %>% \n  node_status()\n\nggplot(height_sex_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(aes(label = label), size = 3.5) +\n  scale_color_manual(values = clrs[c(1, 4)], guide = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n…what’s the causal effect of sex on weight? Or:\n\\[\nE(\\text{Weight} \\mid \\operatorname{do}(\\text{Sex}))\n\\]\nHere’s the official model:\n\\[\n\\begin{aligned}\nH_i &\\sim \\mathcal{N}(\\nu_i, \\tau) \\\\\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\nu_i &= h_{S[i]} \\\\\n\\mu_i &= \\alpha_{S[i]} + \\beta_{S[i]}(H_i - \\bar{H}) \\\\\n\\\\\nh_j &\\sim \\mathcal{N}(160, 10) \\\\\n\\alpha_j &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta_j &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma, \\tau &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n\\]\nThe results should look something like this, from the slides:\n\n\n\n\n\n\n\n\n\n\nbrmsStan\n\n\n\npriors <- c(prior(normal(60, 10), resp = weight, class = b, nlpar = a),\n            prior(lognormal(0, 1), resp = weight, class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), resp = weight, class = sigma, lb = 0, ub = 10),\n            # prior(normal(160, 10), resp = height, class = b),\n            prior(normal(0, 1), resp = heightz, class = b),\n            prior(uniform(0, 10), resp = heightz, class = sigma, lb = 0, ub = 10))\n\nmodel_luxury <- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE) + \n    bf(height_z ~ 0 + sex) + \n    set_rescor(TRUE),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n\n\nmodel_luxury\n##  Family: MV(gaussian, gaussian) \n##   Links: mu = identity; sigma = identity\n##          mu = identity; sigma = identity \n## Formula: weight ~ 0 + a + b * height_z \n##          a ~ 0 + sex\n##          b ~ 0 + sex\n##          height_z ~ 0 + sex \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## weight_a_sex0    42.58      0.64    41.52    44.07 1.01      598      285\n## weight_a_sex1    47.97      0.64    46.49    49.02 1.01      552      313\n## weight_b_sex0     1.09      0.77     0.17     3.02 1.01      565      255\n## weight_b_sex1     0.89      0.70     0.12     2.73 1.01      491      275\n## heightz_sex0     -0.66      0.05    -0.77    -0.55 1.00     2384     1681\n## heightz_sex1      0.74      0.06     0.62     0.85 1.00     2806     2528\n## \n## Family Specific Parameters: \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_weight      5.13      0.29     4.49     5.67 1.01      666      291\n## sigma_heightz     0.72      0.03     0.67     0.77 1.00     3262     1965\n## \n## Residual Correlations: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## rescor(weight,heightz)     0.54      0.09     0.32     0.65 1.01      560\n##                        Tail_ESS\n## rescor(weight,heightz)      265\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPosterior mean contrast in weights:\n\nluxury_post_mean_diff <- expand_grid(\n  height_z = seq(min(d$height_z), max(d$height_z), length.out = 50),\n  sex = 0:1\n) %>% \n  add_epred_draws(model_luxury) %>%\n  compare_levels(variable = .epred, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_mean_diff %>% \n  filter(.category == \"weight\") %>% \n  ggplot(aes(x = .epred)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\nPosterior predicted contrast in weights:\n\nluxury_post_pred_diff <- expand_grid(\n  height_z = seq(min(d$height_z), max(d$height_z), length.out = 50),\n  sex = 0:1\n) %>% \n  add_predicted_draws(model_luxury) %>%\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_pred_diff %>% \n  filter(.category == \"weight\") %>% \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Stan code from Rethinking models\n\n\n\n\n\n\nm_SHW_full <- rethinking::quap(\n  alist(\n    # Weight\n    W ~ dnorm(mu, sigma),\n    mu <- a[S] + b[S] * (H - Hbar),\n    a[S] ~ dnorm(60, 10),\n    b[S] ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 10),\n    \n    # Height\n    H ~ dnorm(nu, tau),\n    nu <- h[S],\n    h[S] ~ dnorm(160, 10),\n    tau ~ dunif(0, 10)\n  ), data = list(\n    W = d$weight,\n    H = d$height,\n    Hbar = mean(d$height),\n    S = d$male + 1\n  )\n)\n\ncat(rethinking::ulam(m_SHW_full, sample = FALSE)$model)\n\n\n\n\nluxury_stan.stan\n\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] weight;\n  real Hbar;\n  vector[n] height;\n  int sex[n];\n}\n\nparameters {\n  // Things to estimate\n  vector[2] a;\n  vector<lower=0>[2] b;\n  real<lower=0,upper=10> sigma;\n  vector[2] h;\n  real<lower=0,upper=10> tau;\n}\n\nmodel {\n  vector[n] mu;\n  vector[n] nu;\n  \n  // Height model\n  tau ~ uniform(0, 10);\n  h ~ normal(160, 10);\n  \n  for (i in 1:n) {\n    nu[i] = h[sex[i]];\n  }\n  \n  // Weight model\n  height ~ normal(nu , tau);\n  sigma ~ uniform(0, 10);\n  b ~ lognormal(0, 1);\n  a ~ normal(60, 10);\n  \n  for (i in 1:n) {\n    mu[i] = a[sex[i]] + b[sex[i]] * (height[i] - Hbar);\n  }\n  \n  weight ~ normal(mu, sigma);\n}\n\ngenerated quantities {\n  matrix[n, 2] weight_rep;\n  matrix[n, 2] height_rep;\n  vector[n] w_do_s;\n  vector[2] mu_sex;\n  real mu_diff;\n  \n  for (i in 1:2) {\n    mu_sex[i] = a[sex[i]] + b[sex[i]] * (h[sex[i]] - Hbar);\n  }\n  \n  mu_diff = mu_sex[1] - mu_sex[2];\n  \n  // Generate a posterior predictive distribution for each sex\n  // To do this we have to create a matrix, with a column per sex\n  for (j in 1:2) {\n    for (i in 1:n) {\n      height_rep[i, j] = normal_rng(h[sex[j]], tau);\n      weight_rep[i, j] = normal_rng(a[sex[j]] + b[sex[j]] * (height_rep[i, j] - Hbar), sigma);\n    }\n  }\n  \n  // Generate a posterior predictive distribution of group contrasts\n  for (i in 1:n) {\n    w_do_s[i] = weight_rep[i, 1] - weight_rep[i, 2];\n  }\n}\n\n\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  Hbar = mean(d$height),\n                  sex = d$male + 1)\n\nmodel_luxury_stan <- rstan::sampling(\n  object = luxury_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n\n\nprint(model_luxury_stan,\n      pars = c(\"a[1]\", \"a[2]\", \"b[1]\", \"b[2]\", \"sigma\", \"h[1]\", \"h[2]\", \"tau\", \n               \"mu_sex[1]\", \"mu_sex[2]\", \"mu_diff\"))\n## Inference for Stan model: 2ac72225665508fb4100c7f800818c3a.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##             mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n## a[1]       45.18       0 0.46  44.28  44.87  45.17  45.48  46.08 12392    1\n## a[2]       45.14       0 0.46  44.25  44.83  45.14  45.45  46.03 12553    1\n## b[1]        0.65       0 0.06   0.52   0.60   0.65   0.69   0.77 12075    1\n## b[2]        0.61       0 0.06   0.50   0.57   0.61   0.65   0.72 13048    1\n## sigma       4.28       0 0.16   3.97   4.17   4.28   4.39   4.62 21039    1\n## h[1]      149.50       0 0.41 148.70 149.21 149.50 149.78 150.30 22463    1\n## h[2]      160.37       0 0.44 159.52 160.08 160.37 160.67 161.22 22320    1\n## tau         5.57       0 0.21   5.18   5.43   5.56   5.71   6.01 19301    1\n## mu_sex[1]  48.63       0 0.42  47.79  48.35  48.63  48.92  49.46 21139    1\n## mu_sex[2]  41.85       0 0.42  41.04  41.57  41.85  42.13  42.67 21824    1\n## mu_diff     6.78       0 0.59   5.62   6.37   6.78   7.18   7.94 20965    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep 15 01:56:43 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\nPosterior mean contrast in weights:\n\nluxury_post_mean_diff_stan <- model_luxury_stan %>% \n  spread_draws(mu_diff)\n\nluxury_post_mean_diff_stan %>% \n  mean_hdci()\n## # A tibble: 1 × 6\n##   mu_diff .lower .upper .width .point .interval\n##     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1    6.78   5.64   7.95   0.95 mean   hdci\n\nggplot(luxury_post_mean_diff_stan, aes(x = mu_diff)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\")\n\n\n\n\n\nluxury_post_pred_diff_stan <- model_luxury_stan %>% \n  spread_draws(w_do_s[i])\n\nluxury_post_pred_diff_stan %>% \n  mean_hdci()\n## # A tibble: 346 × 7\n##        i w_do_s .lower .upper .width .point .interval\n##    <int>  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n##  1     1   6.79  -8.96   21.5   0.95 mean   hdci     \n##  2     2   6.86  -8.69   22.2   0.95 mean   hdci     \n##  3     3   6.79  -8.46   22.6   0.95 mean   hdci     \n##  4     4   6.86  -9.38   21.3   0.95 mean   hdci     \n##  5     5   6.71  -8.26   22.7   0.95 mean   hdci     \n##  6     6   6.85  -8.15   22.8   0.95 mean   hdci     \n##  7     7   6.78  -8.74   22.4   0.95 mean   hdci     \n##  8     8   6.85  -8.36   22.6   0.95 mean   hdci     \n##  9     9   6.82  -8.79   21.9   0.95 mean   hdci     \n## 10    10   6.80  -8.53   21.8   0.95 mean   hdci     \n## # … with 336 more rows\n\nggplot(luxury_post_pred_diff_stan, aes(x = w_do_s)) +\n  stat_halfeye(aes(fill = stat(x > 0))) +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(colorspace::lighten(clrs[3], 0.5), clrs[3]),\n                    guide = \"none\") +\n  labs(x = \"Posterior weight contrast (kg)\\nWomen − Men\", y = \"Density\")"
  }
]