{
  "hash": "43680d49619be0050bdeca9f7ef94f3a",
  "result": {
    "markdown": "---\ntitle: \"Video code\"\nsubtitle: \"Geocentric models / linear regression\"\ndate: \"September 7, 2022\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/zYYBtxHWE0A\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(patchwork)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nBAYES_SEED <- 1234\nset.seed(1234)\n\n# Data\ndata(Howell1, package = \"rethinking\")\n\nd <- Howell1 %>% \n  filter(age > 18) %>% \n  mutate(height_z = scale(height))\n\nheight_scale <- attributes(d$height_z) %>% \n  set_names(janitor::make_clean_names(names(.)))\n```\n:::\n\n\n# Linear regression\n\nThe general process for drawing the linear regression owl:\n\n1. Question/goal/estimand\n2. Scientific model\n3. Statistical model(s)\n4. Validate model\n5. Analyze data\n\n\n## 1. Question/goal/estimand\n\nWe want to describe the association between adult weight and height\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = height, y = weight)) +\n  geom_point(color = clrs[3])\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n## 2. Scientific model\n\nHow does height influence weight? \n\nHeight has a causal relationship with weight:\n\n$$\nH \\rightarrow W\n$$\n\nWeight is a function of height. Plug in some value of height, get some weight:\n\n$$\nW = f(H)\n$$\n\nWe need to write down that model somehow. The normal $y = mx + b$ way of writing a linear model looks like this, where $\\alpha$ is the intercept and $\\beta$ is the slope:\n\n$$\ny_i = \\alpha + \\beta x_i\n$$\n\nWe can also write it like this, where $\\mu$ is the expectation ($E(y \\mid x) = \\mu$), and $\\sigma$ is the standard deviation:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\n\\end{aligned}\n$$\n\nSo, we can think of a generative model for height causing weight like this:\n\n$$\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta H_i\n\\end{aligned}\n$$\n\nThis can map directly to code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nalpha <- 0\nbeta <- 0.5\nsigma <- 5\nn_individuals <- 100\n\nfake_people <- tibble(height = runif(n_individuals, 130, 170),\n                      mu = alpha + beta*height,\n                      weight = rnorm(n_individuals, mu, sigma))\n\nlm(weight ~ height, data = fake_people) %>% \n  tidy()\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    1.22     6.37       0.191 8.49e- 1\n## 2 height         0.494    0.0431    11.5   7.64e-20\n\nggplot(fake_people, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## 3: Statistical model\n\n### Simulating and checking the priors\n\nWe don't actually know $\\alpha$, $\\beta$, and $\\sigma$ - they all have priors and limits and bounds. For instance, $\\sigma$ is a scale parameter - it shifts distributions up and down - it has to be positive (hence the uniform distribution here). We can write a generic model like this:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 1)\n\\end{aligned}\n$$\n\nBut if we sample from that prior distribution, we get lines that are all over the place!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nn_samples <- 10\n\ntibble(alpha = rnorm(n_samples, 0, 1),\n       beta = rnorm(n_samples, 0, 1)) %>% \n  ggplot() +\n  geom_abline(aes(slope = beta, intercept = alpha)) +\n  xlim(c(-2, 2)) + ylim(c(-2, 2))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nInstead, we can set some more specific priors and rescale variables so that the intercept makes more sense.\n\n$$\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (H_i - \\bar{H}) \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n$$\n\nHere's what those priors look like:\n\n::: {.panel-tabset}\n\n### Plain old ggplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_prior_alpha <- ggplot() +\n  stat_function(fun = ~dnorm(., 60, 10), geom = \"area\", fill = clrs[1]) +\n  xlim(c(30, 90)) +\n  labs(title = \"Normal(60, 10)\", subtitle = \"Prior for intercept (α)\", caption = \"Average adult weight\")\n\nplot_prior_beta <- ggplot() +\n  stat_function(fun = ~dnorm(., 0, 10), geom = \"area\", fill = clrs[2]) +\n  xlim(c(-30, 30)) +\n  labs(title = \"Normal(0, 10)\", subtitle = \"Prior for slope (β)\", caption = \"kg per cm\")\n\nplot_prior_sigma <- ggplot() +\n  stat_function(fun = ~dunif(., 0, 10), geom = \"area\", fill = clrs[3]) +\n  xlim(c(0, 10)) +\n  labs(title = \"Uniform(0, 10)\", subtitle = \"Prior for sd (σ)\")\n\nplot_prior_alpha | plot_prior_beta | plot_prior_sigma\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\n### `brms::prior()` and `ggdist::parse_dist()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\npriors %>% \n  parse_dist() %>% \n  ggplot(aes(y = 0, dist = .dist, args = .args, fill = prior)) +\n  stat_slab() +\n  scale_fill_manual(values = clrs[1:3]) +\n  facet_wrap(vars(prior), scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::\n\nLet's see how well those priors work!\n\n::: {.panel-tabset}\n\n### Manual plotting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nn <- 100\nHbar <- 150\nHseq <- seq(130, 170, length.out = 30)\n\ntibble(alpha = rnorm(n, 60, 10),\n       beta = rnorm(n, 0, 10)) %>% \n  mutate(weight = map2(alpha, beta, ~.x + .y*(Hseq - Hbar)),\n         height = list(Hseq),\n         id = 1:n) %>% \n  unnest(c(weight, height)) %>% \n  ggplot(aes(x = height, y = weight)) + \n  geom_line(aes(group = id), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### brms and `sample_prior = \"only\"`\n\n\n::: {.cell hash='03-video_cache/html/model-prior-only-normal_a2eb17591c6cc91b843ca707f24c93ed'}\n\n```{.r .cell-code}\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_prior_only_normal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\"\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 100)) %>% \n  add_epred_draws(height_weight_prior_only_normal, ndraws = 100) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\ndraws_prior %>% \n  ggplot(aes(x = height_unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::\n\nLots of those lines are completely implausible, so we'll use a lognormal β prior instead:\n\n$$\n\\begin{aligned}\nW_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (H_i - \\bar{H}) \\\\\n\\\\\n\\alpha &\\sim \\mathcal{N}(60, 10) \\\\\n\\beta &\\sim \\operatorname{LogNormal}(0, 1) \\\\\n\\sigma &\\sim \\operatorname{Uniform}(0, 10)\n\\end{aligned}\n$$\n\nThe lognormal distribution forces βs to be > 0 and it's clusttered down at low values like 1ish:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ~dlnorm(., 0, 1), geom = \"area\", fill = clrs[1]) +\n  xlim(c(0, 5)) +\n  labs(x = \"Simulated β values\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n::: {.panel-tabset}\n\n### Manual plotting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nn <- 100\nHbar <- 150\nHseq <- seq(130, 170, length.out = 30)\n\ntibble(alpha = rnorm(n, 60, 10),\n       beta = rlnorm(n, 0, 1)) %>% \n  mutate(weight = map2(alpha, beta, ~.x + .y*(Hseq - Hbar)),\n         height = list(Hseq),\n         id = 1:n) %>% \n  unnest(c(weight, height)) %>% \n  ggplot(aes(x = height, y = weight)) + \n  geom_line(aes(group = id), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### brms and `sample_prior = \"only\"`\n\n\n::: {.cell hash='03-video_cache/html/model-prior-only-lognormal_846cfbd6bc4306d96476ce5028974390'}\n\n```{.r .cell-code}\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_prior_only_lognormal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Start sampling\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 100)) %>% \n  add_epred_draws(height_weight_prior_only_lognormal, ndraws = 100) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\ndraws_prior %>% \n  ggplot(aes(x = height_unscaled, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.2) +\n  coord_cartesian(xlim = c(130, 170), ylim = c(10, 100))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n\n### Fitting the model\n\nFitting the model is super complex because it's the joint posterior of all those distributions:\n\n$$\n\\begin{aligned}\n\\operatorname{Pr}(\\alpha, \\beta, \\sigma \\mid W, H) \\propto~ & \\mathcal{N}(W \\mid \\mu, \\sigma) \\\\\n&\\times \\mathcal{N}(\\alpha \\mid 60, 10) \\\\\n&\\times \\operatorname{LogNormal}(\\beta \\mid 0, 1) \\\\\n&\\times \\operatorname{Uniform}(\\sigma \\mid 0, 10)\n\\end{aligned}\n$$\n\nIf we do this with grid approximation, 100 values of each parameter = 1 million calculations. In the book he shows it with grid approximation and with quadratic approximation.\n\nI'll do it with brms and Stan instead.\n\n::: {.panel-tabset}\n\n### brms\n\n\n::: {.cell hash='03-video_cache/html/model-lognormal_e2d1fd9d73e59c8a1b5ed7d5ca02a811'}\n\n```{.r .cell-code}\npriors <- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nheight_weight_lognormal <- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\n## Compiling Stan program...\n## recompiling to avoid crashing R session\n## Trying to compile a simple C file\n## Start sampling\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(height_weight_lognormal)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: weight ~ 1 + height_z \n##    Data: d (Number of observations: 346) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    45.05      0.23    44.61    45.50 1.00     3979     3023\n## height_z      4.83      0.23     4.38     5.29 1.00     3802     2716\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     4.27      0.17     3.96     4.62 1.00     4119     2987\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\n\n### Stan\n\nThis is a little trickier because `add_epred_draws()` and `predicted_draws()` and all those nice functions [don't work with raw Stan samples](https://www.andrewheiss.com/blog/2021/12/20/fully-bayesian-ate-iptw/#using-the-results-with-brms), [by design](https://mc-stan.org/docs/2_26/stan-users-guide/simulating-from-the-posterior-predictive-distribution.html).\n\nInstead, we need to generate predictions that incorporate sigma using a `generated quantities` block in Stan (see [the official Stan documentation](https://mc-stan.org/docs/2_26/stan-users-guide/simulating-from-the-posterior-predictive-distribution.html) and [this official example](https://mc-stan.org/docs/stan-users-guide/prediction-forecasting-and-backcasting.html) and [this neat blog post by Monica Alexander](https://www.monicaalexander.com/posts/2020-28-02-bayes_viz/#running-the-models-in-stan) and [this Medium post](https://medium.com/@alex.pavlakis/making-predictions-from-stan-models-in-r-3e349dfac1ed)).\n\nHere we return two different but similar things: `weight_rep[i]` for the posterior predictive (which corresponds to `predicted_draws()` in **tidybayes**) and `mu[i]` for the expectation of the posterior predictive (which corresponds to `epred_draws()` in **tidybayes**):\n\n\n::: {.cell output.var='lognormal_stan' hash='03-video_cache/html/model-lognormal-stan_2271075dceb2b5c0da5c609292d912f5'}\n\n```{.stan .cell-code}\ndata {\n  // Stuff from R\n  int<lower=1> n;\n  vector[n] height;  // Explanatory variable\n  vector[n] weight;  // Outcome variable\n  real height_bar;   // Average height\n}\n\nparameters {\n  // Things to estimate\n  real<lower=0, upper=10> sigma;\n  real beta;\n  real alpha;\n}\n\ntransformed parameters {\n  // Regression-y model of mu with scaled height\n  vector[n] mu;\n  mu = alpha + beta * (height - height_bar);\n}\n\nmodel {\n  // Likelihood\n  weight ~ normal(mu, sigma);\n  \n  // Priors\n  alpha ~ normal(60, 10);\n  beta ~ lognormal(0, 1);\n  sigma ~ uniform(0, 10);\n}\n\ngenerated quantities {\n  // Generate a posterior predictive distribution\n  vector[n] weight_rep;\n\n  for (i in 1:n) {\n    // Calculate a new predicted mu for each iteration here\n    real mu_hat_n = alpha + beta * (height[i] - height_bar);\n    weight_rep[i] = normal_rng(mu_hat_n, sigma);\n    \n    // Alternatively, we can use mu[i] from the transformed parameters\n    // weight_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n```\n:::\n\n::: {.cell hash='03-video_cache/html/sample-lognormal-stan_37466f530d351056efea980f1d6088a2'}\n\n```{.r .cell-code}\n# compose_data listifies things for Stan\n# stan_data <- d %>% compose_data()\n# stan_data$height_bar <- mean(stan_data$height)\n\n# Or we can manually build the list\nstan_data <- list(n = nrow(d),\n                  weight = d$weight,\n                  height = d$height,\n                  height_bar = mean(d$height))\n\nmodel_lognormal_stan <- rstan::sampling(\n  object = lognormal_stan,\n  data = stan_data,\n  iter = 5000, warmup = 1000, seed = BAYES_SEED, chains = 4, cores = 4\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(model_lognormal_stan, \n      pars = c(\"sigma\", \"beta\", \"alpha\", \"mu[1]\", \"mu[2]\"))\n## Inference for Stan model: 3c0d0064926253eea8547fd513e8019e.\n## 4 chains, each with iter=5000; warmup=1000; thin=1; \n## post-warmup draws per chain=4000, total post-warmup draws=16000.\n## \n##        mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\n## sigma  4.27       0 0.16  3.97  4.16  4.27  4.38  4.60 13529    1\n## beta   0.62       0 0.03  0.57  0.60  0.62  0.64  0.68 15170    1\n## alpha 45.06       0 0.23 44.61 44.90 45.05 45.21 45.51 15781    1\n## mu[1] 43.26       0 0.24 42.78 43.09 43.26 43.42 43.74 15594    1\n## mu[2] 35.72       0 0.50 34.73 35.39 35.72 36.06 36.69 15326    1\n## \n## Samples were drawn using NUTS(diag_e) at Thu Sep  8 13:44:25 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n```\n:::\n\n\n:::\n\n## 4 & 5: Validate model and analyze data\n\n::: {.panel-tabset}\n\n### brms\n\nExpectation of the posterior (plotting uncertainty of the mean):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_posterior_epred <- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 100)) %>% \n  add_epred_draws(height_weight_lognormal, ndraws = 50) %>% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  geom_line(data = draws_posterior_epred,\n            aes(x = height_unscaled, y = .epred, group = .draw), alpha = 0.2, color = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nPosterior predictions (plotting uncertainty of the predictions):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_posterior_pred <- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 500)) %>% \n  add_predicted_draws(height_weight_lognormal, ndraws = 100) %>%\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  stat_lineribbon(data = draws_posterior_pred,\n                  aes(x = height_unscaled, y = .prediction), .width = 0.95, \n                  alpha = 0.2, color = clrs[5], fill = clrs[5]) +\n  coord_cartesian(ylim = c(30, 65))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nAnd here's the posterior predictive check, just for fun:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(height_weight_lognormal, type = \"dens_overlay\", ndraws = 25)\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n### Stan\n\nSummarized predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_values_lognormal <- model_lognormal_stan %>% \n  spread_draws(mu[i], weight_rep[i]) %>%\n  mean_qi() %>%\n  mutate(weight = d$weight,\n         height = d$height)\n```\n:::\n\n\nExpectation of the posterior (plotting uncertainty of the mean):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(predicted_values_lognormal, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_line(aes(y = mu), color = clrs[6]) +\n  geom_ribbon(aes(ymin = mu.lower, ymax = mu.upper), alpha = 0.2, fill = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nPosterior predictions (plotting uncertainty of the predictions):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(predicted_values_lognormal, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_line(aes(y = mu), color = clrs[5]) +\n  geom_ribbon(aes(ymin = weight_rep.lower, ymax = weight_rep.upper), alpha = 0.2, fill = clrs[5]) +\n  coord_cartesian(ylim = c(30, 65))\n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nAnd here's the posterior predictive check, just for fun:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyrep1 <- rstan::extract(model_lognormal_stan)[[\"weight_rep\"]]\nsamp25 <- sample(nrow(yrep1), 25)\nbayesplot::ppc_dens_overlay(d$weight, yrep1[samp25, ])  \n```\n\n::: {.cell-output-display}\n![](03-video_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}