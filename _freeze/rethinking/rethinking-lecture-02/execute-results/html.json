{
  "hash": "90d6e162c4be220d7fec73cdaaf77294",
  "result": {
    "markdown": "---\ntitle: \"*Rethinking* lecture 2\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(patchwork)\nlibrary(posterior)\nlibrary(broom.mixed)\n\nset.seed(1234)\n```\n:::\n\n\nAssuming 9 globe tosses, 6 are water:\n\n```\nW L W W W L W L W\n```\n\nOr in code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntosses <- c(\"W\", \"L\", \"W\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\ntable(tosses)\n## tosses\n## L W \n## 3 6\n```\n:::\n\n\n\nGiven this data, what's the proportion of water on the globe?\n\n- Make a list of all possible proportions, ranging from 0 to 1\n- Calculate the number of possible pathways to get to that proportion\n\n## Grid approximation\n\nFor each possible value of $p$, compute the product $\\operatorname{Pr}(W, L \\mid p) \\times \\operatorname{Pr}(p)$. The relative sizes of each of those products are the posterior probabilities.\n\n### Base R with *Rethinking*\n\n#### Uniform flat prior\n\n\n::: {.cell .column-page layout-nrow=\"2\" layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# List of possible explanations of p to consider\np_grid <- seq(from = 0, to = 1, length.out = 10)\nplot(p_grid, main = \"Possible proportions (p)\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n\n```{.r .cell-code}\n#\n\n# Probability of each value of p\n# Super vague uniform prior: just 1 at each possible p\nprob_p_uniform <- rep(1, 10)\nplot(prob_p_uniform, main = \"Uniform flat prior\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-2-2.png){width=576}\n:::\n\n```{.r .cell-code}\n#\n\n# Probability of each proportion, given 6/9 water draws\nprob_data <- dbinom(6, size = 9, prob = p_grid)\n\n# Unnormalized posterior\nposterior_raw <- prob_data * prob_p_uniform\nplot(posterior_raw, main = \"Unnormalized posterior\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-2-3.png){width=576}\n:::\n\n```{.r .cell-code}\n#\n\n# Normalized posterior that sums to 1\nposterior_normalized <- posterior_raw / sum(posterior_raw)\nplot(posterior_normalized, main = \"Normalized posterior\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-2-4.png){width=576}\n:::\n:::\n\n\n#### Beta prior\n\n\n::: {.cell .column-page layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Beta distribution with 3 / (3 + 1)\nprob_p_beta <- dbeta(p_grid, shape1 = 3, shape2 = 1)\nplot(prob_p_beta, main = \"Beta(3, 1) prior\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n\n```{.r .cell-code}\n#\n# Posterior that sums to 1\nposterior_normalized_beta <- (prob_data * prob_p_beta) / sum(posterior_raw)\nplot(posterior_normalized_beta, main = \"Normalized postiorior with beta prior\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-3-2.png){width=576}\n:::\n:::\n\n\n### Tidyverse style from Solomon Kurz\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobe_tossing <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1001),\n                        prior_uniform = 1) %>%  # prob_p_uniform from earlier\n  mutate(prior_beta = dbeta(p_grid, shape1 = 3, shape2 = 1)) %>%  # prob_p_beta from earlier\n  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%   # prob_data from earlier\n  mutate(posterior_uniform = (likelihood * prior_uniform) / sum(likelihood * prior_uniform),\n         posterior_beta = (likelihood * prior_beta) / sum(likelihood * prior_beta))\nglobe_tossing\n## # A tibble: 1,001 × 6\n##    p_grid prior_uniform prior_beta likelihood posterior_uniform posterior_beta\n##     <dbl>         <dbl>      <dbl>      <dbl>             <dbl>          <dbl>\n##  1  0                 1  0           0                 0              0       \n##  2  0.001             1  0.000003    8.37e-17          8.37e-19       1.97e-24\n##  3  0.002             1  0.000012    5.34e-15          5.34e-17       5.04e-22\n##  4  0.003             1  0.0000270   6.07e-14          6.07e-16       1.29e-20\n##  5  0.004             1  0.000048    3.40e-13          3.40e-15       1.28e-19\n##  6  0.005             1  0.000075    1.29e-12          1.29e-14       7.62e-19\n##  7  0.006             1  0.000108    3.85e-12          3.85e-14       3.27e-18\n##  8  0.007             1  0.000147    9.68e-12          9.68e-14       1.12e-17\n##  9  0.008             1  0.000192    2.15e-11          2.15e-13       3.24e-17\n## 10  0.009             1  0.000243    4.34e-11          4.34e-13       8.30e-17\n## # … with 991 more rows\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglobe_tossing %>% \n  pivot_longer(starts_with(\"posterior\")) %>% \n  ggplot(aes(x = p_grid, y = value, fill = name)) +\n  geom_area(position = position_identity(), alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-5-1.png){width=576}\n:::\n:::\n\n\n\n## Working with the posterior\n\nWe now have a posterior! We typically can't use the posterior alone. We have to average any inference across the entire posterior. This requires calculus, which is (1) hard, and (2) often impossible. So instead, we can use samples from the distribution and make inferences based on those.\n\n### 3.2: Sampling to summarize\n\nHere are 10,000 samples from the posterior (based on the uniform flat prior). These are the **sampling distributions**.\n\n\n::: {.cell .column-page layout-ncol=\"2\"}\n\n```{.r .cell-code}\nsamples <- sample(p_grid, prob = posterior_normalized, size = 10000, replace = TRUE)\nplot(samples, main = \"10,000 posterior samples\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-6-1.png){width=576}\n:::\n\n```{.r .cell-code}\n#\n\nplot(density(samples), main = \"Distribution of 10,000 posterior samples\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-6-2.png){width=576}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_tidy <- globe_tossing %>% \n  slice_sample(n = 10000, weight_by = posterior_uniform, replace = T)\n```\n:::\n\n\n\n#### 3.2.1: Intervals of defined boundaries\n\n::: {.panel-tabset}\n##### Base R\n\nWhat's the probability that the proportion of water is less than 50%?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(samples < 0.5) / 10000\n## [1] 0.1621\n```\n:::\n\n\nHow much of the posterior is between 50% and 75%?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(samples > 0.5 & samples < 0.75) / 10000\n## [1] 0.5443\n```\n:::\n\n\n\n##### Tidyverse\n\nWhat's the probability that the proportion of water is less than 50%?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_line() +\n  geom_area(data = filter(globe_tossing, p_grid < 0.5))\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-10-1.png){width=576}\n:::\n\n```{.r .cell-code}\n\nsamples_tidy %>% \n  count(p_grid < 0.5) %>% \n  mutate(probability = n / sum(n))\n## # A tibble: 2 × 3\n##   `p_grid < 0.5`     n probability\n##   <lgl>          <int>       <dbl>\n## 1 FALSE           8318       0.832\n## 2 TRUE            1682       0.168\n```\n:::\n\n\nHow much of the posterior is between 50% and 75%?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) +\n  geom_line() +\n  geom_area(data = filter(globe_tossing, p_grid > 0.5 & p_grid < 0.75))\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-11-1.png){width=576}\n:::\n\n```{.r .cell-code}\n\nsamples_tidy %>% \n  count(p_grid > 0.5 & p_grid < 0.75) %>% \n  mutate(probability = n / sum(n))\n## # A tibble: 2 × 3\n##   `p_grid > 0.5 & p_grid < 0.75`     n probability\n##   <lgl>                          <int>       <dbl>\n## 1 FALSE                           3900        0.39\n## 2 TRUE                            6100        0.61\n```\n:::\n\n:::\n\n#### 3.2.2: Intervals of defined mass\n\n::: {.panel-tabset}\n##### Base R\n\nLower 80% posterior probability lies below this number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(samples, 0.8)\n##       80% \n## 0.7777778\n```\n:::\n\n\nMiddle 80% posterior probability lies between these numbers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(samples, c(0.1, 0.9))\n##       10%       90% \n## 0.4444444 0.7777778\n```\n:::\n\n\n50% percentile interval vs. 50% HPDI\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(samples, c(0.25, 0.75))\n##       25%       75% \n## 0.5555556 0.7777778\nrethinking::HPDI(samples, prob = 0.5)\n##      |0.5      0.5| \n## 0.5555556 0.6666667\n```\n:::\n\n\n\n##### Tidyverse\n\nLower 80% posterior probability lies below this number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_tidy %>% \n  summarize(`80th percentile` = quantile(p_grid, 0.8))\n## # A tibble: 1 × 1\n##   `80th percentile`\n##               <dbl>\n## 1             0.758\n```\n:::\n\n\nMiddle 80% posterior probability lies between these numbers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_tidy %>% \n  summarize(q = c(0.1, 0.9), percentile = quantile(p_grid, q)) %>% \n  pivot_wider(names_from = q, values_from = percentile)\n## # A tibble: 1 × 2\n##   `0.1` `0.9`\n##   <dbl> <dbl>\n## 1  0.45  0.81\n```\n:::\n\n\n50% percentile interval vs. 50% HPDI\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_tidy %>% \n  summarize(q = c(0.25, 0.75), \n            percentile = quantile(p_grid, q),\n            hpdi = HDInterval::hdi(p_grid, 0.5))\n## # A tibble: 2 × 3\n##       q percentile  hpdi\n##   <dbl>      <dbl> <dbl>\n## 1  0.25      0.543  0.55\n## 2  0.75      0.735  0.74\n```\n:::\n\n:::\n\n#### 3.2.3: Point estimates\n\n::: {.panel-tabset}\n##### Base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(samples)\n## [1] 0.6381889\nmedian(samples)\n## [1] 0.6666667\n```\n:::\n\n\n##### Tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_tidy %>% \n  summarize(mean = mean(p_grid),\n            median = median(p_grid))\n## # A tibble: 1 × 2\n##    mean median\n##   <dbl>  <dbl>\n## 1 0.636  0.644\n```\n:::\n\n:::\n\n### 3.3: Sampling to simulate prediction\n\n#### Base R\n\nWe can use the uncertainty inherent in the sampling distributions from above to generate a **posterior predictive distribution**, based on a 9-toss situation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior predictive distribution\nposterior_predictive_dist <- rbinom(10000, size = 9, prob = samples)\nhist(posterior_predictive_dist, breaks = 0:9)\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-20-1.png){width=576}\n:::\n:::\n\n\n\n#### Tidyverse style\n\n\n::: {.cell .column-page layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Generate 100,000 samples from the posterior\nsamples_tidy <- globe_tossing %>% \n  slice_sample(n = 100000, weight_by = posterior_uniform, replace = T)\n#\nsamples_tidy %>% \n  mutate(sample_number = 1:n()) %>% \n  ggplot(aes(x = sample_number, y = p_grid)) +\n  geom_point(alpha = 0.05) +\n  labs(title = \"100,000 posterior samples\", x = \"Sample number\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-21-1.png){width=576}\n:::\n\n```{.r .cell-code}\n#\nsamples_tidy %>% \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"grey50\", color = NA) +\n  labs(title = \"Distribution of 100,000 posterior samples\")\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-21-2.png){width=576}\n:::\n:::\n\n\nFigure 3.6 with ggplot\n\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\n# Posterior probability\nglobe_smaller <- globe_tossing %>% \n  filter(p_grid %in% c(seq(0.1, 0.9, 0.1), 0.3))\n\npanel_top <- globe_tossing %>% \n  ggplot(aes(x = p_grid, y = posterior_uniform)) + \n  geom_area(fill = \"grey50\", color = NA) +\n  geom_segment(data = globe_smaller, aes(xend = p_grid, yend = 0, size = posterior_uniform)) +\n  geom_point(data = globe_smaller) +\n  scale_size_continuous(range = c(0, 1), guide = \"none\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"Proportion/probability of water\",\n       title = \"Posterior probability\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n\n# Sampling distributions\nglobe_sample_dists <- tibble(probability = seq(0.1, 0.9, 0.1)) %>% \n  mutate(draws = map(probability, ~{\n    set.seed(1234)\n    rbinom(10000, size = 9, prob = .x)\n  })) %>% \n  unnest(draws) %>% \n  mutate(label = paste0(\"p = \", probability))\n\npanel_middle <- ggplot(globe_sample_dists, aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.1) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = NULL, y = NULL, title = \"Sampling distributions\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\")) +\n  facet_wrap(vars(label), ncol = 9)\n\n# Posterior predictive distribution\nglobe_samples <- globe_tossing %>% \n  slice_sample(n = 10000, weight_by = posterior_uniform, replace = TRUE) %>% \n  mutate(prediction = map_dbl(p_grid, rbinom, n = 1, size = 9))\n\npanel_bottom <- globe_samples %>% \n  ggplot(aes(x = prediction)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.5) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = \"Number of water samples\", y = NULL, title = \"Posterior predictive distribution\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n\nlayout <- \"\nAAAAAAAAAAA\n#BBBBBBBBB#\n###CCCCC###\n\"\n\npanel_top / panel_middle / panel_bottom +\n  plot_layout(design = layout)\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-22-1.png){width=100%}\n:::\n:::\n\n\n# brms and tidybayes version of all this\n\nOoh neat, you can pass single values as data instead of a data frame! Everything else here looks like regular **brms** stuff.\n\n\n::: {.cell hash='rethinking-lecture-02_cache/html/model-brms_4d74d28d34c962b504fa9591cd4eeb31'}\n\n```{.r .cell-code}\nmodel_globe <- brm(\n  bf(water | trials(9) ~ 0 + Intercept),\n  data = list(water = 6),\n  family = binomial(link = \"identity\"),\n  # Flat uniform prior\n  prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000, seed = 1234,\n  # TODO: Eventually switch to cmdstanr once this issue is fixed\n  # https://github.com/quarto-dev/quarto-cli/issues/2258\n  backend = \"rstan\", cores = 4\n)\n## Compiling Stan program...\n## Start sampling\n```\n:::\n\n\nCredible intervals / HPDI / etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using broom.mixed\ntidy(model_globe, effects = \"fixed\",\n     conf.level = 0.5, conf.method = \"HPDinterval\")\n## # A tibble: 1 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   <chr>  <chr>     <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  cond      (Intercept)    0.639     0.137    0.568     0.761\n\n# Using the posterior package\ndraws <- as_draws_array(model_globe)\nsummarize_draws(draws, default_summary_measures()) %>% \n  filter(variable == \"b_Intercept\")\n## # A tibble: 1 × 7\n##   variable     mean median    sd   mad    q5   q95\n##   <chr>       <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 b_Intercept 0.639  0.648 0.137 0.144 0.400 0.848\n\n# Using tidybayes\n# get_variables(model_globe)\nmodel_globe %>% \n  spread_draws(b_Intercept) %>% \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.648  0.568  0.761   0.5  median hdci     \n## 2       0.648  0.430  0.864   0.89 median hdci     \n## 3       0.648  0.374  0.892   0.95 median hdci\n\nmodel_globe %>% \n  gather_draws(b_Intercept) %>% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye()\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-23-1.png){width=576}\n:::\n:::\n\n\nPredictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_globe %>% \n  predicted_draws(newdata = tibble(nothing = 1)) %>% \n  ggplot(aes(x = .prediction)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\", size = 0.5) +\n  scale_x_continuous(breaks = seq(0, 9, 3)) +\n  scale_y_continuous(breaks = NULL) +\n  coord_cartesian(xlim = c(0, 9)) +\n  labs(x = \"Number of water samples\", y = NULL, title = \"Posterior predictive distribution\") +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](rethinking-lecture-02_files/figure-html/unnamed-chunk-24-1.png){width=576}\n:::\n:::\n\n\n\n# Homework\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(homeworkch3, package = \"rethinking\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}