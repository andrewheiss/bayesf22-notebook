{
  "hash": "652fdd1a6ff328c66683faf43ea626f8",
  "result": {
    "markdown": "---\ntitle: \"Video #11 code\"\nsubtitle: \"Ordered categories\"\ndate: \"October 31, 2022\"\n---\n\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/-397DMPooR8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(broom.mixed)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\n\n# Data stuff\ndata(Trolley, package = \"rethinking\")\n\nedu_levels <- c(\"Elementary School\",\n                \"Middle School\",\n                \"Some High School\",\n                \"High School Graduate\",\n                \"Some College\",\n                \"Bachelor's Degree\",\n                \"Master's Degree\",\n                \"Graduate Degree\")\n\ntrolley <- Trolley |> \n  mutate(resp_ord = factor(response, ordered = TRUE),\n         edu = factor(edu, levels = edu_levels, ordered = TRUE))\n```\n:::\n\n\n# The weirdness of ordered categories\n\nOur main estimand: How do action, intention, and contact influence someone's response to a trolley story? \n(Where `response` is measured on an ordered categorical scale ranging from 1 to 7)\n\nI don't know what action, intention, and contact really actually mean—I'm not a moral philosopher or anything. \nSo I'll just go with the mechanics of model fitting here.\n\nTo get an overview of the data, here's the distribution of the possible trolley problem responses.\nThe combination of its categoricalness and its orderedness makes it tricky to work with, since something like 4 inherently implies values of 1, 2, and 4 (i.e. the responses are cumulative), and since the distance between these categories isn't the same (i.e. moving from 3 → 4 isn't the same as moving from 6 → 7).\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- trolley |> \n  ggplot(aes(x = resp_ord)) +\n  stat_count(width = 0.5, fill = clrs[3]) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Response\", y = \"Count\")\np1\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThese responses build on each other—if someone responds with a 4, it means that they also were a 1, 2, and 3. \nEach value is bigger than the previous value. \nWe can calculate the probability of each category on its own as well as the cumulative probability, or the probability of at least 4, or 5, or whatever\n\n\n::: {.cell}\n\n```{.r .cell-code}\np2 <- trolley |> \n  count(resp_ord) |> \n  mutate(pr_k = n / sum(n),\n         cum_pr_k = cumsum(pr_k)) |> \n  ggplot(aes(x = resp_ord, y = cum_pr_k)) +\n  geom_line(aes(group = 0), color = clrs[2], size = 1) +\n  geom_point(shape = 21, fill = clrs[2], color = \"white\", size = 5, stroke = 1) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Response\", y = \"Cumulative proportion\")\np2\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nBut modeling probabilities is tricky, so instead we use a link function to transform the probabilities into something more amenable to linear modeling, like log odds or logits (just like logistic regression). \nWe use the log cumulative odds that a response value $y_i$ is some possible outcome value $k$ (1–7 here), and each $k$ has its own \"intercept\", or $\\alpha_k$, that represents the boundary between categories\n\n$$\n\\log \\frac{\\Pr(y_i \\leq k)}{1 - \\Pr(y_i \\leq k)} = \\alpha_k\n$$\nWe can convert probabilities to this log cumulative odds scale with `qlogis()`. \n(McElreath makes his own `logit()` function like `logit <- function(x) log(x / (1 - x))`, but that's the same as `qlogis()`.)\nNote that $\\alpha_7$ here is infinity; it's the end of of the distribution and already contains 100% of the previous values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrolley |> \n  count(resp_ord) |> \n  mutate(pr_k = n / sum(n),\n         cum_pr_k = cumsum(pr_k)) |> \n  mutate(alpha_k = qlogis(cum_pr_k))\n##   resp_ord    n       pr_k  cum_pr_k    alpha_k\n## 1        1 1274 0.12829809 0.1282981 -1.9160912\n## 2        2  909 0.09154079 0.2198389 -1.2666056\n## 3        3 1071 0.10785498 0.3276939 -0.7186340\n## 4        4 2323 0.23393756 0.5616314  0.2477857\n## 5        5 1462 0.14723061 0.7088620  0.8898637\n## 6        6 1445 0.14551863 0.8543807  1.7693809\n## 7        7 1446 0.14561934 1.0000000        NaN\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np3 <- trolley |> \n  count(resp_ord) |> \n  mutate(pr_k = n / sum(n),\n         cum_pr_k = cumsum(pr_k)) |> \n  mutate(alpha_k = qlogis(cum_pr_k)) |> \n  ggplot(aes(x = resp_ord, y = alpha_k)) +\n  geom_line(aes(group = 0), size = 1, color = clrs[1]) +\n  geom_point(shape = 21, fill = clrs[1], color = \"white\", size = 5, stroke = 1) +\n  labs(x = \"Response\", y = \"Log cumulative odds\")\np3\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nHere are all three plots simultaneously to see how to translate from category counts into logit-scale cumulative odds:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 | p2 | p3\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-5-1.png){width=768}\n:::\n:::\n\n\n\n# Intercept-only model\n\nWe can write a formal model for this intercept-only approach like so:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\operatorname{Categorical}(p) \\\\\n\\operatorname{logit}(p_k) &= \\alpha_k - \\phi \\\\\n\\phi &= 0 \\\\\n\\alpha_k &\\sim \\mathcal{N}(1, 1.5)\n\\end{aligned}\n$$\n\nThat $\\phi$ is a placeholder for the offsets that come from a linear model; here it's just 0. \nWe can also write this using the \"Ordered distribution\" shorthand distribution:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\operatorname{Ordered\\ logit}(\\phi_i, \\alpha) \\\\\n\\phi_i &= 0 \\\\\n\\alpha_k &\\sim \\mathcal{N}(1, 1.5)\n\\end{aligned}\n$$\n\nIn **brms** we can fit this with `family = cumulative()`. \n**rstanarm** supports ordered logit too with `stan_polr()`, but I won't use it here because it's less fully featured than **brms** for these more advanced models (and it wasn't working with an intercept-only model `:shrug:`).\n\n[Solomon Kurz does a neat thing](https://bookdown.org/content/4857/monsters-and-mixtures.html#describing-an-ordered-distribution-with-intercepts.) with setting initial chain values for each of the intercepts at their approximate locations on the logit scale, which helps with model fit. \nIn his code, he does this with bracketed subscripts:\n\n```r\ninits <- list(`Intercept[1]` = -2,\n              `Intercept[2]` = -1,\n              `Intercept[3]` = 0,\n              `Intercept[4]` = 1,\n              `Intercept[5]` = 2,\n              `Intercept[6]` = 2.5)\n```\n\nNowadays, though, Stan gets mad at that and wants just one named element with a vector of values, like this:\n\n```r\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5))\n```\n\nLet's make the official model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 1.5), class = Intercept))\n\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5))\n\nmodel_int_only <- brm(\n  bf(resp_ord ~ 1),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  init = rep(list(inits), 4),\n  save_warmup = TRUE,\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", refresh = 0,\n  file = \"11-manual-cache/model-int-only\"\n)\n```\n:::\n\n\n**tidybayes** doesn't have a way to extract warmup samples (I don't think?), but we can look at them with `posterior::as_draws_df()`. \nWe'll look at the first 100 draws just to see how the custom `inits` worked.\nThis is actually pretty neat—the chains for each of the intercepts start at `c(-2, -1, 0, 1, 2, 2.5)` respectively (the first draws aren't any of those values precisely, since they're the draws that come after starting at those values). \nAfter the first couple dozen draws, the chains start converging on the real posterior space and dance around it.\nBut we helped the chains get there faster than if we had set all the intercepts to 0 or some random numbers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_only_samples <- as_draws_df(model_int_only, inc_warmup = TRUE) |> \n  pivot_longer(starts_with(\"b_Intercept\"))\nhead(int_only_samples, 6)\n## # A tibble: 6 × 8\n##    disc lprior    lp__ .chain .iteration .draw name             value\n##   <dbl>  <dbl>   <dbl>  <int>      <int> <int> <chr>            <dbl>\n## 1     1  -10.3 -19252.      1          1     1 b_Intercept[1] -2.22  \n## 2     1  -10.3 -19252.      1          1     1 b_Intercept[2] -1.48  \n## 3     1  -10.3 -19252.      1          1     1 b_Intercept[3] -0.756 \n## 4     1  -10.3 -19252.      1          1     1 b_Intercept[4]  0.0863\n## 5     1  -10.3 -19252.      1          1     1 b_Intercept[5]  0.889 \n## 6     1  -10.3 -19252.      1          1     1 b_Intercept[6]  1.42\n\nint_only_samples |> \n  filter(.iteration < 100) |>\n  ggplot(aes(x = .iteration, y = value, color = factor(.chain))) +\n  geom_line(size = 0.5) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(name), scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nAnyway, here's the actual post-warmup traceplot using the normal **tidybayes** approach, since that's all we really care about:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int_only |> \n  gather_draws(`^b_Intercept.*`, regex = TRUE) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n…and the actual posteriors for these intercept cutpoints:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int_only\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ 1 \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -1.92      0.03    -1.97    -1.86 1.00     2553     2626\n## Intercept[2]    -1.27      0.02    -1.31    -1.22 1.00     3728     3257\n## Intercept[3]    -0.72      0.02    -0.76    -0.67 1.00     4235     3490\n## Intercept[4]     0.25      0.02     0.21     0.29 1.00     4496     3365\n## Intercept[5]     0.89      0.02     0.85     0.93 1.00     4385     3551\n## Intercept[6]     1.77      0.03     1.71     1.83 1.00     4671     3605\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int_only |> \n  gather_draws(`^b_Intercept.*`, regex = TRUE) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_viridis_d(option = \"rocket\", end = 0.85, guide = \"none\") +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThese cutpoints are on the logit scale, so we can unlogit them with `plogis()` to get probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int_only |> \n  gather_draws(`^b_Intercept.*`, regex = TRUE) |> \n  mutate(.value = plogis(.value)) |> \n  ggplot(aes(x = .value, fill = .variable)) +\n  stat_halfeye(normalize = \"xy\") +\n  scale_fill_viridis_d(option = \"rocket\", end = 0.85, guide = \"none\") +\n  scale_x_continuous(labels = label_percent()) +\n  facet_wrap(vars(.variable), scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWoot!\n\n\n# Thinking about shifts in thresholds\n\nIn the intercept-only model we left the $\\phi$ part of the overall model blank ($\\phi = 0$), but in practice, we want to use that part of the model to see how covariates influence the probabilities of specific categories. \n$\\phi$ shifts the different $\\alpha$ cutpoints around.\nThe non-shortcut-y model shows this:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\operatorname{Categorical}(p) \\\\\n\\operatorname{logit}(p_k) &= \\alpha_k - \\phi \\\\\n\\phi &= \\beta_1 x_i + \\beta_2 x_2 + \\dots \\\\\n\\alpha_k &\\sim \\mathcal{N}(1, 1.5)\n\\end{aligned}\n$$\n\n\n\nTo help with the intuition of why we subtract $\\phi$ from each $\\alpha$, let's convert the cutpoints from the intercept-only model to probabilities using `rethinking::dordlogit()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_k <- model_int_only |> \n  tidy(effects = \"fixed\") |> \n  pull(estimate) |> \n  rethinking::dordlogit(1:7, 0, a = _)\nprob_k\n## [1] 0.12837374 0.09155969 0.10794372 0.23384904 0.14722269 0.14547466 0.14557646\n```\n:::\n\n\nThese probabilities imply an average response outcome of 4ish:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(prob_k * 1:7)\n## [1] 4.198717\n```\n:::\n\n\nHere's what that distribution looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(prob_k = prob_k,\n       response = 1:7) |> \n  ggplot(aes(x = response, y = prob_k)) +\n  geom_col(width = 0.5, fill = clrs[4]) +\n  labs(title = \"Response probabilities\",\n       subtitle = \"Converted from original log odds in model\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nIf the cumulative log odds *increases* each of these intercepts, the probability of lower values increases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_k_plus_5 <- model_int_only |> \n  tidy(effects = \"fixed\") |>\n  mutate(estimate = estimate + 0.5) |> \n  pull(estimate) |> \n  rethinking::dordlogit(1:7, 0, a = _)\n\nsum(prob_k_plus_5 * 1:7)\n## [1] 3.655766\n\ntibble(prob_k = prob_k_plus_5,\n       response = 1:7) |> \n  ggplot(aes(x = response, y = prob_k)) +\n  geom_col(width = 0.5, fill = colorspace::darken(clrs[4], 0.3)) +\n  labs(title = \"Response probabilities\",\n       subtitle = \"Log odds + 0.5\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nAnd if the cumulative log odds *decreases* each of these intercepts, the probability of higher values increases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_k_minus_5 <- model_int_only |> \n  tidy(effects = \"fixed\") |>\n  mutate(estimate = estimate - 0.5) |> \n  pull(estimate) |> \n  rethinking::dordlogit(1:7, 0, a = _)\n\nsum(prob_k_minus_5 * 1:7)\n## [1] 4.729218\n\ntibble(prob_k = prob_k_minus_5,\n       response = 1:7) |> \n  ggplot(aes(x = response, y = prob_k)) +\n  geom_col(width = 0.5, fill = colorspace::lighten(clrs[4], 0.3)) +\n  labs(title = \"Response probabilities\",\n       subtitle = \"Log odds − 0.5\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWith a linear model, positive coefficients like $\\beta_1$ increase the value of the outcome $\\phi$, and we want that to match the direction of the shifts in category probabilities so that an increase in one of the $\\beta$s leads to a higher probability of bigger categories. To maintain that direction, we need to *subtract* $\\phi$ from each of the intercepts. McElreath says this:\n\n> This way, a positive $\\beta$ value indicates that an increase in the predictor variable $x$ results in an increase in the average response (p. 387).\n\n***All this is just to say*** that in the formal model we use $\\operatorname{logit}(p_k) = \\alpha_k - \\phi$. Neat.\n\n\n# Model with covariates\n\nHere we'll look at the effect of action, contact, and intention (whatever those are) on the distribution of possible responses to the trolley problem.\n\nFor fun, here's the formal model in two different styles:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\operatorname{Categorical}(p) \\\\\n\\operatorname{logit}(p_k) &= \\alpha_k - \\phi_i \\\\\n\\phi_i &= \\beta_1 \\text{Action}_i + \\beta_2 \\text{Contact}_i + \\beta_3 \\text{Intention}_i \\\\\n\\\\\nB_{1, 2, 3} &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\alpha_k &\\sim \\mathcal{N}(0, 1)\n\\end{aligned}\n$$\n\nOrdered logit shortcut-y thing:\n\n$$\n\\begin{aligned}\nR_i &\\sim \\operatorname{Ordered\\ logit}(\\phi_i, \\alpha) \\\\\n\\phi_i &= \\beta_1 \\text{Action}_i + \\beta_2 \\text{Contact}_i + \\beta_3 \\text{Intention}_i \\\\\n\\\\\nB_{1, 2, 3} &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\alpha_k &\\sim \\mathcal{N}(0, 1)\n\\end{aligned}\n$$\n\nbrms time!\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b))\n\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5),\n              b = 0)\n\nmodel_aci <- brm(\n  bf(resp_ord ~ action + intention + contact),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  init = rep(list(inits), 4),\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", refresh = 0,\n  file = \"11-manual-cache/model-aci-only\"\n)\n```\n:::\n\n\nThat takes a couple minutes to run, but it works great.\nTo verify we can look at the trace plots and trank plots and see that everything converged.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_aci |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +\n  geom_line(size = 0.1) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_aci |> \n  gather_draws(`^b_.*`, regex = TRUE) |> \n  group_by(.variable) |> \n  mutate(draw_rank = rank(.value)) |> \n  ggplot(aes(x = draw_rank, color = factor(.chain))) +\n  stat_bin(geom = \"step\", binwidth = 250, position = position_identity(), boundary = 0) +\n  scale_color_viridis_d(option = \"rocket\", end = 0.85) +\n  facet_wrap(vars(.variable), scales = \"free_y\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThe Rhats and ESS are all fantastic too:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_aci\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ action + intention + contact \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -2.83      0.05    -2.92    -2.73 1.00     2892     2799\n## Intercept[2]    -2.15      0.04    -2.23    -2.06 1.00     3114     2989\n## Intercept[3]    -1.56      0.04    -1.64    -1.49 1.00     3421     3208\n## Intercept[4]    -0.54      0.04    -0.61    -0.47 1.00     3667     3307\n## Intercept[5]     0.12      0.04     0.05     0.20 1.00     3857     3736\n## Intercept[6]     1.03      0.04     0.95     1.11 1.00     4275     3232\n## action          -0.70      0.04    -0.78    -0.62 1.00     3835     3063\n## intention       -0.72      0.04    -0.79    -0.64 1.00     4295     3033\n## contact         -0.95      0.05    -1.04    -0.85 1.00     3861     2987\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nSo what the heck does this all mean?\nIn all my past work with ordered logit models, I've just looked at predicted probabilities and marginal effects for each of the categories across a range of some predictor $X$ because that seemed the easiest.\nTrying to interpret all these cumulative log odds scale things is really hard. \nWe can inverse logit the cutpoints, but the coefficients for action, intention, and contact are all doing… something?… to them?\nEasiest way to see what's going on is to simulate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_conditions <- tribble(\n  ~title, ~newdata,\n  \"A = 0, I = 0, C = 0\", tibble(action = 0, intention = 0, contact = 0),\n  \"A = 0, I = 1, C = 0\", tibble(action = 0, intention = 1, contact = 0),\n  \"A = 1, I = 0, C = 0\", tibble(action = 1, intention = 0, contact = 0),\n  \"A = 1, I = 1, C = 0\", tibble(action = 1, intention = 1, contact = 0),\n  \"A = 0, I = 0, C = 1\", tibble(action = 0, intention = 0, contact = 1),\n  \"A = 0, I = 1, C = 1\", tibble(action = 0, intention = 1, contact = 1)\n) |> \n  mutate(pred_plot = map2(newdata, title, ~{\n    model_aci |> \n      add_predicted_draws(newdata = .x) |> \n      ungroup() |> \n      count(.prediction) |> \n      mutate(prop = n / sum(n),\n             prop_nice = label_percent(accuracy = 0.1)(prop)) |> \n      ggplot(aes(x = .prediction, y = n)) +\n      geom_col(aes(fill = .prediction)) +\n      geom_text(aes(y = 50, label = prop_nice), color = \"white\", size = 2.5, \n                angle = 90, hjust = 0) +\n      scale_y_continuous(labels = label_comma()) +\n      scale_fill_viridis_d(option = \"rocket\", end = 0.85, guide = \"none\") +\n      labs(x = \"Response\", y = \"Count\", \n           title = .y) +\n      theme(plot.title = element_text(size = rel(1), hjust = 0.5))\n  }))\n\nwrap_plots(simulated_conditions$pred_plot, nrow = 2, byrow = FALSE)\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nWhen we move from all the conditions being off (A = 0, I = 0, C = 0) and then flip on Intention, the probability of the story seeming appropriate drops. \nOriginally the most common response was a 7; now it's a 4. \nFlipping A to 1 also increases the probability of lower categories, and using both A and I makes 1s really common.\nThe same thing happens with C—flipping C to 1 increases the probability of 4, and when using both I and C, 1 becomes the most common response.\nThe coefficients for all these covariates are negative, so enabling them shifts the distribution down and makes lower response values more likely.\n\n\n# Model with covariates and gender-based stratification\n\nWe can include other competing causes here, like stratifying by gender.\nMcElreath uses `ulam()` to directly stratify these terms by gender like `bA[G]*A + bI[G]*I + bC[G]*C`.\nWe can do the same thing with **brms** either with some weird `nl` syntax or by including an interaction term with `:` instead of `*` ([just like in video 4 here](04-video.qmd#sex-height)).\n\nHowever, neither the `nl` syntax nor the `:` approaches seem to work here. With regular regression, we can remove the intercept term (with `0 + ...`) and get group-specific coefficients. \nWith ordered logit, though, the model for $\\phi$ doesn't have an intercept, so including `0 + ...` doesn't work. \nThe coefficients we get are still correct—the model isn't wrong. \nIt's just that the pairs of coefficients aren't the full values—one is an offset.\nLike with `action`, the mean of the two action coefficients look like this:\n\n```\naction       -0.89\naction:male   0.35\n```\n\nIn McElreath's video, they look like this:\n\n```\nbA[1]        -0.88\nbA[2]        -0.53\n```\n\nThese are the same. It's just that `action:male` is the offset from `action` when `male` is set to 1, making its coefficient -0.89 + 0.35 = -0.54, just like `bA[2]`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b))\n\ninits <- list(Intercept = c(-2, -1, 0, 1, 2, 2.5),\n              b = 0)\n\nmodel_aci_gender <- brm(\n  bf(resp_ord ~ action + action:male + intention + \n       intention:male + contact + contact:male),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  init = rep(list(inits), 4),\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", threads = threading(2),\n  file = \"11-manual-cache/model-aci-gender\", refresh = 0\n)\n```\n:::\n\n\nBy default we get the offsets for `male`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_aci_gender\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ action + action:male + intention + intention:male + contact + contact:male \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]      -2.85      0.05    -2.94    -2.75 1.00     3918     3277\n## Intercept[2]      -2.16      0.04    -2.25    -2.08 1.00     4372     3395\n## Intercept[3]      -1.58      0.04    -1.66    -1.50 1.00     4746     3541\n## Intercept[4]      -0.54      0.04    -0.62    -0.47 1.00     5084     3577\n## Intercept[5]       0.13      0.04     0.06     0.20 1.00     5203     4048\n## Intercept[6]       1.05      0.04     0.97     1.12 1.00     5526     4131\n## action            -0.89      0.05    -0.99    -0.79 1.00     3884     3272\n## intention         -0.90      0.05    -0.99    -0.81 1.00     4021     2910\n## contact           -1.07      0.07    -1.20    -0.93 1.00     3989     3030\n## action:male        0.35      0.06     0.23     0.46 1.00     4153     3340\n## intention:male     0.34      0.06     0.22     0.45 1.00     3300     3157\n## contact:male       0.21      0.08     0.04     0.38 1.00     3781     2904\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nBut we can wrangle the data a little to replicate what McElreath has with his `precis()` output:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-aci-gender-precis.png){width=942}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_aci_gender |> \n  spread_draws(`^b_.*`, regex = TRUE) |> \n  mutate(`b_action:male` = b_action + `b_action:male`,\n         `b_intention:male` = b_intention + `b_intention:male`,\n         `b_contact:male` = b_contact + `b_contact:male`) |> \n  pivot_longer(starts_with(\"b_\"), names_to = \".variable\") |> \n  group_by(.variable) |> \n  summarize(avg = mean(value))\n## # A tibble: 12 × 2\n##    .variable           avg\n##    <chr>             <dbl>\n##  1 b_action         -0.890\n##  2 b_action:male    -0.544\n##  3 b_contact        -1.07 \n##  4 b_contact:male   -0.858\n##  5 b_intention      -0.901\n##  6 b_intention:male -0.563\n##  7 b_Intercept[1]   -2.85 \n##  8 b_Intercept[2]   -2.16 \n##  9 b_Intercept[3]   -1.58 \n## 10 b_Intercept[4]   -0.545\n## 11 b_Intercept[5]    0.131\n## 12 b_Intercept[6]    1.05\n```\n:::\n\n\nCool cool. Everything here makes people rate things lower, but with different magnitudes—men shift less than women.\n\n\n# Ordered predictors and monotonic effects\n\nAccording to the DAG in this example, we need to also control for age and education to close backdoors for identification. \nThat seems straightforward enough—here's how those two variables are distributed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- trolley |> \n  ggplot(aes(x = edu)) +\n  stat_count(width = 0.5, fill = clrs[6]) +\n  scale_x_discrete(labels = label_wrap(15)) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Education\", y = \"Count\")\n\np2 <- trolley |> \n  ggplot(aes(x = age)) +\n  stat_count(fill = clrs[5]) +\n  labs(x = \"Age\", y = \"Count\")\n\np1 / p2\n```\n\n::: {.cell-output-display}\n![](10-video_files/figure-html/unnamed-chunk-23-1.png){width=768}\n:::\n:::\n\n\nAge is fine and continuous and we can just throw `+ age` into the model.\nEducation is a little trickier though. \nIt's an ordered category, which means it is analogous to the outcome variable on the lefthand side of the model. \nWe don't want to assume that the distance between each ordinal value in the response is the same; we also don’t want to assume that the distance between values in an ordinal predictor is the same either. \nIncluding a factor version of education in a regression model gives us category-specific coefficients (`+ factor(educ)`), this removes the inherent cumulative ordering (where someone with a doctorate also has some high school). \nIncluding a numeric version of education (`+ as.numeric(educ)`) in a regression model forces the correct cumulative ordering, but then it assumes that distance between 1→2 is the same as 5→6, and so on, and we don't want that.\nWe want to somehow maintain the ordered categorical properties of education on the righthand side of the model.\n\n**This is all super neat and new to me!** \nI've never worried about the weirdness inherent in working with righthand side ordered variables and have just lived with their categorical coefficients.\nBut there's a magical way of dealing with this that mostly goes over my head and beyond my math skills. We can define the probability of specific levels of education as a series of cumulative cutpoints (super similar to what we do with categorical outcomes), like this:\n\n| Level             | Equation                                                                            |\n|----------------------------|--------------------------------------------|\n| Elementary school | $\\phi = 0$                                                                          |\n| Middle school     | $\\phi = \\delta_1$                                                                   |\n| Some high school  | $\\phi = \\delta_1 + \\delta_2$                                                        |\n| High school       | $\\phi = \\delta_1 + \\delta_2 + \\delta_3$                                             |\n| Some college      | $\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4$                                |\n| College           | $\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4 + \\delta_5$                       |\n| Masters           | $\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4 + \\delta_5 + \\delta_6$            |\n| Doctorate         | $\\phi = \\delta_1 + \\delta_2 + \\delta_3 + \\delta_4 + \\delta_5 + \\delta_6 + \\delta_7$ |\n\nWe can then collapse all these levels into a single sum that adds up to 1, creating something called a *simplex* (or a vector that adds to 1), and we can include *that* as a term in the regression model:\n\n$$\n\\phi_i = \\beta_\\text{E} \\sum_{j = 0}^{\\text{E}_i - 1} \\delta_j\n$$\n\nPhew, that's wild. \nThis new term takes a new set of priors, both for the $\\beta$ coefficient and for each of the $\\delta$ cutpoints. \nIn the video and book, McElreath uses a Dirichlet prior for these $\\delta$ values.\n\nMcElreath's `ulam()` function requires manual construction of the simplex parameter for education, but **brms** can do this all automatically if we put the education term inside `mo()`, which forces it to be a monotonic categorical variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(normal(0, 0.143), class = b, coef = moedu),\n            prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu1))\n\nmodel_monotonic <- brm(\n  bf(resp_ord ~ 1 + action + contact + intention + mo(edu)),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", threads = threading(2), \n  file = \"11-manual-cache/model-monotonic\"\n)\n```\n:::\n\n\nOooh look at all these parameters now! We have a whole new section I've never seen before called \"Simplex Parameters\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_monotonic\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ 1 + action + contact + intention + mo(edu) \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]    -3.12      0.17    -3.51    -2.84 1.00     1832     1857\n## Intercept[2]    -2.44      0.17    -2.83    -2.16 1.00     1834     1819\n## Intercept[3]    -1.86      0.17    -2.24    -1.58 1.00     1833     1941\n## Intercept[4]    -0.84      0.16    -1.22    -0.57 1.00     1860     1941\n## Intercept[5]    -0.17      0.16    -0.55     0.10 1.00     1864     2021\n## Intercept[6]     0.74      0.17     0.35     1.01 1.00     1814     1916\n## action          -0.70      0.04    -0.78    -0.62 1.00     3961     3329\n## contact         -0.95      0.05    -1.05    -0.86 1.00     3685     2854\n## intention       -0.72      0.04    -0.79    -0.65 1.00     4434     3307\n## moedu           -0.05      0.03    -0.11    -0.01 1.00     1852     1895\n## \n## Simplex Parameters: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## moedu1[1]     0.26      0.15     0.04     0.59 1.00     2300     2342\n## moedu1[2]     0.14      0.09     0.02     0.36 1.00     3906     2317\n## moedu1[3]     0.19      0.11     0.03     0.43 1.00     3915     2433\n## moedu1[4]     0.16      0.09     0.03     0.38 1.00     3701     2816\n## moedu1[5]     0.04      0.04     0.00     0.13 1.00     2793     2354\n## moedu1[6]     0.09      0.06     0.01     0.24 1.00     3433     2752\n## moedu1[7]     0.12      0.07     0.02     0.30 1.00     4205     3054\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nWe can even interact the education monotonic simplex thing with other variables like age:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(normal(0, 0.143), class = b, coef = moedu),\n            prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu1))\n\nmodel_monotonic_age <- brm(\n  bf(resp_ord ~ action*male + intention*male + contact*male + \n       age*male + mo(edu)*male),\n  data = trolley,\n  family = cumulative(link = \"logit\"),\n  prior = priors,\n  chains = 4, iter = 2000, seed = BAYES_SEED, cores = 4,\n  backend = \"cmdstanr\", threads = threading(2), \n  file = \"11-manual-cache/model-monotonic-age\"\n)\n```\n:::\n\n\nThough it takes a while to fit these `mo()` models! (This is on an 8-core 2021 M1 MacBook Pro)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# No interactions\nrstan::get_elapsed_time(model_monotonic$fit) |> \n  as_tibble(rownames = \"chain\") |> \n  mutate(total = lubridate::as.duration(warmup + sample))\n## # A tibble: 4 × 4\n##   chain   warmup sample total                   \n##   <chr>    <dbl>  <dbl> <Duration>              \n## 1 chain:1   294.   165. 459.86s (~7.66 minutes) \n## 2 chain:2   295.   136. 430.263s (~7.17 minutes)\n## 3 chain:3   279.   169. 448.288s (~7.47 minutes)\n## 4 chain:4   236.   189. 425.502s (~7.09 minutes)\n\n# With age*education interaction\nrstan::get_elapsed_time(model_monotonic_age$fit) |> \n  as_tibble(rownames = \"chain\") |> \n  mutate(total = lubridate::as.duration(warmup + sample))\n## # A tibble: 4 × 4\n##   chain   warmup sample total                    \n##   <chr>    <dbl>  <dbl> <Duration>               \n## 1 chain:1   407.   220. 626.811s (~10.45 minutes)\n## 2 chain:2   377.   255. 632.486s (~10.54 minutes)\n## 3 chain:3   449.   233. 681.639s (~11.36 minutes)\n## 4 chain:4   437.   218. 654.839s (~10.91 minutes)\n```\n:::\n\n\nWild. \nAbsolutely bonkers. \nI have no idea what to do with all these moving parts lol.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_monotonic_age\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: resp_ord ~ action * male + intention * male + contact * male + age * male + mo(edu) * male \n##    Data: trolley (Number of observations: 9930) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]      -3.12      0.26    -3.71    -2.69 1.00     1222     1722\n## Intercept[2]      -2.43      0.26    -3.02    -2.01 1.00     1226     1829\n## Intercept[3]      -1.85      0.26    -2.44    -1.43 1.00     1221     1750\n## Intercept[4]      -0.80      0.26    -1.39    -0.39 1.00     1225     1772\n## Intercept[5]      -0.11      0.26    -0.69     0.30 1.00     1231     1829\n## Intercept[6]       0.82      0.26     0.24     1.24 1.00     1240     1864\n## action            -0.55      0.06    -0.67    -0.44 1.00     2307     2667\n## male               0.53      0.19     0.13     0.87 1.00     1637     2178\n## intention         -0.66      0.05    -0.76    -0.56 1.00     2725     2680\n## contact           -0.77      0.07    -0.90    -0.63 1.00     2560     2896\n## age               -0.00      0.00    -0.00     0.00 1.00     5184     3384\n## action:male       -0.29      0.08    -0.45    -0.14 1.00     2192     2740\n## male:intention    -0.12      0.07    -0.27     0.02 1.00     2664     3138\n## male:contact      -0.37      0.09    -0.56    -0.18 1.00     2612     3033\n## male:age          -0.01      0.00    -0.01    -0.00 1.00     5852     2837\n## moedu             -0.12      0.04    -0.21    -0.06 1.00     1163     1756\n## moedu:male         0.13      0.03     0.07     0.19 1.00     1402     2017\n## \n## Simplex Parameters: \n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## moedu1[1]          0.32      0.14     0.06     0.58 1.00     1592     2122\n## moedu1[2]          0.10      0.06     0.01     0.25 1.00     3579     2303\n## moedu1[3]          0.15      0.08     0.03     0.35 1.00     4135     2789\n## moedu1[4]          0.20      0.08     0.06     0.39 1.00     2237     2240\n## moedu1[5]          0.03      0.02     0.00     0.07 1.00     2811     2136\n## moedu1[6]          0.15      0.07     0.04     0.30 1.00     2521     2965\n## moedu1[7]          0.06      0.04     0.01     0.18 1.00     3165     2425\n## moedu:male1[1]     0.07      0.06     0.00     0.22 1.00     3129     1703\n## moedu:male1[2]     0.17      0.11     0.01     0.41 1.00     2647     2341\n## moedu:male1[3]     0.19      0.11     0.01     0.41 1.00     2808     1838\n## moedu:male1[4]     0.04      0.03     0.00     0.13 1.00     3275     1825\n## moedu:male1[5]     0.31      0.10     0.15     0.53 1.00     2006     2331\n## moedu:male1[6]     0.19      0.09     0.04     0.37 1.00     2191     1994\n## moedu:male1[7]     0.03      0.03     0.00     0.12 1.00     4564     2097\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n",
    "supporting": [
      "10-video_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}