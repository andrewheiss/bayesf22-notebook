{
  "hash": "fee5ca4f1901e7229f88e3810b8294c4",
  "result": {
    "markdown": "---\ntitle: \"Reading notes\"\nsubtitle: \"Posterior inference and prediction\"\ndate: \"September 28, 2022\"\n---\n\n\n[(Original chapter)](https://www.bayesrulesbook.com/chapter-8.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(tidybayes)\nlibrary(ggdist)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\n```\n:::\n\n\n\n# The general setup\n\nWe want to know the probability that an artist in the MoMA is Gen X or younger (born after 1965). This is our $\\pi$.\n\nWe'll use a vague $\\operatorname{Beta}(4, 6)$ prior for $\\pi$ and say that the probability is probably below 0.5, but we're not super sure where it is exactly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ~dbeta(., 4, 6), geom = \"area\", fill = clrs[1])\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nHere's the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"moma_sample\", package = \"bayesrules\")\nhead(moma_sample)\n##                artist  country birth death alive  genx gender count\n## 1        Ad Gerritsen    dutch  1940  2015 FALSE FALSE   male     1\n## 2 Kirstine Roepstorff   danish  1972  <NA>  TRUE  TRUE female     3\n## 3    Lisa Baumgardner american  1958  2015 FALSE FALSE female     2\n## 4         David Bates american  1952  <NA>  TRUE FALSE   male     1\n## 5          Simon Levy american  1946  <NA>  TRUE FALSE   male     1\n## 6      Pierre Mercure canadian  1927  1966 FALSE FALSE   male     8\n##   year_acquired_min year_acquired_max\n## 1              1981              1981\n## 2              2005              2005\n## 3              2016              2016\n## 4              2001              2001\n## 5              2012              2012\n## 6              2008              2008\n```\n:::\n\n\nOnly 14 are Gen X:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoma_sample |> \n  count(genx)\n##    genx  n\n## 1 FALSE 86\n## 2  TRUE 14\n```\n:::\n\n\nThrough the magic of conjugate families, we can calculate the exact posterior:\n\n$$\n\\begin{aligned}\nY &\\sim \\operatorname{Binomial}(100, \\pi) \\\\\n\\pi &= \\operatorname{Beta}(4, 6)\n\\end{aligned}\n$$\n\nSince we observe $Y = 14$, then the actual exact posterior is\n\n$$\n\\pi \\mid (Y = 14) \\sim \\operatorname{Beta}(4 + 14, 6 + 100 - 14) \\rightarrow \\operatorname{Beta}(18, 92)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(aes(fill = \"Prior: Beta(4, 6)\"),\n                fun = ~dbeta(., 4, 6), geom = \"area\") +\n  stat_function(aes(fill = \"Posterior: Beta(18, 92)\"),\n                fun = ~dbeta(., 18, 92), geom = \"area\") +\n  scale_fill_manual(values = c(clrs[2], clrs[1]))\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nNeat! We have a posterior, but now we have to do something with it:\n\n- Estimation\n- Hypothesis testing\n- Prediction\n\nBut first, for fun, here are some MCMC-based approximations of the posterior:\n\n::: {.panel-tabset}\n### brms\n\n\n::: {.cell hash='08-chapter_cache/html/model-pi-brms_eaeca24d11ed780da39bdcb052e06487'}\n\n```{.r .cell-code}\nmodel_pi_brms <- brm(\n  bf(num_genx | trials(artworks) ~ 0 + Intercept),\n  data = list(num_genx = 14, artworks = 100),\n  family = binomial(link = \"identity\"),\n  prior(beta(4, 6), class = b, lb = 0, ub = 1),\n  sample_prior = TRUE,  # For calculating Bayes Ratios\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"cmdstanr\", cores = 4, refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.1 seconds.\n## Chain 2 finished in 0.1 seconds.\n## Chain 3 finished in 0.1 seconds.\n## Chain 4 finished in 0.1 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.1 seconds.\n## Total execution time: 0.2 seconds.\n\nmodel_pi_brms_prior_only <- brm(\n  bf(num_genx | trials(artworks) ~ 0 + Intercept),\n  data = list(num_genx = 14, artworks = 100),\n  family = binomial(link = \"identity\"),\n  prior(beta(4, 6), class = b, lb = 0, ub = 1),\n  sample_prior = \"only\",  # For calculating Bayes Ratios\n  iter = 5000, warmup = 1000, seed = BAYES_SEED,\n  backend = \"cmdstanr\", cores = 4, refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms\n##  Family: binomial \n##   Links: mu = identity \n## Formula: num_genx | trials(artworks) ~ 0 + Intercept \n##    Data: list(num_genx = 14, artworks = 100) (Number of observations: 1) \n##   Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.16      0.03     0.10     0.24 1.00     6534     6657\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\n\n### Stan\n\n**08-stan/genx.stan**\n\n\n::: {.cell file='08-stan/genx.stan' output.var=''}\n\n```{.stan .cell-code}\n// Things coming in from R\ndata {\n  int<lower=0> artworks;\n  int<lower=0> num_genx;\n}\n\n// Thing to estimate\nparameters {\n  real<lower=0, upper=1> pi;  // Proportion of Gen X artists\n}\n\n// Prior and likelihood\nmodel {\n  // Prior\n  pi ~ beta(4, 6);\n  \n  // Likelihood\n  num_genx ~ binomial(artworks, pi);\n}\n```\n:::\n\n::: {.cell hash='08-chapter_cache/html/compile-genx-model_266d0a5f4d682a0b013e748a2278007e'}\n\n```{.r .cell-code}\nmodel_pi_stan <- cmdstan_model(\"08-stan/genx.stan\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npi_stan_samples <- model_pi_stan$sample(\n  data = list(artworks = 100, num_genx = 14),\n  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n```\n:::\n\n\n:::\n\n\n# 8.1: Posterior estimation\n\nOur posterior $\\operatorname{Beta}(18, 92)$ is a complete distribution, but we often need to work with summaries of that distribution. The mean here is 16% ($\\frac{18}{18 + 92} = 0.1636$), meaning that it is most likely the case that 16% of MoMA artists are Gen X or younger, but it could be anywhere between 10-25ish%\n\nWe can calculate a 95% credible interval around the median using quantiles:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqbeta(c(0.025, 0.975), 18, 92)\n## [1] 0.1009084 0.2379286\n```\n:::\n\n\nThere's a 95% posterior probability that somewhere between 10% and 24$ of museum artists are Gen X or younger:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_mean <- 18 / (18 + 92)\npost_median <- qbeta(0.5, 18, 92)\npost_mode <- (18 - 1)/(18 + 92 - 2)\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.4)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                fill = clrs[3]) +\n  geom_vline(xintercept = post_mode) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWe don't have to use 95%; that's just arbitrary. We can use different levels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.9)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                aes(fill = \"95%\")) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.055, 0.945), 18, 92),\n                aes(fill = \"89%\")) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.1, 0.9), 18, 92),\n                aes(fill = \"80%\")) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.25, 0.75), 18, 92),\n                aes(fill = \"50%\")) +\n  geom_vline(xintercept = post_mode) +\n  scale_fill_manual(values = colorspace::lighten(clrs[3], c(0.1, 0.3, 0.5, 0.7))) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\", fill = \"Credible interval\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis posterior is a little lopsided, so we might want to make an interval that's not centered at the mode of π, but instead centered at the highest posterior density. \n\n::: {.panel-tabset}\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  median_hdci(b_Intercept, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##   b_Intercept .lower .upper .width .point .interval\n##         <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1       0.162 0.138   0.184   0.5  median hdci     \n## 2       0.162 0.106   0.215   0.89 median hdci     \n## 3       0.162 0.0944  0.230   0.95 median hdci\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  ggplot(aes(x = b_Intercept)) +\n  stat_slab(aes(fill_ramp = stat(level)),\n            .width = c(0.02, 0.5, 0.89, 0.95, 1),\n            point_interval = \"median_hdci\",\n            fill = clrs[3]) +\n  scale_fill_ramp_discrete(range = c(0.2, 1)) +\n  labs(fill_ramp = \"Credible interval\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi_stan_samples |> \n  spread_draws(pi) |> \n  median_hdci(pi, .width = c(0.5, 0.89, 0.95))\n## # A tibble: 3 × 6\n##      pi .lower .upper .width .point .interval\n##   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n## 1 0.161 0.137   0.182   0.5  median hdci     \n## 2 0.161 0.108   0.217   0.89 median hdci     \n## 3 0.161 0.0987  0.234   0.95 median hdci\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npi_stan_samples |> \n  spread_draws(pi) |> \n  ggplot(aes(x = pi)) +\n  stat_slab(aes(fill_ramp = stat(level)),\n            .width = c(0.02, 0.5, 0.89, 0.95, 1),\n            point_interval = \"median_hdci\",\n            fill = clrs[3]) +\n  scale_fill_ramp_discrete(range = c(0.2, 1)) +\n  labs(fill_ramp = \"Credible interval\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n\n# 8.2: Posterior hypothesis testing\n\nWhat if we read somewhere that fewer than 20% of museum artists are Gen X or younger? We can calculate the posterior probability of this scenario, or $P(\\pi < 0.2 \\mid Y = 14)$\n\nWith the exact posterior, that's super easy:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_prob <- pbeta(0.2, 18, 92)\npost_prob\n## [1] 0.8489856\n\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.4)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = c(0, 0.2),\n                fill = clrs[3]) +\n  geom_vline(xintercept = 0.2) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n85% of the distribution is below 0.2, so we can say there's an 85% chance that Gen X artists constitute 20% or fewer of modern art museum artists.\n\nThat's easy!\n\nHere it is with MCMC:\n\n::: {.panel-tabset}\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  count(b_Intercept < 0.2) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 2 × 3\n##   `b_Intercept < 0.2`     n  prob\n##   <lgl>               <int> <dbl>\n## 1 FALSE                2332 0.146\n## 2 TRUE                13668 0.854\n\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  ggplot(aes(x = b_Intercept)) +\n  stat_halfeye(aes(fill_ramp = stat(x < 0.2)), fill = clrs[3]) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[3], 0.4), guide = \"none\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n### Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi_stan_samples |> \n  spread_draws(pi) |> \n  count(pi < 0.2) |> \n  mutate(prob = n / sum(n))\n## # A tibble: 2 × 3\n##   `pi < 0.2`     n  prob\n##   <lgl>      <int> <dbl>\n## 1 FALSE       1445 0.144\n## 2 TRUE        8555 0.856\n\npi_stan_samples |> \n  spread_draws(pi) |> \n  ggplot(aes(x = pi)) +\n  stat_halfeye(aes(fill_ramp = stat(x < 0.2)), fill = clrs[3]) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[3], 0.4), guide = \"none\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## One-sided tests (probability of direction)\n\nWe can also use a hypothesis testing framework and present two competing hypotheses:\n\n$$\n\\begin{split}\nH_0: & \\; \\; \\pi \\ge 0.2 \\\\\nH_a: & \\; \\; \\pi < 0.2\n\\end{split}\n$$\n\nWe already know the probability of $H_a$ (0.849), so the probability of $H_0$ is 1 minus tuat, or 0.151. The posterior odds is the ratio of those two probabilities\n\n$$\n\\text{posterior odds} = \\frac{P(H_a \\mid Y = 14)}{P(H_0 \\mid Y = 14)} = \\frac{0.849}{0.151} \\approx 5.622\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_odds <- post_prob / (1 - post_prob)\npost_odds\n## [1] 5.621883\n```\n:::\n\n\n\nThat means that π is ≈6 times more likely to be below 20% than to be above 20%\n\nThat's all based on the posterior though. Back before we knew anything, we had a prior of $\\operatorname{Beta}(6, 4)$, an in that world, we had a 9% chance that it was true and a 91% chance that it was all false\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_prob <- pbeta(0.2, 4, 6)\nprior_prob\n## [1] 0.08564173\n1 - prior_prob\n## [1] 0.9143583\n```\n:::\n\n\nSo the prior odds were only 1 in 10:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_odds <- prior_prob / (1 - prior_prob)\nprior_odds\n## [1] 0.09366321\n```\n:::\n\n\nFinally, we can do something more useful with these prior and posterior odds and calculate the **Bayes Factor**, which is just their ratio:\n\n$$\n\\text{Bayes Factor} = \\frac{\\text{Posterior odds}}{\\text{Prior odds}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBF <- post_odds / prior_odds\nBF\n## [1] 60.02232\n```\n:::\n\n\nAfter learning about 14 Gen X artists, \"the posterior odds of our hypothesis … are roughly 60 times higher than the prior odds\", which is \"fairly convincing\"\n\nNo significance testing, no failing to reject nulls. Just vibes.\n\n`Evid.Ratio` here is the posterior probability of the hypothesis being true / posterior probability of the hypothesis not being true, or the same as `post_odds` above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh <- hypothesis(model_pi_brms, \"Intercept < 0.2\")\nh\n## Hypothesis Tests for class b:\n##              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n## 1 (Intercept)-(0.2) < 0    -0.04      0.03    -0.09     0.02       5.86\n##   Post.Prob Star\n## 1      0.85     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\nplot(h)\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nIf we want the same Bayes Factor ratio that *Bayes Rules!* calculates, we need to use the evidence ratio from brms and calculate `prior_odds` by hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_prob <- pbeta(0.2, 4, 6)\nprior_odds <- prior_prob / (1 - prior_prob)\n\npost_odds_brms <- h$hypothesis$Evid.Ratio\n\nBF_brms <- post_odds_brms / prior_odds\nBF_brms\n## [1] 62.57594\n```\n:::\n\n\n\n## Two-sided tests (ROPE stuff)\n\nWhat if we want to know whether or not 30% of museum artists are Gen X or younger, not just a direction? Now we're dealing with two sides:\n\n$$\n\\begin{split}\nH_0: & \\; \\; \\pi = 0.3 \\\\\nH_a: & \\; \\; \\pi \\ne 0.3 \\\\\n\\end{split}\n$$\n\nWe already know the 95% credible interval for π, and 0.3 doesn't really fit well in it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.9)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                fill = clrs[3]) +\n  geom_vline(xintercept = 0.3) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\", fill = \"Credible interval\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nThat provides us with good evidence that the hypothesis that 30% of artists are Gen X is not correct. It's subtantially outside of the credible interval. But what does substantial mean? We get to define that.\n\nWe can be like Kruschke and define a buffer around 0.3, or a region of practical equivalence (ROPE). Here we'll do 0.3±0.05, or between 0.25 and 0.35. We can calculate how much of the posterior is outside of that ROPE.\n\nSince we know the actual posterior is $\\operatorname{Beta}(18, 92)$, we can find the percentage of the area of the curve that falls in the ROPE with `pbeta()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop_in_rope <- pbeta(0.35, 18, 92) - pbeta(0.25, 18, 92)\nprop_in_rope\n## [1] 0.01250077\n1 - prop_in_rope\n## [1] 0.9874992\n```\n:::\n\n\n98.7% of the posterior is outside of that ROPE. I'd say a value of 30% is pretty substantially far away from the posterior and thus really unlikely.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\",\n                fill = colorspace::lighten(clrs[3], 0.9)) +\n  stat_function(fun = ~dbeta(., 18, 92), geom = \"area\", \n                xlim = qbeta(c(0.025, 0.975), 18, 92),\n                fill = clrs[3]) +\n  annotate(geom = \"rect\", xmin = 0.25, xmax = 0.35, ymin = -Inf, ymax = Inf, alpha = 0.3) +\n  geom_vline(xintercept = 0.3) +\n  xlim(c(0, 0.4)) +\n  labs(x = \"π\", fill = \"Credible interval\")\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nWe can do this with the MCMC draws too and we get the same results:\n\n::: {.panel-tabset}\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms |> \n  spread_draws(b_Intercept) |> \n  summarize(prop_in_rope = sum(b_Intercept > 0.25 & b_Intercept < 0.35) / n(),\n            prop_outside_rope = 1 - prop_in_rope)\n## # A tibble: 1 × 2\n##   prop_in_rope prop_outside_rope\n##          <dbl>             <dbl>\n## 1       0.0119             0.988\n```\n:::\n\n\n### Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi_stan_samples |> \n  spread_draws(pi) |> \n  summarize(prop_in_rope = sum(pi > 0.25 & pi < 0.35) / n(),\n            prop_outside_rope = 1 - prop_in_rope)\n## # A tibble: 1 × 2\n##   prop_in_rope prop_outside_rope\n##          <dbl>             <dbl>\n## 1       0.0109             0.989\n```\n:::\n\n\n:::\n\n\n# 8.3: Posterior prediction\n\n(This stuff is all covered [in my guide here](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/) too)\n\nWe get data for 20 more pieces of art at the museum. Based on what we know about π, how many would we predict would be by Gen X artists?\n\nIt's reasonable to think 3 (since 20 * 0.16 = 3), but that misses out on two levels of uncertainty:\n\n- Sampling variability in the data - even if π is truly 0.16, the amount we get in the sample will vary just because of randomness\n- Posterior variability in π - it could be anywhere between 0.1 and 0.24\n\nThe posterior predictive model takes both kinds of uncertainty into account\n\nThere's technically a mathy way to get at posterior predictions, and the book covers it, but it's a complicated mess and they even conclude by saying \"In this book, we’ll never need to do something like this again\"\n\nIn the book, the actual posterior predictive probability that 3 of the 20 new artists will be Gen X, based on a posterior that saw 14 (i.e. the model we created), is 0.2217.\n\nWe can approximate that exact 0.2217 with the MCMC draws too. With brms models we can use `posterior_predict()`, `posterior_linpred()`, and `posterior_epred()` to extract different types of posterior outcomes on different scales. With raw Stan output, we have to do a little more work ourselves.\n\n::: {.panel-tabset}\n\n### brms\n\nWe want to use `predicted_draws()` since that incorporates both kinds of uncertainty, and it returns values that are predicted counts, not probabilities or π ([see my guide for more](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/#tldr-diagrams-and-cheat-sheets))\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_genx_after_20 <- model_pi_brms |> \n  predicted_draws(newdata = tibble(artworks = 20)) |> \n  group_by(.prediction) |> \n  summarize(n = n()) |> \n  mutate(prop = n / sum(n))\npredicted_genx_after_20\n## # A tibble: 13 × 3\n##    .prediction     n      prop\n##          <int> <int>     <dbl>\n##  1           0   637 0.0398   \n##  2           1  1963 0.123    \n##  3           2  3297 0.206    \n##  4           3  3533 0.221    \n##  5           4  2904 0.182    \n##  6           5  1881 0.118    \n##  7           6  1097 0.0686   \n##  8           7   432 0.027    \n##  9           8   183 0.0114   \n## 10           9    52 0.00325  \n## 11          10    16 0.001    \n## 12          11     4 0.00025  \n## 13          14     1 0.0000625\n\nggplot(predicted_genx_after_20, aes(x = factor(.prediction), y = prop)) + \n  geom_col()\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Posterior predictive probability that 3/20 will be Gen X is roughly the same\n# as 0.2217!\npredicted_genx_after_20 |> \n  filter(.prediction == 3) |> \n  pull(prop)\n## [1] 0.2208125\n```\n:::\n\n\nWe can also get the variability in just π if we wanted by using `linpred_draws()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms |> \n  linpred_draws(newdata = tibble(artworks = 20)) |> \n  ungroup() |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye()\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nAnd if we use `epred_draws()`, we'll get the expected number of Gen X artworks:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pi_brms |> \n  epred_draws(newdata = tibble(artworks = 20)) |> \n  ungroup() |> \n  ggplot(aes(x = .epred)) +\n  stat_halfeye()\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nLovely.\n\n### Stan\n\nRaw Stan requires a little more work. We could theoretically use Stan to generate posterior predictions with a `generated quantities` block:\n\n``` stan\ngenerated quantities {\n  vector[1000] num_genx_rep;\n\n  for (i in 1:1000) {\n    num_genx_rep[i] = binomial_rng(20, pi);\n  }\n}\n```\n\nBut that requires either hard-coding two numbers into the Stan code: 1000 for the number of simulations and 20 for the number of new artworks. If we want to change any of those, we'd have to recompile, which is tedious.\n\nAlternatively, we could add a couple variables to the `data` block and pass them through R:\n\n``` stan\ndata {\n  // other variables\n  int<lower=1> n_sims;\n  int<lower=1> new_artworks;\n}\n\n// other blocks\n\ngenerated quantities {\n  vector[n_sims] num_genx_rep;\n\n  for (i in 1:n_sims) {\n    num_genx_rep[i] = binomial_rng(new_artworks, pi);\n  }\n}\n```\n\nWe'd then need to include values for those new variables in the list of data we pass to Stan:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi_stan_samples <- model_pi_stan$sample(\n  data = list(artworks = 100, num_genx = 14, new_artworks = 20, n_sims = 1000),\n  parallel_chains = 4, iter_warmup = 2500, iter_sampling = 2500, \n  refresh = 0, seed = BAYES_SEED\n)\n```\n:::\n\n\nThat would work great and the results from Stan would include 1000 predictions for the number of Gen X artists. But it feels a little excessive to keep rerunning the original 14-artworks model over and over for different numbers of new artworks.\n\nSo instead we can use R to build the posterior predictions, since we have all the posterior values of π in the MCMC chains, and since all we're really doing with Stan is using Stan's version of `rbinom()` anyway (`binomial_rng()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_genx_after_20_stan <- pi_stan_samples |> \n  spread_draws(pi) |> \n  mutate(.prediction = rbinom(n(), size = 20, prob = pi)) |> \n  group_by(.prediction) |> \n  summarize(n = n()) |> \n  mutate(prop = n / sum(n))\npredicted_genx_after_20_stan\n## # A tibble: 12 × 3\n##    .prediction     n   prop\n##          <int> <int>  <dbl>\n##  1           0   403 0.0403\n##  2           1  1214 0.121 \n##  3           2  2042 0.204 \n##  4           3  2250 0.225 \n##  5           4  1765 0.176 \n##  6           5  1215 0.122 \n##  7           6   642 0.0642\n##  8           7   291 0.0291\n##  9           8   116 0.0116\n## 10           9    44 0.0044\n## 11          10    13 0.0013\n## 12          11     5 0.0005\n\nggplot(predicted_genx_after_20_stan, aes(x = factor(.prediction), y = prop)) + \n  geom_col()\n```\n\n::: {.cell-output-display}\n![](08-chapter_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Posterior predictive probability that 3/20 will be Gen X is roughly the same\n# as 0.2217!\npredicted_genx_after_20_stan |> \n  filter(.prediction == 3) |> \n  pull(prop)\n## [1] 0.225\n```\n:::\n\n\n:::\n\nI haven't figured out a way to get `posterior_linpred()` and `posterior_epred()` with raw Stan like this though. :(\n\n\n# 8.4: Posterior analysis with MCMC\n\nOh ha, this whole section shows how to do everything above with Stan, but I already did that above with both brms and raw Stan, so just, um look up there ↑.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}