{
  "hash": "4c24095b8e7dca234e2db4a097e4c2de",
  "result": {
    "markdown": "---\ntitle: \"15: Hierarchical models are exciting\"\nsubtitle: \"Reading notes\"\ndate: \"November 1, 2022\"\n---\n\n\n[(Original chapter)](https://www.bayesrulesbook.com/chapter-15.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(rstanarm)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(parameters)\nlibrary(tidybayes)\nlibrary(patchwork)\nlibrary(scales)\n\n# tikz stuff\n# Necessary for using dvisvgm on macOS\n# See https://www.andrewheiss.com/blog/2021/08/27/tikz-knitr-html-svg-fun/\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n\nfont_opts <- list(dvisvgm.opts = \"--font-format=woff\")\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Tell bayesplot to use the Lakota palette for things like pp_check()\n# bayesplot::color_scheme_set(clrs)\n\n# Tell bayesplot to use the viridis rocket palette for things like pp_check()\nviridisLite::viridis(6, option = \"rocket\", end = 0.85, direction = -1) |> \n  # Take off the trailing \"FF\" in the hex codes\n  map_chr(~str_sub(., 1, 7)) |> \n  bayesplot::color_scheme_set()\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\n\n# Data\ndata(cherry_blossom_sample, package = \"bayesrules\")\n\nrunning <- cherry_blossom_sample %>% \n  select(runner, age, net) |> \n  mutate(runner_nice = glue::glue(\"Runner {runner}\"),\n         runner_nice = fct_inorder(runner_nice))\n```\n:::\n\n\n# The general setup\n\nThis `running` data has the race times for 36 different runners for a 10-mile in race that they've run for multiple years\n\nIn the book, Figure 15.1 is a boxplot showing each runner's race time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunning |> \n  ggplot(aes(x = runner, y = net)) +\n  geom_boxplot() +\n  labs(x = \"Runner number\", y = \"Race time\")\n```\n\n::: {.cell-output-display}\n![](15-chapter_files/figure-html/unnamed-chunk-2-1.png){width=864}\n:::\n:::\n\n\nThere's variation across runners (e.g., runner 4 is consistently slower than runners 5 and 6), and there's variation within runners (e.g., runner 6 has had roughly the same time every race; runner 17 ranges from 80ish to 120ish)\n\nWe can also see this if we look at patterns across age within each runner. People like runner 29 are consistently fast over time; runner 17 bounces around a lot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunning |> \n  ggplot(aes(x = age, y = net)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1, color = clrs[4]) +\n  facet_wrap(vars(runner_nice), ncol = 4) +\n  labs(x = \"Age\", y = \"Race time\") +\n  theme(panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![](15-chapter_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWhen analyzing this kind of data, we have to do special stuff with it to incorporate our understanding of these different structures (i.e. repeated runners across time). Before getting into hierarchical models (the point of this unit), it's helpful to look at two extreme ways of handling the data: complete pooling and no pooling.\n\n# 15.1: Complete pooling\n\nWith complete pooling we lump all the observations together into one pool of information and basically treat each row as independent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunning |> \n  ggplot(aes(x = age, y = net)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = clrs[4]) +\n  labs(x = \"Age\", y = \"Race time\")\n```\n\n::: {.cell-output-display}\n![](15-chapter_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWe can then model this with a normal regression model with vague priors:\n\n$$\n\\begin{aligned}\n\\text{Race time}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\, \\text{Age}_i \\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(0, 35) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/10)\n\\end{aligned}\n$$\n\n::: {.panel-tabset}\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 35), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(0.1), class = sigma))\n\nmodel_complete_pooling_brms <- brm(\n  bf(net ~ age),\n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"15-manual_cache/complete-pooling-brms\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(model_complete_pooling_brms, effects = \"fixed\", conf.level = 0.8)\n## # A tibble: 2 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   <chr>  <chr>     <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed  cond      (Intercept)   75.2      24.6     43.1     107.   \n## 2 fixed  cond      age            0.267     0.447   -0.306     0.850\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Put the results in a list\nm_pool_complete <- model_complete_pooling_brms |> \n  tidy(effects = \"fixed\", conf.level = 0.8) |> \n  mutate(term = janitor::make_clean_names(term)) |> \n  split(~term)\n```\n:::\n\n\n\n### rstanarm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_complete_pooling_rstanarm <- stan_glm(\n  net ~ age,\n  data = running,\n  family = gaussian,\n  prior_intercept = normal(0, 35),\n  prior = normal(0, 2.5),\n  prior_aux = exponential(0.1),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(model_complete_pooling_rstanarm, conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 2 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)   75.4      24.3     44.5     107.   \n## 2 age            0.263     0.439   -0.309     0.826\n```\n:::\n\n\n:::\n\nBased on this, a one-year increase in age is associated with a race time that is 0.27 minutes longer, on average, but with an 80% credible interval of -0.31 to 0.85, so essentially there's no effect. But that's likely not the case—we would expect most people to get slower over time. And that is indeed the case if we look at the age trends for each runner. The pooled line is basically flat while the individual lines all have (generally upward) slopes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_complete_pooling_brms |> \n  add_epred_draws(newdata = tibble(age = seq(min(running$age), \n                                             max(running$age), \n                                             length.out = 100))) |> \n  ggplot(aes(x = age, y = .epred)) +\n  geom_smooth(data = running,\n              aes(y = net, group = runner), method = \"lm\", se = FALSE, \n              color = \"grey60\", size = 0.5) +\n  stat_lineribbon(alpha = 0.25, fill = clrs[4], color = clrs[4]) +\n  labs(x = \"Age\", y = \"Race time\")\n```\n\n::: {.cell-output-display}\n![](15-chapter_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nlol this model isn't great. It doesn't keep any information about participants and treats every row as its own independent chunk of the population:\n\n\n::: {.cell layout-align=\"center\" engine.opts='{\"dvisvgm.opts\":\"--font-format=woff\"}'}\n::: {.cell-output-display}\n![Complete pooling](15-chapter_files/figure-html/complete-pooling-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n# 15.2: No pooling\n\nInstead of lumping all the runners together, we can keep *complete* information about each runner by not lumping anything together at all. Note how here there's no node for \"Population\" here like before—with the no pooling approach, we treat each runner as the complete population for that person:\n\n\n::: {.cell layout-align=\"center\" engine.opts='{\"dvisvgm.opts\":\"--font-format=woff\"}'}\n::: {.cell-output-display}\n![No pooling](15-chapter_files/figure-html/no-pooling-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nMore formally, this entails running the linear model for each runner $j$ in their $i$th race:\n\n$$\n\\begin{aligned}\n\\text{Race time}_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma) \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j}\\, \\text{Age}_{i_j} \\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(0, 35) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2.5) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/10)\n\\end{aligned}\n$$\n\nThere's some mathematical trickery we can do with interaction terms to just fit one model and get unique slopes and intercepts for each runner (`net ~ age + runner + age*runner`), but practically speaking, this no pooled approach entails running a single model for each runner. \n\nSo for fun and excitement and learning, we'll do that here! The underlying Stan code for `model_complete_pooling_brms` is the same as what we need for this no pooling model, so we'll just re-run that model over and over (with `update()`) with subsets of the data—one subset per runner.\n\n\n::: {.cell hash='15-chapter_cache/html/no-pooling-brms_3ecdea457aa6b529ccc82894dae66785'}\n\n```{.r .cell-code}\nno_pooling <- tibble(runner = levels(running$runner)) |> \n  mutate(data = map(runner, ~filter(running, runner == .x))) |> \n  mutate(model = map(data, ~update(model_complete_pooling_brms, \n                                   newdata = .x)))\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## Warning: 1 of 8000 (0.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## Warning: 5 of 8000 (0.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n## The desired updates require recompiling the model\n## Start sampling\n```\n:::\n\n\nNow each runner has their own intercept and slope!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_pooling_results <- no_pooling |> \n  mutate(tidied = map(model, ~tidy(., effects = \"fixed\"))) |> \n  select(-data, -model) |> \n  unnest(tidied) |> \n  filter(term %in% c(\"(Intercept)\", \"age\")) |> \n  select(runner, term, estimate) |> \n  pivot_wider(names_from = \"term\", values_from = \"estimate\") |> \n  rename(beta0 = `(Intercept)`, beta1 = age)\n\nno_pooling_results\n## # A tibble: 36 × 3\n##    runner  beta0   beta1\n##    <chr>   <dbl>   <dbl>\n##  1 1      154.   -1.42  \n##  2 2       40.7   0.791 \n##  3 3       86.4   0.0585\n##  4 4       56.4   0.831 \n##  5 5        5.52  1.31  \n##  6 6       24.4   0.903 \n##  7 7       40.7   1.14  \n##  8 8        2.68  1.84  \n##  9 9       23.1   1.32  \n## 10 10      32.4   1.47  \n## # … with 26 more rows\n```\n:::\n\n\nWe can even plot these runner-specific models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunner_ages <- running |> \n  group_by(runner) |> \n  summarize(age_min = min(age), age_max = max(age)) |> \n  mutate(age_range = map2(age_min, age_max, ~tibble(age = seq(.x, .y, length.out = 100)))) |> \n  select(runner, age_range)\n\nno_pooling_epreds <- no_pooling |> \n  left_join(runner_ages, by = \"runner\") |> \n  mutate(epred = map2(model, age_range, ~epred_draws(.x, .y, ndraws = 100)))\n\nno_pooling_plots <- no_pooling_epreds |> \n  mutate(plot = map2(epred, data, ~{\n    ggplot(.x, aes(x = age, y = .epred)) +\n      geom_line(aes(group = .draw), size = 0.25, color = clrs[4], alpha = 0.5) +\n      geom_point(data = .y, aes(y = net), size = 0.75, color = clrs[6]) +\n      labs(x = NULL, y = NULL) +\n      coord_cartesian(xlim = c(50, 64), ylim = c(60, 132)) +\n      theme(panel.grid.minor = element_blank()) +\n      facet_wrap(vars(runner_nice))\n  }))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrap_plots(no_pooling_plots$plot, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](15-chapter_files/figure-html/no-pooling-plots-1.png){width=864}\n:::\n:::\n\n\n\nBut that's not really all that helpful at all. If we want to predict the race time for a new runner who is 58 years old (or any age), we can't! As *Bayes Rules!* says:\n\n> Since they’re tailored to the 36 individuals in our sample, the resulting 36 models don’t reliably extend beyond these individuals.\n\nThese individual models can't \"communicate\" with each other and are too isolated and unwieldy.\n\n\n# 15.3 & 15.4: Partial pooling with hierarchical models\n\nThis data, however, has a natural hierarchical structure to it. Within our population, we have observations that are grouped by runner. There is variation within each runner, and variation across or between each runner. According to *Bayes Rules!*, we have:\n\n> 1. **Within-group variability**: The degree of the variability among multiple observations within each group can be interesting on its own. For example, we can examine how consistent an individual’s running times are from year to year.\n> 2. **Between-group variability**: Hierarchical data also allows us to examine the variability from group to group. For example, we can examine the degree to which running patterns vary from individual to individual.\n\nWe can visualize the structure like this:\n\n\n::: {.cell layout-align=\"center\" engine.opts='{\"dvisvgm.opts\":\"--font-format=woff\"}'}\n::: {.cell-output-display}\n![Partial pooling with hierarchical models](15-chapter_files/figure-html/partial-pooling-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nAnd this chapter doesn't cover how to run a model with this structure, but we'll do it here for fun:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(0, 35), class = Intercept),\n            prior(normal(0, 2.5), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(student_t(3, 0, 15), class = sd, lb = 0))\n\nmodel_partial_pooling_brms <- brm(\n  bf(net ~ age + (1 + age | runner)),\n  data = running,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 8000, threads = threading(2), seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"15-manual_cache/partial-pooling-brms\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(model_partial_pooling_brms)\n## # A tibble: 6 × 8\n##   effect   component group    term               estim…¹ std.e…² conf.…³ conf.…⁴\n##   <chr>    <chr>     <chr>    <chr>                <dbl>   <dbl>   <dbl>   <dbl>\n## 1 fixed    cond      <NA>     (Intercept)         22.1    12.1   -1.51    46.0  \n## 2 fixed    cond      <NA>     age                  1.23    0.222  0.790    1.67 \n## 3 ran_pars cond      runner   sd__(Intercept)      8.24    6.11   0.334   23.0  \n## 4 ran_pars cond      runner   sd__age              0.269   0.117  0.0463   0.549\n## 5 ran_pars cond      runner   cor__(Intercept).…  -0.273   0.561 -0.972    0.890\n## 6 ran_pars cond      Residual sd__Observation      5.18    0.305  4.62     5.81 \n## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​conf.low,\n## #   ⁴​conf.high\n```\n:::\n\n\nWe'll get into how to interpret all these population-level (\"fixed effects\") and group-level (\"random effects\") things in future chapter. Or [look at this guide of mine here](https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/).\n\nIn the meantime, for fun, here's what this all looks like when plotted:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_partial_pooling_brms |> \n  epred_draws(newdata = unnest(runner_ages, age_range),\n              ndraws = 100,\n              re_formula = NULL) |> \n  ggplot(aes(x = age, y = .epred)) +\n  geom_line(aes(group = paste(runner, .draw)),\n            size = 0.1, color = clrs[4], alpha = 0.5) +\n  geom_point(data = running, aes(y = net), size = 0.75, color = clrs[6]) +\n  labs(x = \"Age\", y = \"Race time\") +\n  theme(panel.grid.minor = element_blank()) +\n  facet_wrap(vars(runner), ncol = 4,\n             labeller = as_labeller(function(x) glue::glue(\"Runner {x}\")))\n```\n\n::: {.cell-output-display}\n![](15-chapter_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}