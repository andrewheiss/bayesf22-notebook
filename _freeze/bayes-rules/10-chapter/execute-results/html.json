{
  "hash": "3f206b7902daa6b0cf0c4fbe284caf3c",
  "result": {
    "markdown": "---\ntitle: \"Reading notes\"\nsubtitle: \"Evaluating regression models\"\ndate: \"October 4, 2022\"\n---\n\n\n[(Original chapter)](https://www.bayesrulesbook.com/chapter-10.html)\n\n$$\n\\require{mathtools}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(patchwork)\nlibrary(modelr)\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\n\ndata(bikes, package = \"bayesrules\")\n\nbikes <- bikes |> \n  mutate(temp_feel_centered = scale(temp_feel, scale = FALSE),\n         temp_feel_c = as.numeric(temp_feel_centered))\n\ntemp_details <- attributes(bikes$temp_feel_centered) %>%\n  set_names(janitor::make_clean_names(names(.)))\n```\n:::\n\n\n# The setup\n\nBack to the model from chapter 9, modeling Capital Bikeshare rides based on daily temperatures:\n\n$$\n\\begin{aligned}\nY_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\text{, or}  & \\text{[McElreath's syntax]} \\\\\nY_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(\\mu_i, \\sigma^2) & \\text{[Bayes Rules!'s syntax]}\n\\\\\n\\mu_i &= \\beta_{0c} + \\beta_1 X_i \\\\\n\\\\\n\\beta_{0c} &\\sim \\mathcal{N}(5000, 1000) \\\\\n\\beta_{1} &\\sim \\mathcal{N}(100, 40) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1 / 1250)\n\\end{aligned}\n$$\n\n::: {.panel-tabset}\n### rstanarm\n\n\n::: {.cell hash='10-chapter_cache/html/model-bike-rstanarm_9f35b20d338f712aac0d77b021e89347'}\n\n```{.r .cell-code}\nbike_rstanarm <- stan_glm(\n  rides ~ temp_feel_c,\n  data = bikes,\n  family = gaussian(),\n  prior_intercept = normal(5000, 1000),\n  prior = normal(100, 40),\n  prior_aux = exponential(0.0008),\n  chains = 4, iter = 5000*2, seed = 84735, refresh = 0\n)\n```\n:::\n\n\n### brms\n\n\n::: {.cell hash='10-chapter_cache/html/model-bike-brms_33b99ce2f5a317cff49ad45eb6ec3caf'}\n\n```{.r .cell-code}\npriors <- c(prior(normal(5000, 1000), class = Intercept),\n            prior(normal(100, 40), class = b, coef = \"temp_feel_c\"),\n            prior(exponential(0.0008), class = sigma))\n\nbike_brms <- brm(\n  bf(rides ~ temp_feel_c),\n  data = bikes,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0\n)\n## Start sampling\n```\n:::\n\n\n### Stan\n\n\n::: {.cell file='10-stan/bike-better.stan' output.var='' filename='10-stan/bike-better.stan'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 0> n;\n  vector[n] Y;\n  vector[n] X;\n}\n\nparameters {\n  real beta0;\n  real beta1;\n  real<lower = 0> sigma;\n}\n\ntransformed parameters {\n  vector[n] mu;\n  mu = beta0 + beta1 * X;\n}\n\nmodel {\n  Y ~ normal(mu, sigma);\n  \n  beta0 ~ normal(5000, 1000);\n  beta1 ~ normal(100, 40);\n  sigma ~ exponential(0.0008);\n}\n\ngenerated quantities {\n  vector[n] Y_rep;\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n    Y_rep[i] = normal_rng(mu[i], sigma);\n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_stan_better <- cmdstan_model(\"10-stan/bike-better.stan\")\n```\n:::\n\n::: {.cell hash='10-chapter_cache/html/stan-bike-model_2a4266ede412a7e7b4371a3953eadc88'}\n\n```{.r .cell-code}\nbike_stan_samples <- bike_stan_better$sample(\n  data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel_c),\n  parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n  refresh = 0, seed = BAYES_SEED\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 2.5 seconds.\n## Chain 2 finished in 2.5 seconds.\n## Chain 3 finished in 2.5 seconds.\n## Chain 4 finished in 2.5 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 2.5 seconds.\n## Total execution time: 2.6 seconds.\n```\n:::\n\n\n:::\n\nResults:\n\n::: {.panel-tabset}\n### rstanarm\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(bike_rstanarm, effects = c(\"fixed\", \"aux\"), \n     conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 4 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)   3487.      58.0    3413.     3561. \n## 2 temp_feel_c     82.2      5.08     75.7      88.7\n## 3 sigma         1282.      40.9    1231.     1336. \n## 4 mean_PPD      3487.      82.0    3382.     3593.\n```\n:::\n\n\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(bike_brms, conf.int = TRUE, conf.level = 0.8) |> \n  select(-c(effect, component, group))\n## # A tibble: 3 × 5\n##   term            estimate std.error conf.low conf.high\n##   <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)       3487.      57.5    3414.     3561. \n## 2 temp_feel_c         82.1      5.08     75.6      88.6\n## 3 sd__Observation   1283.      40.3    1232.     1336.\n```\n:::\n\n\n### Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_stan_samples$print(variables = c(\"beta0\", \"beta1\", \"sigma\"), \n                        \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.1, 0.9)))\n##  variable    mean  median    sd     10%     90%\n##     beta0 3487.16 3486.85 57.13 3414.32 3560.02\n##     beta1   82.12   82.10  5.06   75.68   88.58\n##     sigma 1282.72 1281.84 40.44 1231.39 1335.31\n```\n:::\n\n:::\n\n\n# 10.2: How wrong is the model?\n\nThe model is definitely wrong—we just care about how wrong it is. There are three assumptions in this model:\n\n\n$$\n\\begin{aligned}\nY_i \\mid \\beta_0, \\beta_1, \\sigma &\\overbrace{\\stackrel{\\text{ind}}{\\sim}}^{\\mathclap{\\text{Assumption 1}}} \\underbrace{\\mathcal{N}(\\mu_i, \\sigma^2)}_{\\text{Assumption 3}}\\\\\n\\mu_i &= \\underbrace{\\beta_{0c} + \\beta_1 X_i}_{\\text{Assumption 2}}\n\\end{aligned}\n$$\n\n1. When conditioned on $X$, the observed data $Y_i$ for each case $i$ is independent of the observed data for any other case (like $Y_j$, $Y_k$, etc)\n2. The typical $Y$ outcome can be written as a linear function of $X$, or $\\mu = \\beta_0 + \\beta_1 X$\n3. At any $X$ value, $Y$ varies normally around $\\mu$ with a consistent variability of $\\sigma$\n\nWe can use statsy tests for assumption 1, but we don't really need to—we can use logic instead. By itself, ridership count $Y$ is highly correlated over time, but after controlling for $X$, it's likely that the autocorrelation with $Y$ is cancelled out or controlled away. We're probably reasonably okay.\n\nFor assumptions 2 and 3, we can use a scatterplot and see if (1) it looks linear and (2) it looks normally distributed around the line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bikes, aes(x = temp_feel, y = rides)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nLooks linear to me.\n\nBut just eyeballing it isn't great. We can test the assumptions more formally with a posterior predictive check:\n\n> If the combined model assumptions are reasonable, then our posterior model should be able to simulate ridership data that's similar to the original data.\n\nWe can check that by using the $\\beta_0$, $\\beta_1$, and $\\sigma$ parameters from the chains to generate predictions for each of the MCMC draws. Here's how that works behind the scenes:\n\nWe have 20,000 sets of intercepts, slopes, and sigmas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_brms |> \n  spread_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  select(starts_with(\"b_\"), sigma)\n## # A tibble: 20,000 × 3\n##    b_Intercept b_temp_feel_c sigma\n##          <dbl>         <dbl> <dbl>\n##  1       3457.          80.5 1281.\n##  2       3525.          80.3 1240.\n##  3       3470.          80.1 1259.\n##  4       3500.          80.3 1215 \n##  5       3521.          80.7 1336.\n##  6       3569.          86.7 1272.\n##  7       3562.          87.6 1290.\n##  8       3412.          76.7 1271.\n##  9       3475.          87.9 1329.\n## 10       3490.          74.2 1246.\n## # … with 19,990 more rows\n```\n:::\n\n\nLet's take just the first draw and plug the original dataset into that model to calculate $\\mu$, then draw from a random normal distribution using $\\mu$ and $\\sigma$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_draw <- bike_brms |> \n  spread_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  slice(1)\n\none_rep <- bikes |> \n  mutate(mu = first_draw$b_Intercept + (first_draw$b_temp_feel_c * temp_feel_c),\n         y_rep = rnorm(500, mu, first_draw$sigma))\n\none_rep |> \n  select(temp_feel, temp_feel_c, rides, y_rep) |> \n  head(5)\n##   temp_feel temp_feel_c rides     y_rep\n## 1  64.72625   -4.417453   654  2885.575\n## 2  49.04645  -20.097253  1229  2652.113\n## 3  51.09098  -18.052723  1454  2775.513\n## 4  52.63430  -16.509403  1518 -1045.352\n## 5  50.79551  -18.348193  1362  1839.174\n```\n:::\n\n\nAnd we can plot the two distributions to compare the model to the actual data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(one_rep, aes(x = y_rep)) +\n  geom_density(color = \"lightblue\") + \n  geom_density(aes(x = rides), color = \"darkblue\")\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nChoose a bunch of posterior draws, plug them in, and you've got a homegrown `pp_check()`!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlotsa_draws <- bike_brms |> \n  spread_draws(b_Intercept, b_temp_feel_c, sigma) |> \n  slice_sample(n = 25) |> \n  mutate(id = 1:n())\n\nlotsa_reps <- lotsa_draws |> \n  mutate(mu = map2(b_Intercept, b_temp_feel_c, ~.x + .y * bikes$temp_feel_c),\n         y_rep = map2(mu, sigma, ~rnorm(500, .x, .y))) |> \n  unnest(y_rep)\n\nggplot(lotsa_reps, aes(x = y_rep)) +\n  geom_density(aes(group = id), color = \"lightblue\", size = 0.25) + \n  geom_density(data = bikes, aes(x = rides), \n               color = \"darkblue\", size = 1)\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAnd here's `pp_check()` for comparison:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(bike_brms, type = \"dens_overlay\", ndraws = 25)\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nHeck yes.\n\nThe plot looks okay-ish. It picks up the average and the general range, but it doesn't pick up the bimodality in ridership.\n\n\n# 10.3: How accurate are the posterior predictive models?\n\nA good model should be able to accurately predict new values of $Y. Here are 3 ways to evaluate the quality of predictions:\n\n## 10.3.1: Posterior predictive summaries\n\nHow well does the model predict the data we used to build the model? We can check the fit for October 22, 2012 for fun:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noctober22 <- bikes |> \n  filter(date == \"2012-10-22\") |> \n  select(temp_feel, rides)\noctober22\n##   temp_feel rides\n## 1  75.46478  6228\n```\n:::\n\n\nThe temperature that day was 75˚, so let's simulate the posterior predictive distribution for a day that's 75˚:\n\n::: {.panel-tabset}\n### rstanarm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_rstanarm |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = october22$rides, color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = october22$rides, color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_75_stan <- bike_stan_samples |> \n  spread_draws(beta0, beta1, sigma) |> \n  mutate(mu = beta0 + (beta1 * (75 - temp_details$scaled_center)),\n         y_new = rnorm(n(), mu, sigma))\n\npredict_75_stan |> \n  ggplot(aes(x = y_new)) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = october22$rides, color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n\nThat orange line is kind of far out there—is that a problem? We can calculate the prediction error, or the distance between the observed $Y$ and the posterior predictive mean $Y'$, or $Y - Y'$. In this case we under-predicted rids. There were 6,228 actual rides; the model only predicted 4,000ish:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ungroup() |> \n  summarize(actual_rides = october22$rides, \n            avg_prediction = mean(.prediction),\n            error = october22$rides - avg_prediction)\n## # A tibble: 1 × 3\n##   actual_rides avg_prediction error\n##          <int>          <dbl> <dbl>\n## 1         6228          3964. 2264.\n```\n:::\n\n\nWe can also think about the relative distance or error by dividing by the standard deviation. In this standardized scale, values beyond 2 or 3 standard deviations are pretty far off. Here we're 1.76 standard deviations off from the mean, which is fine I guess:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_brms |> \n  predicted_draws(newdata = tibble(temp_feel_c = 75 - temp_details$scaled_center)) |> \n  ungroup() |> \n  summarize(actual_rides = october22$rides, \n            avg_prediction = mean(.prediction),\n            error = october22$rides - avg_prediction,\n            error_scaled = error / sd(.prediction))\n## # A tibble: 1 × 4\n##   actual_rides avg_prediction error error_scaled\n##          <int>          <dbl> <dbl>        <dbl>\n## 1         6228          3963. 2265.         1.77\n```\n:::\n\n\nPretty much every observed point falls within the 95% prediction interval, but that range is big (range of ≈4000 rides!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_brms |> \n  add_predicted_draws(newdata = bikes) |> \n  ungroup() |> \n  mutate(error = rides - .prediction) |> \n  summarize(mae = median(abs(error)),\n            mae_scaled = median(abs(error / sd(.prediction))))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1 1251.      0.792\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_brms |> \n  add_predicted_draws(newdata = bikes, ndraws = 100) |> \n  ggplot(aes(x = temp_feel)) +\n  stat_interval(aes(y = .prediction, color_ramp = stat(level)), alpha = 0.25,\n                .width = c(0.5, 0.89, 0.95), color = clrs[3]) +\n  geom_point(aes(y = rides), size = 2, pch = 21, color = \"white\", fill = \"black\")\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nWe can get more official measures with `bayesrules::prediction_summary()`, but it only works with rstanarm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_rstanarm |> prediction_summary(data = bikes)\n##        mae mae_scaled within_50 within_95\n## 1 991.2531  0.7658091     0.434     0.968\n```\n:::\n\n\n## 10.3.2: Cross validation\n\nWe can use cross validation with $k$ folds: hold out 50 rows for testing, use 450 rows for training, to that a bunch of times with different holdout rows. Leave-one-out CV builds the model with 499 rows, holding just one out as the test data.\n\n::: {.panel-tabset}\n### rstanarm\n\n\n::: {.cell hash='10-chapter_cache/html/cv-rstanarm_34f87bd7242d31a09015f649d7b641b9'}\n\n```{.r .cell-code}\nset.seed(84735)\ncv_procedure <- prediction_summary_cv(\n  model = bike_rstanarm, data = bikes, k = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_procedure$folds\n##    fold       mae mae_scaled within_50 within_95\n## 1     1  990.2897  0.7710331      0.46      0.98\n## 2     2  965.8851  0.7434397      0.42      1.00\n## 3     3  950.3186  0.7298511      0.42      0.98\n## 4     4 1018.1896  0.7905312      0.46      0.98\n## 5     5 1161.8012  0.9085403      0.36      0.96\n## 6     6  937.3706  0.7322373      0.46      0.94\n## 7     7 1269.8300  1.0054441      0.32      0.96\n## 8     8 1111.8921  0.8606423      0.36      1.00\n## 9     9 1098.8982  0.8676561      0.40      0.92\n## 10   10  786.3265  0.6053804      0.56      0.96\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_procedure$cv\n##       mae mae_scaled within_50 within_95\n## 1 1029.08  0.8014756     0.422     0.968\n```\n:::\n\n\n### brms\n\n\n::: {.cell hash='10-chapter_cache/html/cv-brms_e474670ad0471709f9c9af405a5264ee'}\n\n```{.r .cell-code}\nkfold_brms <- bikes |> \n  crossv_kfold(k = 10) |> \n  mutate(model = map(train, ~{\n    brm(\n      bf(rides ~ temp_feel_c),\n      data = .,\n      family = gaussian(),\n      prior = priors,\n      chains = 4, iter = 5000*2, seed = BAYES_SEED, \n      backend = \"cmdstanr\", refresh = 0\n    )\n  }))\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n## Start sampling\n```\n:::\n\n::: {.cell hash='10-chapter_cache/html/cv-brms-stats_90401fa04d3601583257f2026ab83395'}\n\n```{.r .cell-code}\nmae_kfold_brms <- kfold_brms |> \n  mutate(predictions = map2(test, model, ~{\n    .x |> \n      as_tibble() |> \n      add_predicted_draws(.y) |> \n      ungroup()\n  })) |> \n  mutate(mae = map(predictions, ~{\n    .x |> \n      mutate(error = rides - .prediction) |> \n      summarize(mae = median(abs(error)),\n                mae_scaled = median(abs(error / sd(.prediction))))\n  }))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_kfold_brms |> \n  select(mae) |> \n  unnest(mae)\n## # A tibble: 10 × 2\n##      mae mae_scaled\n##    <dbl>      <dbl>\n##  1 1247.      0.783\n##  2 1249.      0.817\n##  3 1190.      0.753\n##  4 1230.      0.745\n##  5 1254.      0.811\n##  6 1322.      0.833\n##  7 1309.      0.802\n##  8 1298.      0.834\n##  9 1180.      0.747\n## 10 1274.      0.837\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_kfold_brms |> \n  select(mae) |> \n  unnest(mae) |> \n  summarize(across(everything(), mean))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1 1255.      0.796\n```\n:::\n\n\n### Stan\n\n\n::: {.cell hash='10-chapter_cache/html/cv-stan_ccb93affe5563ea03e8646f7e1d1d401'}\n\n```{.r .cell-code}\nkfold_stan <- bikes |> \n  crossv_kfold(k = 10) |> \n  mutate(model = map(train, ~{\n    # Stan likes working with data frames, not whatever the kfold split is\n    df <- as_tibble(.)\n    \n    bike_stan_better$sample(\n      data = list(n = nrow(df), Y = df$rides, X = df$temp_feel_c),\n      parallel_chains = 4, iter_warmup = 5000, iter_sampling = 5000, \n      refresh = 0, seed = BAYES_SEED\n    )\n  }))\n```\n:::\n\n\nPhew this is complex and creates a massively huge R object (like multiple GBs when trying to write it as RDS), and it makes Quarto choke when rendering, so I don't actually run it here. Here's the code though—it works when running interactively in RStudio (but takes a long time still).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_kfold_stan <- kfold_stan |> \n  mutate(predictions = map2(test, model, ~{\n    df <- .x |> as_tibble()\n    \n    draws <- .y |> \n      spread_draws(beta0, beta1, sigma)\n    \n    df |> \n      mutate(mu = map(temp_feel_c, ~draws$beta0 + draws$beta1 * .),\n             yrep = map(mu, ~rnorm(length(.), ., draws$sigma))) |> \n      unnest(yrep)\n  })) |> \n  mutate(mae = map(predictions, ~{\n    .x |> \n      mutate(error = rides - yrep) |> \n      summarize(mae = median(abs(error)),\n                mae_scaled = median(abs(error / sd(yrep))))\n  }))\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_kfold_stan |> \n  select(mae) |> \n  unnest(mae)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is the saved RDS object that contains mae_kfold_stan |> select(mae) |> unnest(mae)\nmae_kfold_stan_summary\n## # A tibble: 10 × 2\n##      mae mae_scaled\n##    <dbl>      <dbl>\n##  1 1386.      0.896\n##  2 1202.      0.786\n##  3 1246.      0.779\n##  4 1211.      0.736\n##  5 1238.      0.769\n##  6 1266.      0.803\n##  7 1306.      0.843\n##  8 1277.      0.823\n##  9 1207.      0.756\n## 10 1225.      0.787\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_kfold_stan |> \n  select(mae) |> \n  unnest(mae) |> \n  summarize(across(everything(), mean))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_kfold_stan_summary |> \n  summarize(across(everything(), mean))\n## # A tibble: 1 × 2\n##     mae mae_scaled\n##   <dbl>      <dbl>\n## 1 1256.      0.798\n```\n:::\n\n\n:::\n\n## 10.3.3: Expected log-predictive density (ELPD)\n\nThe intuition behind the expected log-predictive density (or ELPD) is that we want higher values of the pdf at observed data point within the posterior for each prediction. Like, for instance, say we have these two pdfs for two predicted values, with the actual value of $Y$ marked at the line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot() +\n  geom_function(fun = ~dnorm(., 0, 1)) +\n  annotate(geom = \"segment\", x = 0.2, xend = 0.2, y = -Inf, yend = dnorm(0.2, 0, 1),\n           linetype = 21) +\n  annotate(geom = \"label\", x = 0.2, y = 0.01, label = \"Actual y\", fontface = \"bold\") +\n  xlim(c(-4, 4)) +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  labs(x = \"Posterior predictive distribution for a single Y′\", y = \"Density\",\n       title = \"Scenario 1\")\n\np2 <- ggplot() +\n  geom_function(fun = ~dnorm(., 0, 1)) +\n  annotate(geom = \"segment\", x = 1.8, xend = 1.8, y = -Inf, yend = dnorm(1.8, 0, 1),\n           linetype = 21) +\n  annotate(geom = \"label\", x = 1.8, y = 0.01, label = \"Actual y\", fontface = \"bold\") +\n  xlim(c(-4, 4)) +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  labs(x = \"Posterior predictive distribution for a single Y′\", y = \"Density\",\n       title = \"Scenario 2\")\n\np1 | p2\n```\n\n::: {.cell-output-display}\n![](10-chapter_files/figure-html/unnamed-chunk-31-1.png){width=768}\n:::\n:::\n\n\nIn this case, scenario 1 is better and more accurate—the actual observed value of y fits more nicely in the posterior predictive distribution for that y. We can assign a numerical value to the length of that line (or the value of the pdf at that point). For whatever reason, Bayesians log that distance, so we want the average *log* posterior predictive density at each new data point. We do this with leave-one-out CV (using the one hold-out data point as the actual data in the scenario above).\n\nThese ELPD values are on a weird meaningless scale that you can't really interpret directly. You can compare them across models, though, and **higher is better**.\n\nCalculating the log likelihood is tricky and has to be defined in the Stan model. rstanarm and brms handle this automatically; with raw Stan, it has to be done in the `generated quantities` block ([like Monica Alexander does here](https://www.monicaalexander.com/posts/2020-28-02-bayes_viz/#running-the-models-in-stan))\n\n::: {.panel-tabset}\n### rstanarm\n\n\n::: {.cell hash='10-chapter_cache/html/loo-rstanarm_dea511e4304a6c474909b9f8602058c6'}\n\n```{.r .cell-code}\nloo(bike_rstanarm)\n## \n## Computed from 20000 by 500 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo  -4289.0 13.1\n## p_loo         2.5  0.2\n## looic      8578.1 26.2\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n```\n:::\n\n\n### brms\n\n\n::: {.cell hash='10-chapter_cache/html/loo-brms_3d6dc8dfaa0c630bb7df0756a2d2ece0'}\n\n```{.r .cell-code}\nloo(bike_brms)\n## \n## Computed from 20000 by 500 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo  -4289.0 13.1\n## p_loo         2.5  0.2\n## looic      8578.0 26.2\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n```\n:::\n\n\n### Stan\n\n\n::: {.cell hash='10-chapter_cache/html/loo-stan_cb59c463ca8f1ace269c006c25a7cbf3'}\n\n```{.r .cell-code}\nbike_stan_samples$loo()\n## \n## Computed from 20000 by 500 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo  -4289.0 13.1\n## p_loo         2.5  0.2\n## looic      8578.0 26.2\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\n```\n:::\n\n\n:::\n\n\n# 10.4: How good is the MCMC simulation vs. how good is the model?\n\nIn general with all these diagnostics, we're exploring two different questions:\n\n1. How good is the MCMC simulation? Is it long enough, are the chains well-mixed, does it converge, etc.?\n2. How good is the model? Are the assumptions reasonable, is the model fair, and does it create good predictions?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}