{
  "hash": "1b7a5ec0fada468a518d948760f31921",
  "result": {
    "markdown": "---\ntitle: \"Reading notes\"\nsubtitle: \"Hierarchical models without predictors\"\ndate: \"November 1, 2022\"\n---\n\n\n[(Original chapter)](https://www.bayesrulesbook.com/chapter-16.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(rstanarm)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(tidybayes)\nlibrary(patchwork)\nlibrary(scales)\n\n# tikz stuff\n# Necessary for using dvisvgm on macOS\n# See https://www.andrewheiss.com/blog/2021/08/27/tikz-knitr-html-svg-fun/\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n\nfont_opts <- list(dvisvgm.opts = \"--font-format=woff\")\n\n# Plot stuff\nclrs <- MetBrewer::met.brewer(\"Lakota\", 6)\ntheme_set(theme_bw())\n\n# Tell bayesplot to use the Lakota palette for things like pp_check()\n# bayesplot::color_scheme_set(clrs)\n\n# Tell bayesplot to use the viridis rocket palette for things like pp_check()\nviridisLite::viridis(6, option = \"rocket\", end = 0.85, direction = -1) |> \n  # Take off the trailing \"FF\" in the hex codes\n  map_chr(~str_sub(., 1, 7)) |> \n  bayesplot::color_scheme_set()\n\n# Seed stuff\nset.seed(1234)\nBAYES_SEED <- 1234\n\n# Data\ndata(spotify, package = \"bayesrules\")\n\nspotify <- spotify %>% \n  select(artist, title, popularity) %>% \n  mutate(artist = fct_reorder(artist, popularity, .fun = mean))\n```\n:::\n\n\n# The general setup\n\nThe data in this example comes from a sample of music from Spotify. There are 350 songs by 44 different artists:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspotify |> \n  summarize(n_songs = n(),\n            n_artists = n_distinct(artist))\n## # A tibble: 1 × 2\n##   n_songs n_artists\n##     <int>     <int>\n## 1     350        44\n\nspotify |> \n  count(artist)\n## # A tibble: 44 × 2\n##    artist            n\n##    <fct>         <int>\n##  1 Mia X             4\n##  2 Chris Goldarg    10\n##  3 Soul&Roll         5\n##  4 Honeywagon        3\n##  5 Röyksopp          4\n##  6 Freestyle         3\n##  7 DA Image          3\n##  8 Jean Juan         5\n##  9 TV Noise         14\n## 10 Kid Frost         3\n## # … with 34 more rows\n```\n:::\n\n\nThe main thing we care about in this example is song popularity and its variation within and between artists. We'll answer three different questions:\n\n- What's the typical popularity of a typical Spotify song?\n- How much does popularity vary from artist to artist?\n- For any single artist, how much does popularity vary from song to song?\n\nPopularity is measured on a scale of 0–100, with 100 representing the highest popularity:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspotify %>% \n  group_by(artist) %>% \n  summarize(count = n(), popularity = mean(popularity)) |> \n  arrange(popularity)\n## # A tibble: 44 × 3\n##    artist        count popularity\n##    <fct>         <int>      <dbl>\n##  1 Mia X             4       13.2\n##  2 Chris Goldarg    10       16.4\n##  3 Soul&Roll         5       24.2\n##  4 Honeywagon        3       31.7\n##  5 Röyksopp          4       33.2\n##  6 Freestyle         3       33.7\n##  7 DA Image          3       36.7\n##  8 Jean Juan         5       36.8\n##  9 TV Noise         14       38.1\n## 10 Kid Frost         3       40.7\n## # … with 34 more rows\n```\n:::\n\n\nHere's how much these artists' songs vary in popularity. Some are consistently popular with little variation within the artist, like Lil Skies and Sufjan Stevens; others like Kendrik Lamar have enormous variation. There are also patterns across/between artists—Beyoncé is clearly more popular than Honeywagon. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspotify |> \n  ggplot(aes(y = artist, x = popularity)) +\n  geom_point(color = \"grey70\", size = 0.75) +\n  stat_pointinterval(color = clrs[6], point_interval = \"mean_qi\") +\n  labs(x = \"Popularity\", y = NULL)\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWe'll analyze these questions with three different approaches:\n\n- **Complete pooling**: Ignore artists and lump all songs together\n- **No pooling**: Analyze each artist separately and assume that data from one artist has no information about any other artist\n- **Partial pooling** (with multilevel models): Incorporate the grouping structure into the analysis \"so that even though artists differ in popularity, they might share valuable information about each other *and* about the broader population of artists\"\n\nTo be super mathy and formal about the structure of the data, we'll use $i$ to represent a song and $j$ to represent an artist, where $j \\in \\{1, 2, \\dots, 44\\}$, and $n_j$ represents the number of songs each artist has (like Mia X is $n_1 = 4$, Chris Goldarg is $n_2 = 10$, and so on).\n\nWe can calculate the total sample size by adding all the $n_j$s together:\n\n$$\nn = \\sum_{j = 1}^{44} n_j = n_1 + n_2 + \\dots + n_{44} = 350\n$$\n\nThat's one level of the hierarchy—a count of artists with songs inside them.\n\nWe can also think about the songs themselves. Each song $Y$ gets two subscripts for the song id and the artist id: $Y_{i,j}$, where $i \\in \\{1, 2, \\dots, n_j\\}$ and $j \\in \\{1, 2, \\dots, 44\\}$\n\nWhen we look at the data at a song level, we can think of $Y$ as a collection of all the songs by the 44 different artists, or:\n\n$$\nY = \\Bigl( (Y_{1_1}, Y_{2_1}, Y_{n_{1_1}}), (Y_{1_2}, Y_{2_2}, Y_{n_{2_2}}), \\dots, (Y_{1_{44}}, Y_{2_{44}}, Y_{n_{{44}_{44}}}) \\Bigr)\n$$\n\nThe data follows a hierarchical structure, with songs nested inside artists:\n\n\n::: {.cell layout-align=\"center\" engine.opts='{\"dvisvgm.opts\":\"--font-format=woff\"}'}\n::: {.cell-output-display}\n![Partial hierarchical pooling](16-chapter_files/figure-html/partial-pooling-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n# 16.1: Complete pooled model\n\nFirst we'll do a pooled model where we ignore the fact that different (and often repeated) artists recorded these songs. We'll pretend that these all came from 350 completely different independent artists. That means that instead of looking at the data like this:\n\n$$\nY = \\Bigl( (Y_{1_1}, Y_{2_1}, Y_{n_{1_1}}), (Y_{1_2}, Y_{2_2}, Y_{n_{2_2}}), \\dots, (Y_{1_{44}}, Y_{2_{44}}, Y_{n_{{44}_{44}}}) \\Bigr)\n$$\n\n…we'll look at it like this:\n\n$$\nY = ( Y_1, Y_2, \\dots, Y_{350} )\n$$\n\nLumping everything together, here's what the distribution of popularity looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspotify |> \n  ggplot(aes(x = popularity)) +\n  geom_density(fill = clrs[1], color = NA) +\n  labs(x = \"Popularity\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe'll use an intercept-only model to estimate the mean and variance of this distribution based on this official model and these priors:\n\n- For $\\mu$, we'll assume that an average song will have a popularity score of 50, but it could be somewhere between 10 and 90\n- For $\\sigma$, we'll assume that for all songs, popularity will vary with a standard deviation of 25 popularity points\n\n$$\n\\begin{aligned}\n\\text{Popularity}_{i,j} &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/25)\n\\end{aligned}\n$$\n\nWe'll do it both with brms and rstanarm:\n\n::: {.panel-tabset}\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(50, 20), class = Intercept),\n            prior(exponential(0.04), class = sigma))\n\nmodel_pooled_brms <- brm(\n  bf(popularity ~ 1),\n  data = spotify,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"16-manual_cache/model-pooled-brms\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pooled_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  select(-component, -group)\n## # A tibble: 2 × 6\n##   effect   term            estimate std.error conf.low conf.high\n##   <chr>    <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed    (Intercept)         58.4     1.12      56.9      59.8\n## 2 ran_pars sd__Observation     20.7     0.782     19.7      21.7\n```\n:::\n\n\n\n\n### rstanarm\n\n\n::: {.cell hash='16-chapter_cache/html/model-pooled-rstanarm_fe0e00b608a9e91f0e38be05fecdf7d6'}\n\n```{.r .cell-code}\nmodel_pooled_rstanarm <- stan_glm(\n  popularity ~ 1,\n  data = spotify,\n  family = gaussian,\n  prior_intercept = normal(50, 20),\n  prior_aux = exponential(0.04),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pooled_rstanarm |> \n  tidy(effects = c(\"fixed\", \"aux\"),\n       conf.int = TRUE, conf.level = 0.8) |> \n  filter(term != \"mean_PPD\")\n## # A tibble: 2 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 (Intercept)     58.4     1.09      57.0      59.8\n## 2 sigma           20.7     0.785     19.7      21.7\n```\n:::\n\n\n:::\n\nThese results mean that Spotify songs have an average popularity rating ($\\mu$) of 58.36 points. The $\\sigma$ implies that the score bounces around from song to song with a variance of 20.71 points.\n\nWe can see how this model predicts popularity if we plug in the average popularities for each of the artists and see the predicted values. The point ranges here represent the posterior predictions for each artist; the red diamonds show the actual average popularity. This isn't that great—the model treats every artist as the same, so there are no differences in predictions across artists.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_predictions <- spotify |> \n  group_by(artist) |> \n  summarize(popularity = mean(popularity)) |> \n  add_predicted_draws(model_pooled_brms)\n\npooled_predictions |> \n  ggplot(aes(y = artist, x = .prediction)) +\n  stat_pointinterval(color = clrs[1]) +\n  stat_summary(data = spotify, aes(x = popularity), \n               geom = \"point\", fun = mean, color = clrs[3], size = 3, pch = 18) +\n  labs(x = \"Popularity\", y = NULL)\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n# 16.2: No pooled model\n\nAs we saw above, the pooled model isn't great because it erases artist effects. Average popularity and variability in popularity vary substantially across artists. So instead of pretending that there's a global mean popularity level, we'll look at group/artist-specific averages instead. Rather than looking at the data like this:\n\n$$\nY = \\Bigl( (Y_{1_1}, Y_{2_1}, Y_{n_{1_1}}), (Y_{1_2}, Y_{2_2}, Y_{n_{2_2}}), \\dots, (Y_{1_{44}}, Y_{2_{44}}, Y_{n_{{44}_{44}}}) \\Bigr)\n$$\n\n…we'll look at it like this:\n\n$$\n\\begin{aligned}\nY_{j = 1} &= ( Y_1, Y_2, \\dots, Y_{n, j = 1} ) \\\\\nY_{j = 2} &= ( Y_1, Y_2, \\dots, Y_{n, j = 2} ) \\\\\n& \\dots \\\\\nY_{j = 44} &= ( Y_1, Y_2, \\dots, Y_{n, j = 44} ) \\\\\n\\end{aligned}\n$$\n\nThat gives us this formal model:\n\n$$\n\\begin{aligned}\nY_{i, j} &\\sim \\mathcal{N}(\\mu_j, \\sigma) \\\\\n\\\\\n\\mu_j &\\sim \\mathcal{N}(50, 20) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1/25)\n\\end{aligned}\n$$\n\nThis implies that the popularity of all songs $i$ is normally distributed around some group mean $\\mu_j$ with standard deviation $\\sigma$ within each artist.\n\nThis also implies that \n\n- every song $i$ by the same artist $j$ has the same mean $\\mu_j$\n- songs by different artists like $j$ and $k$ have different group averages $\\mu_j$ and $\\mu_k$, so each artist's $\\mu_j$ is unique and doesn't tell us anything about other artists' $\\mu_j$s\n- the $\\sigma$ term is technically still pooled across all songs; we *could* use artist-specific $\\sigma_j$ terms if we wanted, but that would make life too complex\n\nModeling time. This time instead of estimating 44 separate models [like we did in the previous chapter](/bayes-rules/15-chapter.qmd#no-pooling), we can get rid of the global intercept term in the model formula by using `popularity ~ 0 + artist` (in the book they use `popularity ~ artist - 1`, but I like the `0 + artist` syntax since the 0 is in the spot where $\\beta_0$ would normally go and it feels like it lines up with the model syntax better).\n\n::: {.panel-tabset}\n\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(50, 20), class = b),\n            prior(exponential(0.04), class = sigma))\n\nmodel_no_pooled_brms <- brm(\n  bf(popularity ~ 0 + artist),\n  data = spotify,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"16-manual_cache/model-no-pooled-brms\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_no_pooled_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  group_by(effect) |> \n  slice(1:3) |> \n  select(-component, -group)\n## # A tibble: 4 × 6\n## # Groups:   effect [2]\n##   effect   term               estimate std.error conf.low conf.high\n##   <chr>    <chr>                 <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed    artistMiaX             17.2     6.73      8.66      25.8\n## 2 fixed    artistChrisGoldarg     18.0     4.37     12.4       23.6\n## 3 fixed    artistSoul&Roll        26.5     5.94     18.9       34.1\n## 4 ran_pars sd__Observation        14.0     0.562    13.3       14.7\n```\n:::\n\n\n\n\n### rstanarm\n\n\n::: {.cell hash='16-chapter_cache/html/model-no-pooled-rstanarm_c304b0aa974b974ae230989ef1213883'}\n\n```{.r .cell-code}\nmodel_no_pooled_rstanarm <- stan_glm(\n  popularity ~ 0 + artist,\n  data = spotify,\n  family = gaussian,\n  prior = normal(50, 20),\n  prior_aux = exponential(0.04),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_no_pooled_rstanarm |> \n  tidy(effects = c(\"fixed\", \"aux\"),\n       conf.int = TRUE, conf.level = 0.8) |> \n  filter(term != \"mean_PPD\") |> \n  slice(1:3, 45)\n## # A tibble: 4 × 5\n##   term                estimate std.error conf.low conf.high\n##   <chr>                  <dbl>     <dbl>    <dbl>     <dbl>\n## 1 artistMia X             17.3     6.66      8.67      25.8\n## 2 artistChris Goldarg     18.0     4.28     12.5       23.5\n## 3 artistSoul&Roll         26.5     5.92     18.9       34.0\n## 4 sigma                   14.0     0.565    13.3       14.7\n```\n:::\n\n\n:::\n\nThese models give us one coefficient for each artist, representing the artist average, or $\\mu_j$. In the output above I show just the first three artists' means. We also get a global $\\sigma$ of 13.99 points.\n\nAs before, we can see how this model predicts popularity if we plug in the average popularities for each of the artists and see the predicted values. Again, these point ranges represent the posterior predictions for each artist; the red diamonds show the actual average popularity. This time things seem to fit really well—that's because the model is designed to predict the artist-specific $\\mu_j$ averages.\n\nBut this great apparent fit comes at a cost:\n\n1. No information is shared across these artists. This is equivalent to running 44 separate models and showing them all in one plot. The number of songs and artist has in the data could influence overall popularity, but since each artist is in an informational silo here, we can't see if that's the case.\n2. The fit is hyper-specific to artists in the sample and can't be used to predict the popularity of other artists.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_pooled_predictions <- spotify |> \n  group_by(artist) |> \n  summarize(popularity = mean(popularity)) |> \n  add_predicted_draws(model_no_pooled_brms)\n\nno_pooled_predictions |> \n  ggplot(aes(y = artist, x = .prediction)) +\n  stat_pointinterval(color = clrs[5], point_interval = \"mean_qi\") +\n  stat_summary(data = spotify, aes(x = popularity), \n               geom = \"point\", fun = mean, color = clrs[2], size = 3, pch = 18) +\n  labs(x = \"Popularity\", y = NULL)\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n# 16.3: Building the hierarchical model\n\nWe can can overcome these shortcomings of the no pooled model by incorporating the group structure into the model and allowing information to be shared across different artist groups. The formal math gets a little bit more gnarly, and there are different ways of describing the models. *Bayes Rules!* shows two different approaches. \n\n## Layer-based notation\n\nFirst they show a model with two layers: one layer that models the popularity of songs within artists and one layer that models the variability of popularity between arists:\n\n$$\n\\begin{aligned}\nY_{i,j} &\\sim \\mathcal{N}(\\mu_j, \\sigma_y) & \\text{Individual songs within artist } j \\\\\n\\mu_j &= \\mathcal{N}(\\mu, \\sigma_{\\mu}) & \\text{Between-artist differences} \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) & \\text{Prior for global average} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/25) & \\text{Prior for within-artist variability} \\\\\n\\sigma_{\\mu} &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-artist variability}\n\\end{aligned}\n$$\n\nIn the first layer, we're modeling the popularity of individual songs within each artist $j$. We have two parameters:\n\n- $\\mu_j$: mean song popularity for artist $j$\n- $\\sigma_y$: **within-group variability**, or the standard deviation of popularity within each artist\n\nIn the second layer, we're modeling the variability of popularity across or between artists. We have two different parameters for this part of the model:\n\n- $\\mu$: **global average** of mean song popularity across all artists $j$\n- $\\sigma_\\mu$: **between-group variability**, or the standard deviation in mean popularity $\\mu_j$ from artist to artist, or how much popularity bounces around as we move from artist to artist\n\n\n## Offset-based notation\n\nI like this notation better (and [it's what I use here](https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/#introduction-to-random-effects-intercepts-for-each-continent)). It's still focused on layers—the first line is the same in both model definitions. The second layer is different now, though:\n\n$$\n\\begin{aligned}\nY_{i,j} &\\sim \\mathcal{N}(\\mu_j, \\sigma_y) & \\text{Individual songs within artist } j \\\\\n\\mu_j &= (\\mu + b_j)  & \\text{Between-artist differences} \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) & \\text{Prior for global average} \\\\\nb_j &\\sim \\mathcal{N}(0, \\sigma_{\\mu}) & \\text{Prior for random artist offsets} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/25) & \\text{Prior for within-artist variability} \\\\\n\\sigma_{\\mu} &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-artist variability}\n\\end{aligned}\n$$\n\nIn the second layer, we're still modeling the variability across artists ($\\mu_j$), but we have a new parameter $b_j$. This requires some short explanation to show where this is coming from.\n\nLet's think about song popularity like a slope/intercept regression model with $\\beta$ terms instead of with $\\mu$. Since this is an intercept-only model, we just have a $\\beta_0$ term:\n\n$$\n\\text{Popularity}_j = \\beta_{0_\\text{Global artist mean}} + \\varepsilon\n$$\n\nNote the $\\varepsilon$ term here. We haven't been using that throughout this book, but it's always been lurking there. It's the error term and represents all the variation in popularity that isn't captured by the global artist mean. Right now the $\\beta_0$ coefficient represents the global average of artist-specific averages. Any deviance from that is stuffed away in $\\varepsilon$.\n\nHere's an example to help illustrate this. $\\mu_j$ represents artist-specific averages, and Beyoncé's average popularity ($\\mu_\\text{Beyoncé}$) is 69.7:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_averages <- spotify |> \n  group_by(artist) |> \n  summarise(mu_j = mean(popularity))\n\nartist_averages |> \n  filter(artist == \"Beyoncé\")\n## # A tibble: 1 × 2\n##   artist   mu_j\n##   <fct>   <dbl>\n## 1 Beyoncé  69.7\n```\n:::\n\n\nThe global mean artist-specific popularity is the average of the `mu_j` column, or 52.1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_averages |> \n  summarise(global_mu = mean(mu_j))\n## # A tibble: 1 × 1\n##   global_mu\n##       <dbl>\n## 1      52.1\n```\n:::\n\n\nAn intercept-only model of popularity using only artist-specific averages would look something like this:\n\n$$\n\\text{Popularity}_j = 52.1 + \\varepsilon\n$$\n\nBeyoncé's average popularity is 17.6 points higher than the global average of 52.1, but that's lost in the error term. We can highlight it to show that it really exists:\n\n$$\n\\text{Popularity}_\\text{Beyoncé} = 52.1 + (17.6_\\text{Beyoncé} + \\varepsilon)\n$$\n\nWith some algebra, we can group that Beyoncé offset with the global mean:\n\n$$\n\\text{Popularity}_\\text{Beyoncé} = (52.1 + 17.6_\\text{Beyoncé}) + \\varepsilon\n$$\n\nNeat. Beyoncé's artist-specific average is thus the global mean plus an artist-specific offset.\n\nWe can generalize this to any artist $j$. We'll call the offset term $b_j$:\n\n$$\n\\text{Popularity}_j = (\\beta_0 + b_j) + \\varepsilon \n$$\n\nOr if we want to go back to the world of $\\mu$ instead of $\\beta$s:\n\n$$\n\\text{Popularity}_j = (\\mu + b_j) \n$$\n\nPhew. I like this offset approach because it highlights the fact that these are \"random\" effects, or that we're modeling the distribution of group-specific shifts above and below some global mean. Here's the full model again, for reference:\n\n$$\n\\begin{aligned}\nY_{i,j} &\\sim \\mathcal{N}(\\mu_j, \\sigma_y) & \\text{Individual songs within artist } j \\\\\n\\mu_j &= (\\mu + b_j)  & \\text{Between-artist differences} \\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(50, 20) & \\text{Prior for global average} \\\\\nb_j &\\sim \\mathcal{N}(0, \\sigma_{\\mu}) & \\text{Prior for random artist offsets} \\\\\n\\sigma_y &\\sim \\operatorname{Exponential}(1/25) & \\text{Prior for within-artist variability} \\\\\n\\sigma_{\\mu} &\\sim \\operatorname{Exponential}(1) & \\text{Prior for between-artist variability}\n\\end{aligned}\n$$\n\n## 16.3.3: Within- vs between-group variability\n\nSince these multilevel/hierarchical/random effects/whatever-we-want-to-call-them models give us so much information about the variability within and between groups, we can do some neat things with that information. \n\nFor instance, the total variance in individual song popularity within and between artists is the sum of the two $\\sigma$ terms we've estimated:\n\n$$\n\\operatorname{Var}(Y_{i, j}) = \\sigma_y + \\sigma_\\mu\n$$\n\nThis means we can calculate some ratios and percentages:\n\n- $\\frac{\\sigma_y}{\\sigma_y + \\sigma_\\mu}$: Proportion of total variance that is explained by **within-artist** differences\n- $\\frac{\\sigma_\\mu}{\\sigma_y + \\sigma_\\mu}$: Proportion of total variance that is explained by **between-artist** differences\n\n\n# 16.4: Posterior analysis\n\nK, finally we can calculate the posterior. To add a random intercept, or allow the intercept to vary by artist-specific offsets (yay for the offset notation!), we include a `(1 | artist)` term in both brms and rstanarm (and [this table is indispensable](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification) for remembering how to set different random offsets for different nesting and grouping structures)\n\n::: {.panel-tabset}\n\n### brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(50, 20), class = Intercept),\n            prior(exponential(0.04), class = sigma),\n            prior(exponential(1), class = sd))\n\nmodel_multilevel_brms <- brm(\n  bf(popularity ~ (1 | artist)),\n  data = spotify,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, iter = 4000, seed = BAYES_SEED, \n  backend = \"cmdstanr\", refresh = 0,\n  file = \"16-manual_cache/model-multilevel-brms\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  select(-component)\n## # A tibble: 3 × 7\n##   effect   group    term            estimate std.error conf.low conf.high\n##   <chr>    <chr>    <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n## 1 fixed    <NA>     (Intercept)         52.5     2.17      49.8      55.3\n## 2 ran_pars artist   sd__(Intercept)     12.6     1.39      10.9      14.5\n## 3 ran_pars Residual sd__Observation     14.1     0.584     13.3      14.8\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Put the results in a list\nm_mlm <- model_multilevel_brms |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"), conf.level = 0.8) |> \n  mutate(term = janitor::make_clean_names(term)) |> \n  split(~term)\n```\n:::\n\n\n### rstanarm\n\nThe `decov()` prior for $\\sigma_\\mu$ here is weird—it's equivalent to `exponential(1)` in the case since we have no predictors.\n\n\n::: {.cell hash='16-chapter_cache/html/model-multilevel-rstanarm_3d95b56b6a6f3622ce5b5639c7d092c8'}\n\n```{.r .cell-code}\nmodel_multilevel_rstanarm <- stan_glmer(\n  popularity ~ (1 | artist),\n  data = spotify,\n  family = gaussian,\n  prior_intercept = normal(50, 20),\n  prior_aux = exponential(0.04),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = BAYES_SEED, refresh = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_rstanarm |> \n  tidy(effects = c(\"fixed\", \"ran_pars\"),\n       conf.int = TRUE, conf.level = 0.8)\n## # A tibble: 3 × 6\n##   term                    estimate std.error conf.low conf.high group   \n##   <chr>                      <dbl>     <dbl>    <dbl>     <dbl> <chr>   \n## 1 (Intercept)                 52.4      2.40     49.2      55.6 <NA>    \n## 2 sd_(Intercept).artist       15.2     NA        NA        NA   artist  \n## 3 sd_Observation.Residual     14.0     NA        NA        NA   Residual\n```\n:::\n\n\n:::\n\n(Weirdly rstanarm and brms give different results for the between-artist $\\sigma_mu$ term `sd_(Intercept)`. I don't know why—probably a prior thing?)\n\n## Global parameters (\"fixed\" effects)\n\nAt the global level of the model, we have these terms:\n\n- $\\mu$ or `(Intercept)`: Global average artist-specific popularity\n- $\\sigma_y$ or `sd__(Intercept)` for `artist`: Within-artist variability\n- $\\sigma^2_\\mu$ or `sd__Observation` for `Residual`: Between-artist variability\n\nThe average artist has a posterior average ($\\mu$) of 52.5 popularity points, and there's an 80% chance that their average popularity is between 49.8 and 55.3\n\nAccording to the posterior mean of $\\sigma_y$ or `sd__(Intercept)`, the between-artist variability is 12.6, which means that popularity bounces around by 12ish points as we move *from artist to artist*. The posterior mean of $\\sigma^2_\\mu$ or `sd__Observation` shows that the within-artist variability is 14.1, which means that within any artist, popularity varies by 14ish points *from song to song*. Alternatively, this is the total residual variation, or the unexplained variation in song popularity.\n\nWe can play with the ratios of these types of variance too:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Proportion of song popularity explained by differences *between* artists\n(m_mlm$sd_intercept$estimate^2 / \n   (m_mlm$sd_intercept$estimate^2 + m_mlm$sd_observation$estimate^2)) |> \n  unname()\n## [1] 0.4450867\n\n# Proportion of song popularity explained by differences *within* artists\n(m_mlm$sd_observation$estimate^2 / \n    (m_mlm$sd_intercept$estimate^2 + m_mlm$sd_observation$estimate^2)) |> \n  unname()\n## [1] 0.5549133\n```\n:::\n\n\n## Group parameters (\"random\" effects)\n\nAt the artist level of the model, we only have one term:\n\n- $b_j$ or `(Intercept)`: Artist-specific offset from the global mean popularity $\\mu$\n\nWe can extract these offsets with the `ranef()` function, which returns a messy gross array, or we can use `broom.mixed::tidy(..., effects = \"ran_vals\")`. Let's look at Beyoncé's offset, since we used that as an example earlier. It should be somewhere around 17.6 (though not exactly that, since that was the value from the no pooling model—we have more information about artists this time, so that should influence the estimated artist average here).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  tidy(effects = \"ran_vals\", conf.level = 0.8) |> \n  filter(level == \"Beyoncé\") |> \n  select(-effect, -component)\n## # A tibble: 1 × 7\n##   group  level   term        estimate std.error conf.low conf.high\n##   <chr>  <chr>   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 artist Beyoncé (Intercept)     16.4      3.43     11.9      20.7\n```\n:::\n\n\nThe offset is a little smaller (16ish) now that we're partially pooling, and there's an 80% chance that her offset is between 12 and 21ish popularity points from the global average.\n\nWe can look at everyone's offsets and calculate artist-specific posterior group averages, or $\\mu_j$ with some tidybayes wrangling:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  spread_draws(b_Intercept, r_artist[artist,]) |>  \n  mutate(mu_j = b_Intercept + r_artist) |>\n  ungroup() |> \n  mutate(artist = fct_reorder(artist, mu_j, .fun = mean)) |> \n  ggplot(aes(y = artist, x = mu_j)) +\n  stat_pointinterval(color = clrs[3]) +\n  labs(x = \"Popularity\", y = NULL)\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n# 16.5: Posterior prediction\n\nBecause we used a hierarchical model, we aren't limited to making predictions for only the artists that exist in the original data. We can use the model to make predictions both for existing artists and for brand new unseen artists\n\n## Predictions for existing groups / artists\n\nThere are multiple forms of uncertainty in this model at both the global and artist levels. Like I explore in [this guide to different posterior predictions](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/), we have uncertainty in $mu_j$, or the artist-specific average, and uncertainty in $Y$, or the song-specific average. \n\nThe posterior predictions for $mu_j$ will be narrow than those for $Y$ because $mu_j$ is only a function of the global $\\mu$ and artist-specific offsets $b_j$:\n\n$$\n\\mu_j = \\mu + b_j\n$$\n\nThis corresponds to `posterior_linpred()` or `posterior_epred()`.\n\nThe posterior predictions for $Y$ have much more uncertainty because $Y$ is a function of both $\\mu_j$ and $\\sigma_y$, or the within-artist variability:\n\n$$\nY_{i,j} \\sim \\mathcal{N}(\\mu_j, \\sigma_y)\n$$\n\nThis corresponds to `posterior_predict()`.\n\nWe can make these posterior predictions both manually and with the more automatic functions. We'll predict Frank Ocean's popularity since that's what the book does.\n\nManually we need to get the intercept, offset, and sigma from the MCMC chains, add the intercept and offset to create $\\mu_j$, and then randomly draw from a normal distribution with that $\\mu_j$ and $\\sigma_y$:\n\n::: {.panel-tabset}\n### Manually\n\n\n::: {.cell}\n\n```{.r .cell-code}\nocean_posterior_preds <- model_multilevel_brms %>%\n  spread_draws(b_Intercept, r_artist[artist,], sigma) |> \n  filter(artist == \"Frank.Ocean\") |> \n  mutate(mu_ocean = b_Intercept + r_artist,\n         y_ocean = rnorm(n(), mean = mu_ocean, sd = sigma))\n\nocean_posterior_preds |> \n  ggplot(aes(x = y_ocean)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Frank Ocean song\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n### Automatically with `postior_predict()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = \"Frank Ocean\")) |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Frank Ocean song\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n:::\n\nLooking at the posterior distribution of just $\\mu_j$ gives us a narrower range, since this corresponds to our prediction of Frank Ocean's underlying average song popularity, not the popularity of a single song:\n\n::: {.panel-tabset}\n### Manually\n\n\n::: {.cell}\n\n```{.r .cell-code}\nocean_posterior_preds |> \n  ggplot(aes(x = mu_ocean)) +\n  stat_halfeye(fill = clrs[2]) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Frank Ocean songs\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n### Automatically wtih `posterior_linpred()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  linpred_draws(newdata = tibble(artist = \"Frank Ocean\")) |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[2]) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Frank Ocean songs\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Predictions for unseen groups / artists\n\nIf we want to simulate a posterior for a completely unobserved artist, we can't just extract their offset, add it to the global mean, and use `rnorm()`. There's no offset to extract!\n\nSo instead we have to simulate the offset. We estimated a posterior distribution for $\\sigma_\\mu$ for the variability in the offsets:\n\n$$\nb_j \\sim \\mathcal{N}(0, \\sigma_{\\mu})\n$$\n\n…so we can use that to generate a random offset, then add that to the global mean, and then make predictions like normal. In the book they make up an artist named \"Mohsen Beats\" so we'll do that here too.\n\nWhen doing this automatically, we need to include `re_formula = NULL` and `allow_new_levels = TRUE` to incorporate the random effects into the predictions ([see here for more about that](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/#overall-summary-of-different-approaches)). We also use `sample_new_levels = \"gaussian\"` to indicate that the random offsets we're adding in for the new fake artist come from a normal distribution (just like we do with `rnorm()`). It's also possible to use `sample_new_levels = \"uncertainty\"` (and I think that's the default?), which doesn't use `rnorm()` to generate the offsets but instead somehow uses the uncertainty and variation in the existing groups to create the offsets. \n\n::: {.panel-tabset}\n### Manually\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmohsen_posterior_preds <- model_multilevel_brms %>%\n  spread_draws(b_Intercept, sd_artist__Intercept, sigma) |> \n  mutate(sigma_mu = sd_artist__Intercept,\n         mu_mohsen = b_Intercept + rnorm(n(), mean = 0, sd = sigma_mu),\n         y_mohsen = rnorm(n(), mean = mu_mohsen, sd = sigma))\n\nmohsen_posterior_preds |> \n  ggplot(aes(x = y_mohsen)) +\n  stat_halfeye(fill = colorspace::darken(clrs[3], 0.7)) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Mohsen Beats song\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n### Automatically with `posterior_predict(re_formula = NULL)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = \"Mohsen Beats\"),\n                  re_formula = NULL, allow_new_levels = TRUE,\n                  sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .prediction)) +\n  stat_halfeye(fill = colorspace::darken(clrs[3], 0.7)) +\n  labs(x = \"Posterior predictive distribution for the\\npopularity of one new Mohsen Beats song\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n:::\n\nWe can also get the posterior distribution of just the $\\mu_j$ part for Mohsen Beats, providing a prediction of the artist's average song popularity:\n\n::: {.panel-tabset}\n### Manually\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmohsen_posterior_preds |> \n  ggplot(aes(x = mu_mohsen)) +\n  stat_halfeye(fill = colorspace::darken(clrs[2], 0.5)) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Mohsen Beats songs\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n### Automatically wtih `posterior_linpred(re_formula = NULL)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multilevel_brms |> \n  linpred_draws(newdata = tibble(artist = \"Mohsen Beats\"),\n                re_formula = NULL, allow_new_levels = TRUE,\n                sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .linpred)) +\n  stat_halfeye(fill = colorspace::darken(clrs[2], 0.5)) +\n  labs(x = \"Posterior predictive distribution for the\\nmean popularity of Mohsen Beats songs\")\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Comparisons\n\nFor fun, we can compare all four of these posteriors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- model_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = c(\"Frank Ocean\", \"Mohsen Beats\")),\n                  re_formula = NULL, allow_new_levels = TRUE,\n                  sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .prediction, y = artist, fill = artist)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[3], colorspace::darken(clrs[3], 0.7)),\n                    guide = \"none\") +\n  labs(x = \"Posterior predictions for individual songs (Yij)\\n(equivalent to posterior_predict()\",\n       y = NULL)\n\np2 <- model_multilevel_brms |> \n  linpred_draws(newdata = tibble(artist = c(\"Frank Ocean\", \"Mohsen Beats\")),\n                re_formula = NULL, allow_new_levels = TRUE,\n                sample_new_levels = \"gaussian\") |> \n  ggplot(aes(x = .linpred, y = artist, fill = artist)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[2], colorspace::darken(clrs[2], 0.5)),\n                    guide = \"none\") +\n  labs(x = \"Posterior predictions for artist average (µj)\\n(equivalent to posterior_linpred()\",\n       y = NULL)\n\np1 | p2\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-30-1.png){width=768}\n:::\n:::\n\n\n\n# 16.6: Shrinkage and the bias-variance tradeoff\n\nWe've been talking about how information is shared across artists in these hierarchical models, and that's why they're better than the no pooling approach, but what information actually gets shared? In these intercept-only models, there aren't any other covariates that are spread across artists (like age of artist, number of years producing music, gender, income, whatever) that could inform between-artist variation. All we have is the artist name.\n\nEven with the artist name, we're still sharing information across artists, but in a more subtle way. The average popularity for each artist is heavily informed by the number of songs each artist has. Artists with just a handful of songs might have high popularity, but that average is really unstable; artists with lots of songs and who are also highly popular have a much more stable average. \n\n(This is like Amazon ratings too: an item on Amazon with a 4.6 star average with 10,000 reviews is more trustworthy than an item with a 4.8 star average with 4 reviews)\n\nIn the book they compare the ratings and counts of Camila Cabello (38 songs) and Lil Skies (3 songs):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspotify |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\")) |> \n  group_by(artist) |> \n  summarize(count = n(), popularity = mean(popularity))\n## # A tibble: 2 × 3\n##   artist         count popularity\n##   <fct>          <int>      <dbl>\n## 1 Camila Cabello    38       77.1\n## 2 Lil Skies          3       79.3\n```\n:::\n\n\nIn a model without pooling, Lil Skies's average remains high because it ignores the effect of artist-specific sample size, but hierarchical models care about the number of songs within each artist, and that influences the predicted average for artists with fewer songs. *That's* the information sharing we get in an intercept-only model.\n\nWe can compare the predictions from the no pooling and multilevel models for these two to see the smaller prediction after sharing information in the hierarchical model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_pooled_small <- no_pooled_predictions |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\")) |> \n  mutate(model_type = \"No pooling\") |> \n  ungroup() |> \n  select(artist, .row, .chain, .iteration, .draw, .prediction, model_type)\n\nmultilevel_small <-  model_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = unique(spotify$artist))) |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\")) |> \n  mutate(model_type = \"Multilevel\") |> \n  ungroup() |> \n  select(artist, .row, .chain, .iteration, .draw, .prediction, model_type)\n\nboth_small <- rbind(no_pooled_small, multilevel_small)\n\nactual_small <- spotify |> \n  filter(artist %in% c(\"Camila Cabello\", \"Lil Skies\"))\n\nboth_small |> \n  ggplot(aes(x = .prediction, y = fct_rev(artist), color = model_type)) +\n  stat_pointinterval(position = position_dodge(width = 0.4)) +\n  stat_summary(data = actual_small, \n               aes(x = popularity, color = \"Original data\"), \n               fun.data = \"mean_se\", fun.args = list(mult = 1.96),\n               geom = \"pointrange\", size = 1, pch = 18) +\n  scale_color_manual(values = c(clrs[1], \"grey50\", clrs[3]),\n                     breaks = c(\"No pooling\", \"Original data\", \"Multilevel\")) +\n  labs(x = \"Popularity\", y = NULL, color = NULL) +\n  theme(panel.grid.major.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nFor Camila Cabello, the averages are all basically the same since she has so many songs (38). For Lil Skies, though, the predicted average in the multilevel model is smaller than either the no pooled data or the observed mean, since the partial pooling takes the small number of songs (3) into account and shrinks the prediction.\n\nWe can see this phenomenon if we look at everyone's posterior predictions. For artists near the average level of popularity, the model's predictions are basically perfectly inline with the observed average. For artists with more extreme levels of popularity, the multilevel model pulls their averages toward the middle, particularly when artists only have a handful of songs. You can see a sizable divergence in observed and predicted averages for artists like Lil Skies and Sean Kingston at the top end of popularity and Mia X and Soul & Roll at the bottom end. In an artist in the extremes has a lot of songs (like Camila Cabello or Beyoncé), there's no shrinkage—the higher number of songs helps stabilize the prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultilevel_predictions <- model_multilevel_brms |> \n  predicted_draws(newdata = tibble(artist = unique(spotify$artist)))\n\nmultilevel_predictions |> \n  ggplot(aes(y = artist, x = .prediction)) +\n  stat_pointinterval(color = clrs[3], point_interval = \"mean_qi\") +\n  stat_summary(data = spotify, aes(x = popularity), \n               geom = \"point\", fun = mean, color = clrs[2], size = 3, pch = 18) +\n  labs(x = \"Popularity\", y = NULL)\n```\n\n::: {.cell-output-display}\n![](16-chapter_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}